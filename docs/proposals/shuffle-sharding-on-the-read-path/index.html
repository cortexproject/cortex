<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.101.0"><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Shuffle sharding on the read path | Cortex</title><meta name=description content="
Author: @pracucci, @tomwilkie, @pstibrany
Reviewers:
Date: August 2020
Status: Proposed, partially implemented

Background
Cortex currently supports …"><meta property="og:title" content="Shuffle sharding on the read path"><meta property="og:description" content="Author: @pracucci, @tomwilkie, @pstibrany Reviewers: Date: August 2020 Status: Proposed, partially implemented Background Cortex currently supports sharding of tenants to a subset of the ingesters on the write path PR.
This feature is called “subring”, because it computes a subset of nodes registered to the hash ring. The aim of this feature is to improve isolation between tenants and reduce the number of tenants impacted by an outage.
This approach is similar to the techniques described in Amazon’s Shuffle Sharding article, but currently suffers from a non random selection of nodes (proposed solution below)."><meta property="og:type" content="article"><meta property="og:url" content="/docs/proposals/shuffle-sharding-on-the-read-path/"><meta property="og:image" content="/images/logo-twitter-card.jpg"><meta property="article:section" content="docs"><meta property="og:site_name" content="Cortex"><meta itemprop=name content="Shuffle sharding on the read path"><meta itemprop=description content="Author: @pracucci, @tomwilkie, @pstibrany Reviewers: Date: August 2020 Status: Proposed, partially implemented Background Cortex currently supports sharding of tenants to a subset of the ingesters on the write path PR.
This feature is called “subring”, because it computes a subset of nodes registered to the hash ring. The aim of this feature is to improve isolation between tenants and reduce the number of tenants impacted by an outage.
This approach is similar to the techniques described in Amazon’s Shuffle Sharding article, but currently suffers from a non random selection of nodes (proposed solution below)."><meta itemprop=wordCount content="3393"><meta itemprop=image content="/images/logo-twitter-card.jpg"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/images/logo-twitter-card.jpg"><meta name=twitter:title content="Shuffle sharding on the read path"><meta name=twitter:description content="Author: @pracucci, @tomwilkie, @pstibrany Reviewers: Date: August 2020 Status: Proposed, partially implemented Background Cortex currently supports sharding of tenants to a subset of the ingesters on the write path PR.
This feature is called “subring”, because it computes a subset of nodes registered to the hash ring. The aim of this feature is to improve isolation between tenants and reduce the number of tenants impacted by an outage.
This approach is similar to the techniques described in Amazon’s Shuffle Sharding article, but currently suffers from a non random selection of nodes (proposed solution below)."><link rel=preload href=/scss/main.min.3e1c32b3e2f4f1a323bd9e57fff11c26914f9274828c16cd94498b5b557fc2e7.css as=style><link href=/scss/main.min.3e1c32b3e2f4f1a323bd9e57fff11c26914f9274828c16cd94498b5b557fc2e7.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SRR0DSREGX"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SRR0DSREGX",{anonymize_ip:!1})}</script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar navbar-nocover"><a id=cortex-logo class=navbar-brand href=/><span class=navbar-logo><svg xmlns="http://www.w3.org/2000/svg" role="img" viewBox="-17.92 -14.92 1191.84 462.84"><defs><style>.cls-1{fill:#fff}</style></defs><path d="M214.531 8.017C99.353 8.017 5.65 101.72 5.65 216.899S99.353 425.78 214.531 425.78s208.882-93.704 208.882-208.882S329.71 8.017 214.531 8.017zm0 408.558c-110.102.0-199.676-89.574-199.676-199.676S104.429 17.222 214.53 17.222 414.208 106.797 414.208 216.9 324.633 416.575 214.53 416.575z" class="cls-1"/><circle cx="327.452" cy="221.633" r="34.571" class="cls-1"/><circle cx="213.713" cy="353.584" r="34.571" class="cls-1"/><path d="M299.992 167.913l-59-56.05a34.578 34.578.0 10-4.343 4.34l57.828 54.937a60.158 60.158.0 00-27.168 49.123h-12.97l-23.456-67.02-26.055 64.718H175.73l-24.724 57.22-21.808-54.524-.063.025v-.586h-22.048a34.582 34.582.0 10-.275 6.138h18.005l25.97 64.925 28.977-67.06h29.207l21.51-53.424 19.503 55.725H267.5a60.173 60.173.0 0025.202 44.11l-56.52 56.52 4.34 4.338 57.492-57.492a60.155 60.155.0 101.977-105.963zm81.481 53.515a54.02 54.02.0 11-54.022-54.02 54.083 54.083.0 0154.022 54.02zm175.707 42.568q-8.797 8.178-24.405 8.177a34.858 34.858.0 01-17.095-3.965 33.188 33.188.0 01-11.643-10.529 46.374 46.374.0 01-6.566-14.99 71.134 71.134.0 01-2.105-17.341 86.99 86.99.0 011.982-18.706 46.804 46.804.0 016.565-15.98 34.028 34.028.0 0112.263-11.149q7.675-4.21 19.077-4.212 13.38.0 21.306 6.69 7.927 6.689 10.405 18.829h21.803a50.76 50.76.0 00-5.946-19.696 44.06 44.06.0 00-12.015-13.75 49.842 49.842.0 00-16.848-8.051 77.44 77.44.0 00-20.44-2.601q-15.114.0-26.509 5.326a52.935 52.935.0 00-18.952 14.617 62.165 62.165.0 00-11.273 21.8 94.013 94.013.0 00-3.717 26.883 86.195 86.195.0 003.84 26.386 57.783 57.783.0 0011.397 20.686 50.138 50.138.0 0018.829 13.38 66.766 66.766.0 0025.891 4.705q24.527.0 38.772-12.882 14.245-12.878 17.715-36.668h-21.556q-1.986 14.867-10.776 23.041zm157.44-87.828a56.338 56.338.0 00-19.447-14.244q-11.52-5.203-26.882-5.203-15.115.0-26.756 5.203a56.009 56.009.0 00-19.573 14.244 59.75 59.75.0 00-11.892 21.307 85.299 85.299.0 00-3.965 26.386 84.117 84.117.0 003.965 26.262 59.853 59.853.0 0011.892 21.183 54.61 54.61.0 0019.573 14.12q11.643 5.077 26.756 5.08 15.36.0 26.882-5.08a54.908 54.908.0 0019.447-14.12 59.95 59.95.0 0011.892-21.183 84.181 84.181.0 003.965-26.262 85.364 85.364.0 00-3.965-26.386 59.846 59.846.0 00-11.892-21.307zm-9.538 68.38a43.305 43.305.0 01-8.548 15.113 37.061 37.061.0 01-12.759 9.29 38.825 38.825.0 01-30.968.0 37.003 37.003.0 01-12.76-9.29 43.209 43.209.0 01-8.547-15.114 70.669 70.669.0 010-41.373 44.634 44.634.0 018.547-15.237 36.428 36.428.0 0112.76-9.415 38.826 38.826.0 0130.968.0 36.484 36.484.0 0112.76 9.415 44.735 44.735.0 018.547 15.237 70.624 70.624.0 010 41.373zm81.141-80.89q-11.147 7.434-18.83 23.041h-.493v-27.005h-19.82V287.78h21.057v-56.984a87.528 87.528.0 012.478-21.924 41.993 41.993.0 017.93-16.228 33.951 33.951.0 0114.368-10.158q8.919-3.468 21.555-3.468V156.72q-17.097-.493-28.245 6.937zm80.761-42.366h-21.06v38.402h-21.8v18.581h21.8v81.51a48.668 48.668.0 001.734 14.37 17.432 17.432.0 005.326 8.423 20.57 20.57.0 009.415 4.088 75.59 75.59.0 0013.999 1.115h16.104v-18.582h-9.664a70.335 70.335.0 01-8.05-.37 10.361 10.361.0 01-4.833-1.611 6.108 6.108.0 01-2.354-3.469 23.006 23.006.0 01-.617-5.946v-79.528h25.518v-18.581h-25.518zm148.522 60.452a56.135 56.135.0 00-18.085-17.962q-11.276-7.063-28.367-7.06a58.263 58.263.0 00-24.155 4.953A56.796 56.796.0 00925.82 175.55a63.949 63.949.0 00-12.51 21.058 77.064 77.064.0 00-4.461 26.758 102.521 102.521.0 004.338 27.004 58.87 58.87.0 0011.519 21.307 52.43 52.43.0 0018.953 13.873q11.272 4.954 26.632 4.955 21.8.0 36.173-10.901 14.364-10.898 18.58-32.455h-20.81q-2.73 12.635-11.272 18.829-8.549 6.198-21.927 6.195a43.577 43.577.0 01-18.085-3.468 35.417 35.417.0 01-12.636-9.291 36.127 36.127.0 01-7.184-13.38 50.746 50.746.0 01-1.981-15.98h95.879a102.07 102.07.0 00-2.107-24.526 71.064 71.064.0 00-9.415-23.784zm-84.357 29.73a43.81 43.81.0 013.219-13.999 37.354 37.354.0 017.433-11.52 34.013 34.013.0 0111.272-7.803 36.694 36.694.0 0114.74-2.85 36.062 36.062.0 0114.494 2.85 36.492 36.492.0 0111.398 7.68 36.107 36.107.0 017.68 11.52 43.253 43.253.0 013.345 14.122zm170.95 7.184 44.1-58.964h-25.271l-31.961 44.843-30.721-44.843h-27.006l44.595 60.698-48.063 67.389h25.518l35.677-53.019 35.676 53.019h27.006l-49.55-69.123z" class="cls-1"/></svg></span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/docs/><span class=active>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a href=https://twitter.com/cortexmetrics class="nav-link active"><span class=active><i class="fab fa-fw fa-twitter"></i> Twitter</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a href=https://github.com/cortexproject class="nav-link active"><span class=active><i class="fab fa-fw fa-github"></i> Github</span></a></li></ul></div><div class="navbar-nav d-none d-md-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.8e10dc0a457825e6436b09d0cfbe6e87.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><div id=content-mobile><form class="td-sidebar__search d-flex align-items-center"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.8e10dc0a457825e6436b09d0cfbe6e87.json data-offline-search-base-href=/ data-offline-search-max-results=10>
<button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type=button data-toggle=collapse data-target=#td-section-nav aria-controls=td-section-nav aria-expanded=false aria-label="Toggle section navigation"></button></form></div><div id=content-desktop></div><nav class="collapse td-sidebar-nav" id=td-section-nav><ul class="td-sidebar-nav__section pr-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-docs-li><a href=/docs/ class="align-left pl-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-docs><span>Documentation</span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsgetting-started-li><a href=/docs/getting-started/ class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id=m-docsgetting-started><span>Getting Started</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsarchitecture-li><a href=/docs/architecture/ title="Cortex Architecture" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsarchitecture><span>Architecture</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docsblocks-storage-li><a href=/docs/blocks-storage/ class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id=m-docsblocks-storage><span>Blocks Storage</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsblocks-storagequerier-li><a href=/docs/blocks-storage/querier/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagequerier><span>Querier</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsblocks-storagestore-gateway-li><a href=/docs/blocks-storage/store-gateway/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagestore-gateway><span>Store-gateway</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsblocks-storagecompactor-li><a href=/docs/blocks-storage/compactor/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagecompactor><span>Compactor</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsblocks-storageproduction-tips-li><a href=/docs/blocks-storage/production-tips/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storageproduction-tips><span>Production tips</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsblocks-storagebinary-index-header-li><a href=/docs/blocks-storage/binary-index-header/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagebinary-index-header><span>Binary index-header</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsblocks-storagebucket-index-li><a href=/docs/blocks-storage/bucket-index/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagebucket-index><span>Bucket Index</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsblocks-storagemigrate-cortex-cluster-from-chunks-to-blocks-li><a href=/docs/blocks-storage/migrate-cortex-cluster-from-chunks-to-blocks/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagemigrate-cortex-cluster-from-chunks-to-blocks><span>Migrate Cortex cluster from chunks to blocks</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsblocks-storageconvert-long-term-storage-from-chunks-to-blocks-li><a href=/docs/blocks-storage/convert-long-term-storage-from-chunks-to-blocks/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storageconvert-long-term-storage-from-chunks-to-blocks><span>Convert long-term storage from chunks to blocks</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsblocks-storagemigrate-storage-from-thanos-and-prometheus-li><a href=/docs/blocks-storage/migrate-storage-from-thanos-and-prometheus/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagemigrate-storage-from-thanos-and-prometheus><span>Migrate the storage from Thanos and Prometheus</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsblocks-storagelearn-more-li><a href=/docs/blocks-storage/learn-more/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsblocks-storagelearn-more><span>Learn more</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docsconfiguration-li><a href=/docs/configuration/ class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id=m-docsconfiguration><span>Configuration</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsconfigurationconfiguration-file-li><a href=/docs/configuration/configuration-file/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsconfigurationconfiguration-file><span>Configuration file</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsconfigurationarguments-li><a href=/docs/configuration/arguments/ title="Cortex Arguments" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsconfigurationarguments><span>Cortex Arguments Explained</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsconfigurationprometheus-frontend-li><a href=/docs/configuration/prometheus-frontend/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsconfigurationprometheus-frontend><span>Prometheus Frontend</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsconfigurationv1guarantees-li><a href=/docs/configuration/v1guarantees/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsconfigurationv1guarantees><span>v1.x Guarantees</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docsguides-li><a href=/docs/guides/ class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id=m-docsguides><span>Guides</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesrunning-cortex-on-kubernetes-li><a href=/docs/guides/running-cortex-on-kubernetes/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesrunning-cortex-on-kubernetes><span>Running Cortex on Kubernetes</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesgetting-started-with-gossiped-ring-li><a href=/docs/guides/getting-started-with-gossiped-ring/ title="Getting started with gossiped ring" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesgetting-started-with-gossiped-ring><span>Getting started with a gossip ring cluster</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesalertmanager-configuration-li><a href=/docs/guides/alertmanager-configuration/ title="Configuring Notification using Cortex Alertmanager" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesalertmanager-configuration><span>Alertmanager configuration</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesauth-li><a href=/docs/guides/auth/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesauth><span>Authentication and Authorisation</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidescapacity-planning-li><a href=/docs/guides/capacity-planning/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidescapacity-planning><span>Capacity Planning</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesruler-sharding-li><a href=/docs/guides/ruler-sharding/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesruler-sharding><span>Config for horizontally scaling the Ruler</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesha-pair-handling-li><a href=/docs/guides/ha-pair-handling/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesha-pair-handling><span>Config for sending HA Pairs data to Cortex</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesencryption-at-rest-li><a href=/docs/guides/encryption-at-rest/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesencryption-at-rest><span>Encryption at Rest</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesingesters-rolling-updates-li><a href=/docs/guides/ingesters-rolling-updates/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesingesters-rolling-updates><span>Ingesters rolling updates</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesingesters-scaling-up-and-down-li><a href=/docs/guides/ingesters-scaling-up-and-down/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesingesters-scaling-up-and-down><span>Ingesters scaling up and down</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesoverrides-exporter-li><a href=/docs/guides/overrides-exporter/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesoverrides-exporter><span>Overrides Exporter</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidestls-li><a href=/docs/guides/tls/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidestls><span>Securing communication between Cortex components with TLS</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidessecurity-li><a href=/docs/guides/security/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidessecurity><span>Security</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesshuffle-sharding-li><a href=/docs/guides/shuffle-sharding/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesshuffle-sharding><span>Shuffle Sharding</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidestracing-li><a href=/docs/guides/tracing/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidestracing><span>Tracing</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguideszone-aware-replication-li><a href=/docs/guides/zone-aware-replication/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguideszone-aware-replication><span>Zone Aware Replication</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguideslimitations-li><a href=/docs/guides/limitations/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguideslimitations><span>Limitations</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsguidesglossary-li><a href=/docs/guides/glossary/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsguidesglossary><span>Glossary</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsapi-li><a href=/docs/api/ class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id=m-docsapi><span>HTTP API</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docsoperations-li><a href=/docs/operations/ title="Operating Cortex" class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id=m-docsoperations><span>Operations</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsoperationsquery-auditor-li><a href=/docs/operations/query-auditor/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsoperationsquery-auditor><span>Query Auditor (tool)</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsoperationsquery-tee-li><a href=/docs/operations/query-tee/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsoperationsquery-tee><span>Query Tee (service)</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsoperationsrequests-mirroring-to-secondary-cluster-li><a href=/docs/operations/requests-mirroring-to-secondary-cluster/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsoperationsrequests-mirroring-to-secondary-cluster><span>Requests mirroring to secondary cluster</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docsoperationsscaling-query-frontend-li><a href=/docs/operations/scaling-query-frontend/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsoperationsscaling-query-frontend><span>Scaling the Query Frontend</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docscase-studies-li><a href=/docs/case-studies/ class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id=m-docscase-studies><span>Case Studies</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docscase-studiesgojek-li><a href=/docs/case-studies/gojek/ title="How Gojek Is Leveraging Cortex to Keep Up with Its Ever-Growing Scale" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docscase-studiesgojek><span>Gojek</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docscase-studiesrewe-digital-li><a href=/docs/case-studies/rewe-digital/ title="How Cortex helped REWE digital ensure stability while scaling services during the Covid-19 pandemic" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docscase-studiesrewe-digital><span>REWE digital</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docscase-studiesbuoyant-cloud-li><a href=/docs/case-studies/buoyant-cloud/ title="Buoyant Cloud and Cortex: Standing on the shoulders of Linkerd and Prometheus" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docscase-studiesbuoyant-cloud><span>Buoyant Cloud</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-docsproposals-li><a href=/docs/proposals/ class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id=m-docsproposals><span>Proposals</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalsblocks-storage-bucket-index-li><a href=/docs/proposals/blocks-storage-bucket-index/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalsblocks-storage-bucket-index><span>Blocks storage bucket index</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalsblocks-storage-sharding-li><a href=/docs/proposals/blocks-storage-sharding/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalsblocks-storage-sharding><span>Blocks storage sharding</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalscross-tenant-query-federation-li><a href=/docs/proposals/cross-tenant-query-federation/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalscross-tenant-query-federation><span>Cross-Tenant Query Federation</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalstenant-deletion-li><a href=/docs/proposals/tenant-deletion/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalstenant-deletion><span>Deletion of Tenant Data from Blocks Storage</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalsdocumentation-versioning-li><a href=/docs/proposals/documentation-versioning/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalsdocumentation-versioning><span>Documentation Versioning</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalsgeneralize-modules-li><a href=/docs/proposals/generalize-modules/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalsgeneralize-modules><span>Generalize Modules Service to make it extensible</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalshttp-api-design-li><a href=/docs/proposals/http-api-design/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalshttp-api-design><span>HTTP API Design</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalsingesters-migration-li><a href=/docs/proposals/ingesters-migration/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalsingesters-migration><span>Migrating ingesters from chunks to blocks and back.</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalsparallel-compaction-li><a href=/docs/proposals/parallel-compaction/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalsparallel-compaction><span>Parallel Compaction by Time Interval</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalstenant-retention-li><a href=/docs/proposals/tenant-retention/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalstenant-retention><span>Retention of Tenant Data from Blocks Storage</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalsruler-tenant-federation-li><a href=/docs/proposals/ruler-tenant-federation/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalsruler-tenant-federation><span>Ruler Tenant Federation</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalsscalable-alertmanager-li><a href=/docs/proposals/scalable-alertmanager/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalsscalable-alertmanager><span>Scalable Alertmanager</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalsscalable-query-frontend-li><a href=/docs/proposals/scalable-query-frontend/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalsscalable-query-frontend><span>Scalable Query Frontend</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalsshuffle-sharding-and-zone-awareness-li><a href=/docs/proposals/shuffle-sharding-and-zone-awareness/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalsshuffle-sharding-and-zone-awareness><span>Shuffle sharding and zone awareness</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id=m-docsproposalsshuffle-sharding-on-the-read-path-li><a href=/docs/proposals/shuffle-sharding-on-the-read-path/ class="align-left pl-0 active td-sidebar-link td-sidebar-link__page" id=m-docsproposalsshuffle-sharding-on-the-read-path><span class=td-sidebar-nav-active-item>Shuffle sharding on the read path</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalssupport-metadata-api-li><a href=/docs/proposals/support-metadata-api/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalssupport-metadata-api><span>Support metadata API</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsproposalsblock-storage-time-series-deletion-li><a href=/docs/proposals/block-storage-time-series-deletion/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsproposalsblock-storage-time-series-deletion><span>Time Series Deletion from Blocks Storage</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docscontributing-li><a href=/docs/contributing/ class="align-left pl-0 td-sidebar-link td-sidebar-link__section" id=m-docscontributing><span>Contributing</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docscontributinggovernance-li><a href=/docs/contributing/governance/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docscontributinggovernance><span>Governance</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docscontributingdesign-patterns-and-code-conventions-li><a href=/docs/contributing/design-patterns-and-code-conventions/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docscontributingdesign-patterns-and-code-conventions><span>Design patterns and Code conventions</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docscontributinghow-to-run-the-website-locally-li><a href=/docs/contributing/how-to-run-the-website-locally/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docscontributinghow-to-run-the-website-locally><span>How to run the website locally</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docscontributinghow-to-upgrade-golang-version-li><a href=/docs/contributing/how-to-upgrade-golang-version/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docscontributinghow-to-upgrade-golang-version><span>How to upgrade Golang version</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docscontributinghow-integration-tests-work-li><a href=/docs/contributing/how-integration-tests-work/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docscontributinghow-integration-tests-work><span>How integration tests work</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docscontributinghow-to-update-the-build-image-li><a href=/docs/contributing/how-to-update-the-build-image/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docscontributinghow-to-update-the-build-image><span>How to update the build image</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child collapse" id=m-docscontributinghow-to-add-a-maintainer-li><a href=/docs/contributing/how-to-add-a-maintainer/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docscontributinghow-to-add-a-maintainer><span>How to add a maintainer</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsroadmap-li><a href=/docs/roadmap/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docsroadmap><span>Roadmap</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docschangelog-li><a href=/docs/changelog/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docschangelog><span>Changelog</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docscode-of-conduct-li><a href=/docs/code-of-conduct/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-docscode-of-conduct><span>Code of Conduct</span></a></li></ul></li></ul></nav></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"><a href=https://github.com/cortexproject/cortex/edit/master/docs/proposals/shuffle-sharding-on-the-read-path.md target=_blank><i class="fa fa-edit fa-fw"></i> Edit this page</a>
<a href="https://github.com/cortexproject/cortex/issues/new?title=Shuffle%20sharding%20on%20the%20read%20path" target=_blank><i class="fab fa-github fa-fw"></i> Create documentation issue</a>
<a href=https://github.com/cortexproject/cortex/issues/new target=_blank><i class="fas fa-tasks fa-fw"></i> Create project issue</a></div><div class=td-toc><nav id=TableOfContents><ul><li><a href=#background>Background</a></li><li><a href=#goals>Goals</a><ul><li><a href=#prerequisite-fix-subring-shuffling>Prerequisite: fix subring shuffling</a></li><li><a href=#query-frontend--queriers-shuffle-sharding>Query-frontend → Queriers shuffle sharding</a></li><li><a href=#how-querier-runs-query-frontend-jobs>How querier runs query-frontend jobs</a></li><li><a href=#proposal-1>Proposal</a></li><li><a href=#properties>Properties</a></li><li><a href=#implementation-notes>Implementation notes</a></li><li><a href=#configuration>Configuration</a></li><li><a href=#evaluated-alternatives>Evaluated alternatives</a></li></ul></li><li><a href=#querier--store-gateway-shuffle-sharding>Querier → Store-gateway shuffle sharding</a><ul><li><a href=#introduction>Introduction</a></li><li><a href=#proposal-2>Proposal</a></li><li><a href=#configuration-1>Configuration</a></li><li><a href=#implementation-notes-1>Implementation notes</a></li><li><a href=#evaluated-alternatives-1>Evaluated alternatives</a></li></ul></li><li><a href=#querier-ingesters-shuffle-sharding>Querier→ Ingesters shuffle sharding</a><ul><li><a href=#problem>Problem</a></li><li><a href=#proposal-use-only-the-information-contained-in-the-ring>Proposal: use only the information contained in the ring.</a></li><li><a href=#considered-alternative-1-ingesters-expose-list-of-tenants>Considered alternative #1: Ingesters expose list of tenants</a></li><li><a href=#considered-alternative-2-streaming-updates-from-ingesters-to-queriers>Considered alternative #2: streaming updates from ingesters to queriers</a></li></ul></li><li><a href=#ruler-sharding>Ruler sharding</a><ul><li><a href=#introduction-1>Introduction</a></li><li><a href=#proposal-4>Proposal</a></li><li><a href=#other-notes>Other notes</a></li></ul></li></ul></nav></div></aside><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><nav aria-label=breadcrumb class=td-breadcrumbs><ol class=breadcrumb><li class=breadcrumb-item><a href=/docs/>Documentation</a></li><li class=breadcrumb-item><a href=/docs/proposals/>Proposals</a></li><li class="breadcrumb-item active" aria-current=page><a href=/docs/proposals/shuffle-sharding-on-the-read-path/ aria-disabled=true class="btn-link disabled">Shuffle sharding on the read path</a></li></ol></nav><div class=td-content><h1>Shuffle sharding on the read path</h1><header class=article-meta></header><ul><li>Author: @pracucci, @tomwilkie, @pstibrany</li><li>Reviewers:</li><li>Date: August 2020</li><li>Status: Proposed, partially implemented</li></ul><h2 id=background>Background</h2><p>Cortex currently supports sharding of tenants to a subset of the ingesters on the write path <a href=https://github.com/cortexproject/cortex/pull/1947>PR</a>.</p><p>This feature is called “subring”, because it computes a subset of nodes registered to the hash ring. The aim of this feature is to improve isolation between tenants and reduce the number of tenants impacted by an outage.</p><p>This approach is similar to the techniques described in <a href=https://aws.amazon.com/builders-library/workload-isolation-using-shuffle-sharding/>Amazon’s Shuffle Sharding article</a>, but currently suffers from a non random selection of nodes (<em>proposed solution below</em>).</p><p>Cortex can be <strong>configured</strong> with a default subring size, and then it can be <a href=https://cortexmetrics.io/docs/configuration/configuration-file/#limits_config>customized on a per-tenant basis</a>. The per-tenant configuration is live reloaded during runtime and applied without restarting the Cortex process.</p><p>The subring sharding currently supports only the write-path. The read-path is not shuffle sharding aware. For example, an outage of more than one ingester with RF=3 will affect all tenants, or a particularly noisy tenant wrt queries has the ability to affect all tenants.</p><h2 id=goals>Goals</h2><p>The Cortex <strong>read path should support shuffle sharding to isolate</strong> the impact of an outage in the cluster. The shard size must be dynamically configurable on a per-tenant basis during runtime.</p><p>This deliverable involves introducing shuffle sharding in:</p><ul><li><strong>Query-frontend → Querier</strong> (for queries sharding) <a href=https://github.com/cortexproject/cortex/pull/3113>PR #3113</a></li><li><strong>Querier → Store-gateway</strong> (for blocks sharding) <a href=https://github.com/cortexproject/cortex/pull/3069>PR #3069</a></li><li><strong>Querier→ Ingesters</strong> (for queries on recent data)</li><li><strong>Ruler</strong> (for rule and alert evaluation)</li></ul><h3 id=prerequisite-fix-subring-shuffling>Prerequisite: fix subring shuffling</h3><p>The solution is implemented in <a href=https://github.com/cortexproject/cortex/pull/3090>https://github.com/cortexproject/cortex/pull/3090</a>.</p><h4 id=the-problem>The problem</h4><p>The subring is a subset of nodes that should be used for a specific tenant.</p><p>The current subring implementation doesn’t shuffle tenants across nodes. Given a tenant ID, it finds the first node owning the hash(tenant ID) token and then it picks N distinct consecutive nodes walking the ring clockwise.</p><p>For example, in a cluster with 6 nodes (numbered 1-6) and a replication factor of 3, three tenants (A, B, C) could have the following shards:</p><table><thead><tr><th>Tenant ID</th><th>Node 1</th><th>Node 2</th><th>Node 3</th><th>Node 4</th><th>Node 5</th><th>Node 6</th></tr></thead><tbody><tr><td>A</td><td>x</td><td>x</td><td>x</td><td></td><td></td><td></td></tr><tr><td>B</td><td></td><td>x</td><td>x</td><td>x</td><td></td><td></td></tr><tr><td>C</td><td></td><td></td><td>x</td><td>x</td><td>x</td><td></td></tr></tbody></table><h4 id=proposal>Proposal</h4><p>We propose to build the subring picking N distinct and random nodes registered in the ring, using the following algorithm:</p><ol><li>SID = tenant ID</li><li>SID = hash(SID)</li><li>Look for the node owning the token range containing FNV-1a(SID)</li><li>Loop to (2) until we’ve found N distinct nodes (where N is the shard size)</li></ol><p><em>hash() function to be decided. The required property is to be strong enough to not generate loops across multiple subsequent hashing of the previous hash.</em></p><h3 id=query-frontend--queriers-shuffle-sharding>Query-frontend → Queriers shuffle sharding</h3><p>Implemented in <a href=https://github.com/cortexproject/cortex/pull/3113>https://github.com/cortexproject/cortex/pull/3113</a>.</p><h3 id=how-querier-runs-query-frontend-jobs>How querier runs query-frontend jobs</h3><p>Today <strong>each</strong> querier connects to <strong>each</strong> query-frontend instance, and calls a single “Process” method via gRPC.</p><p>“Process” is a bi-directional streaming gRPC method – using the server-to-client stream for sending requests from query-frontend to the querier, and client-to-server stream for returning results from querier to the query-frontend. NB this is the opposite of what might be considered normal. Query-frontend scans all its queues with pending query requests, and picks a query to execute based on a fair schedule between tenants.</p><p>The query request is then sent to an idle querier worker over the stream opened in the Process method, and the query-frontend then waits for a response from querier. This loop repeats until querier disconnects.</p><h3 id=proposal-1>Proposal</h3><p>To support shuffle sharding, Query-Frontends will keep a list of connected Queriers, and randomly (but consistently between query-frontends) choose N of them to distribute requests to. When Query-Frontend looks for the next request to send to a given querier, it will only consider tenants that “belong” to the Querier.</p><p>To choose N Queriers for a tenant, we propose to use a simple algorithm:</p><ol><li>Sort all Queriers by their ID</li><li>SID = tenant ID</li><li>SID = hash(SID)</li><li>Pick the querier from the list of sorted queries with:<br>index = FNV-1a(SID) % number of Queriers</li><li>Loop to (3) until we’ve found N distinct queriers (where N is the shard size) and stop early if there aren’t enough queriers</li></ol><p><em>hash() function to be decided. The required property is to be strong enough to not generate loops across multiple subsequent hashing of the previous hash.</em></p><h3 id=properties>Properties</h3><ul><li><strong>Stability:</strong> this will produce the same result on all query-frontends as long as all queriers are connected to all query-frontends.</li><li><strong>Simplicity:</strong> no external dependencies.</li><li><strong>No consistent hashing:</strong> adding/removing queriers will cause “resharding” of tenants between queriers. While in general that’s not desirable property, queriers are stateless so it doesn’t seem to matter in this case.</li></ul><h3 id=implementation-notes>Implementation notes</h3><ul><li><strong>Caching:</strong> once this list of queriers to use for a tenant is computed in the query-frontend, it is cached in memory until queriers are added or removed. Per-tenant cache entries will have a TTL to discard tenants not “seen” since a while.</li><li><strong>Querier ID:</strong> Query-frontends currently don’t have any identity for queriers. We need to introduce sending of a unique ID (eg. hostname) by querier to query-frontend when it calls “Process” method.</li><li><strong>Backward-compatibility:</strong> when querier shuffle sharding is enabled, the system expects that both query-frontend and querier will run a compatible version. Cluster version upgrade will require to rollout new query-frontends and queriers first, and then enable shuffle sharding.</li><li><strong>UI:</strong> we propose to expose the current state of the query-frontend through a new endpoint which should display:<ul><li>Which querier are connected to the query-frontend</li><li>Are there any “old” queriers, that are receiving requests from all tenants?</li><li>Mapping of tenants to queriers. Note that this mapping may only be available for tenants with pending requests on given query-frontend, and therefore be very dynamic.</li></ul></li></ul><h3 id=configuration>Configuration</h3><ul><li><strong>Shard size</strong> will be configurable on a per-tenant basis via existing “runtime-configuration” mechanism (limits overrides). Changing a value for a tenant needs to invalidate cached per-tenant queriers.</li><li>Queriers shard size will be a different setting than then one used for writes.</li></ul><h3 id=evaluated-alternatives>Evaluated alternatives</h3><h4 id=use-the-subring>Use the subring</h4><p>An alternative option would be using the subring. This implies having queriers registering to the hash ring and query-frontend instances using the ring client to find the queriers subring for each tenant.</p><p>This solution looks adding more complexity without any actual benefit.</p><h4 id=change-query-frontend--querier-architecture>Change query-frontend → querier architecture</h4><p>Completely different approach would be to introduce a place where starting queriers would register (eg. DNS-based service discovery), and let query-frontends discover queriers from this central registry.</p><p>Possible benefit would be that queriers don’t need to initiate connection to all query-frontends, but query-frontends would only connect to queriers for which they have actual pending requests. However this would be a significant redesign of how query-frontend / querier communication works.</p><h2 id=querier--store-gateway-shuffle-sharding>Querier → Store-gateway shuffle sharding</h2><p>Implemented in <a href=https://github.com/cortexproject/cortex/pull/3069>https://github.com/cortexproject/cortex/pull/3069</a>.</p><h3 id=introduction>Introduction</h3><p>As of today, the store-gateway supports blocks sharding with customizable replication factor (defaults to 3). Blocks of a single tenant are sharded across all store-gateway instances and so to execute a query the querier may touch any store-gateway in the cluster.</p><p>The current sharding implementation is based on a <strong>hash ring</strong> formed by store-gateway instances.</p><h3 id=proposal-2>Proposal</h3><p>The proposed solution to add shuffle sharding support to the store-gateway is to <strong>leverage on the existing hash ring</strong> to build a per-tenant <strong>subring</strong>, which is then used both by the querier and store-gateway to know to which store-gateway a block belongs to.</p><h3 id=configuration-1>Configuration</h3><ul><li>Shuffle sharding can be enabled in the <strong>store-gateway configuration.</strong> It supports a <strong>default sharding factor,</strong> which is <strong>overridable on a per-tenant basis</strong> and live reloaded during runtime (using the existing limits config).</li><li>The querier already requires the store-gateway configuration when the blocks sharding is enabled. Similarly, when shuffle sharding is enabled the querier will require the store-gateway shuffle sharding configuration as well.</li></ul><h3 id=implementation-notes-1>Implementation notes</h3><p>When shuffle sharding is enabled:</p><ul><li>The <strong>store-gateway</strong> <code>syncUsersBlocks()</code> will build a tenant’s subring for each tenant found scanning the bucket and will skip any tenant not belonging to its shard.<br>Likewise, ShardingMetadataFilter will first build a <strong>tenant’s subring</strong> and then will use the existing logic to filter out blocks not belonging to store-gateway instance itself. The tenant ID can be read from the block’s meta.json.</li><li>The <strong>querier</strong> <code>blocksStoreReplicationSet.GetClientsFor()</code> will first build a <strong>tenant’s subring</strong> and then will use the existing logic to find out to which store-gateway instance each requested block belongs to.</li></ul><h3 id=evaluated-alternatives-1>Evaluated alternatives</h3><p><em>Given the store-gateways already form a ring and building the shuffle sharding based on the ring (like in the write path) doesn’t introduce extra operational complexity, we haven’t discussed alternatives.</em></p><h2 id=querier-ingesters-shuffle-sharding>Querier→ Ingesters shuffle sharding</h2><p>We’re currently discussing/evaluating different options.</p><h3 id=problem>Problem</h3><p>Cortex must guarantee query correctness; transiently incorrect results may be cached and returned forever. The main problem to solve when introducing ingesters shuffle sharding on the read path is to make sure that a querier fetch data from all ingesters having at least 1 sample for a given tenant.</p><p>The problem to solve is: how can a querier efficiently find which ingesters have data for a given tenant? Each option must consider the changing of the set of ingesters and the changing of each tenant’s subring size.</p><h3 id=proposal-use-only-the-information-contained-in-the-ring>Proposal: use only the information contained in the ring.</h3><p><em>This section describes an alternative approach. Discussion is still on-going.</em></p><p>The idea is for the queries to be able to deduce what ingesters could possibly hold data for a given tenant by just consulting the ring (and the per-tenant sub ring sizes). We posit that this is possible with only a single piece of extra information: a single timestamp per ingester saying when the ingester first joined the ring.</p><h4 id=scenario-ingester-scale-up>Scenario: ingester scale up</h4><p>When a new ingester is added to the ring, there will be a set of user subrings that see a change: an ingester being removed, and a new one being added. We need to guarantee that for some time period (the block flush interval), the ingester removed is also consulted for queries.</p><p>To do this, during the subring selection if we encounters an ingester added within the time period, we will add this to the subring but continue node selection as before - in effect, selecting an extra ingester:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#204a87;font-weight:700>var</span> <span style=color:#000;font-weight:700>(</span>
</span></span><span style=display:flex><span>    <span style=color:#000>subringSize</span>   <span style=color:#204a87;font-weight:700>int</span>
</span></span><span style=display:flex><span>    <span style=color:#000>selectedNodes</span> <span style=color:#000;font-weight:700>[]</span><span style=color:#000>Node</span>
</span></span><span style=display:flex><span>    <span style=color:#000>deadline</span>      <span style=color:#000;font-weight:700>=</span> <span style=color:#000>time</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>Now</span><span style=color:#000;font-weight:700>().</span><span style=color:#000>Add</span><span style=color:#000;font-weight:700>(</span><span style=color:#ce5c00;font-weight:700>-</span><span style=color:#000>flushWindow</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>for</span> <span style=color:#204a87>len</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>selectedNodes</span><span style=color:#000;font-weight:700>)</span> <span style=color:#000;font-weight:700>&lt;</span> <span style=color:#000>subringSize</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>    <span style=color:#000>token</span> <span style=color:#ce5c00;font-weight:700>:=</span> <span style=color:#000>random</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>Next</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>    <span style=color:#000>node</span> <span style=color:#ce5c00;font-weight:700>:=</span> <span style=color:#000>getNodeByToken</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>token</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>for</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>node</span> <span style=color:#000>in</span> <span style=color:#000>selectedNodes</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>            <span style=color:#000>node</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#000>node</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>Next</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>            <span style=color:#204a87;font-weight:700>continue</span>
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>node</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>Added</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>After</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>deadline</span><span style=color:#000;font-weight:700>)</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>            <span style=color:#000>subringSize</span><span style=color:#ce5c00;font-weight:700>++</span>
</span></span><span style=display:flex><span>            <span style=color:#000>selectedNodes</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>Add</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>node</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>            <span style=color:#000>node</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#000>node</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>Next</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>            <span style=color:#204a87;font-weight:700>continue</span>
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>        <span style=color:#000>selectedNodes</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>Add</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>node</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>break</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div><h4 id=scenario-ingester-scale-down>Scenario: ingester scale down</h4><p>When an ingester is permanently removed from the ring it will flush its data to the object store and the subrings containing the removed ingester will gain a “new” ingester. Queries consult the store and merge the results with those from the ingesters, so no data will be missed.</p><p>Queriers and store-gateways will discover newly flushed blocks on next sync (<code>-blocks-storage.bucket-store.sync-interval</code>, default 5 minutes).
Multiple ingesters should not be scaled-down within this interval.</p><p>To improve read-performance, queriers and rulers are usually configured with non-zero value of <code>-querier.query-store-after</code> option.
This option makes queriers and rulers to consult <strong>only</strong> ingesters when running queries within specified time window (eg. 12h).
During scale-down this needs to be lowered in order to let queriers and rulers use flushed blocks from the storage.</p><h4 id=scenario-increase-size-of-a-tenants-subring>Scenario: increase size of a tenant’s subring</h4><p>Node selection for subrings is stable - increasing the size of a subring is guaranteed to only add new nodes to it (and not remove any nodes). Hence, if a tenant’s subring is increase in size the queriers will notice the config change and start consulting the new ingester.</p><h4 id=scenario-decreasing-size-of-a-tenants-subring>Scenario: decreasing size of a tenant’s subring</h4><p>If a tenant’s subring decreases in size, there is currently no way for the queriers to know how big the ring was previously, and hence they will potentially miss an ingester with data for that tenant.</p><p>This is deemed an infrequent operation that we considered banning, but have a proposal for how we might make it possible:</p><p>The proposal is to have separate read subring and write subring size in the config. The read subring will not be allowed to be smaller than the write subring. When reducing the size of a tenant’s subring, operators must first reduce the write subring, and then two hours later when the blocks have been flushed, the read subring. In the majority of cases the read subring will not need to be specified, as it will default to the write subring size.</p><h3 id=considered-alternative-1-ingesters-expose-list-of-tenants>Considered alternative #1: Ingesters expose list of tenants</h3><p>A possible solution could be keeping in the querier an in-memory data structure to map each ingester to the list of tenants for which it has some data. This data structure would be constructed at querier startup, and then periodically updated, interpolating two information:</p><ol><li>The current state of the ring</li><li>The list of tenants directly exposed by each ingester (via a dedicated gRPC call)</li></ol><h4 id=scenario-new-querier-starts-up>Scenario: new querier starts up</h4><p>When a querier starts up and before getting ready:</p><ol><li>It scans all ingesters (discovered via the ring) and fetches the list of tenants for which each ingester has some data</li><li>For each found tenant (unique list of tenant IDs across all ingesters responses), the querier looks at the current state of the ring and adds to the map the list of ingesters currently assigned to the tenant shard, even if they don’t hold any data yet (because may start receiving series shortly)</li></ol><p>Then the querier watches the ingester ring and rebuilds the in-memory map whenever the ring topology changes.</p><h4 id=scenario-querier-receives-a-query-for-an-unknown-tenant>Scenario: querier receives a query for an unknown tenant</h4><p>A new tenant starts remote writing to the cluster. The querier doesn’t know it in its in-memory map, so it adds the tenant on the fly to the map just looking at the current state of the ring.</p><h4 id=scenario-ingester-scale-up--down>Scenario: ingester scale up / down</h4><p>When a new ingester is added / removed to / from the ring, the ring topology changes and queriers will update the in-memory map.</p><h4 id=scenario-per-tenant-shard-size-increases>Scenario: per-tenant shard size increases</h4><p>Queriers periodically (every 1m) reload the limits config file. When a tenant shard size change is detected, the querier updates the in-memory map for the affected tenant.</p><p><strong>Issue:</strong> some time series data may be missing in queries up to 1m.</p><h4 id=edge-case-queriers-notice-the-ring-topology-change-before-distributors>Edge case: queriers notice the ring topology change before distributors</h4><p>Consider the following scenario:</p><ol><li>Tenant A shard is composed by ingesters 1,2,3,4,5,6</li><li>Tenant A is remote writing 1 single series and gets replicated to ingester 1,2,3</li><li>The ring topology changes and tenant A shard is ingesters 1,2,3,7,8,9</li><li>Querier notices the ring topology change and updates the in-memory map. Given tenant A series were only on ingester 1,2,3, the querier maps tenant A to ingester 1,2,3 (because of what received from ingesters via gRPC) and 7,8,9 (because of the current state of the ring)</li><li>Distributor hasn’t updated the ring state yet</li><li>Tenant A remote writes 1 <strong>new</strong> series, which get replicated to 4,5,6</li><li>Distributor updates the ring state</li><li><strong>Race condition:</strong> querier will not know that ingesters 4,5,6 contains tenant A data until the next sync</li></ol><h3 id=considered-alternative-2-streaming-updates-from-ingesters-to-queriers>Considered alternative #2: streaming updates from ingesters to queriers</h3><p><em>This section describes an alternative approach.</em></p><h4 id=current-state>Current state</h4><p>As of today, queriers discover ingesters via the ring:</p><ul><li><strong>Ingesters</strong> register (and update their heartbeat timestamp) to the ring and queriers watch the ring, keeping an in-memory copy of the latest ingesters ring state.</li><li><strong>Queriers</strong> use the in-memory ring state to discover all ingesters that should be queried at query time.</li></ul><h4 id=proposal-3>Proposal</h4><p>The proposal is to expose a new gRPC endpoint on ingesters, which allows queriers to receive a stream of real time updates from ingesters about the tenants for which an ingester currently has time series data.</p><p>From the querier side:</p><ul><li>At <strong>startup</strong> the querier discovers all existing ingesters. For each ingester, the querier calls the ingester’s gRPC endpoint WatchTenants() (to be created). As soon as the WatchTenants() rpc is called, the ingester sends the entire set of tenants to the querier and then will send incremental updates (tenant added or removed from ingester) while the WatchTenants() stream connection is alive.</li><li>If the querier <strong>loses the connection</strong> to an ingester, it will automatically retry (with backoff) while the ingester is within the ring.</li><li>The querier <strong>watches the ring</strong> to discover added/removed ingesters. When an ingester is added, the querier adds the ingester to the pool of ingesters whose state should be monitored via WatchTenants().</li><li>At <strong>query time,</strong> the querier looks for all ingesters within the ring. There are two options:<ol><li>The querier knows the state of the ingester: the ingester will be queried only if it contains data for the query’s tenant.</li><li>The querier doesn’t know the state of the ingester (eg. because it was just registered to the ring and WatchTenants() hasn’t succeeded yet): the ingester will be queried anyway (correctness first).</li></ol></li><li>The querier will fine tune <a href=https://godoc.org/google.golang.org/grpc/keepalive>gRPC keepalive</a> settings to ensure a lost connection between the querier and ingester will be early detected and retried.</li></ul><h4 id=trade-offs>Trade-offs</h4><p>Pros:</p><ul><li>The querier logic, used to find ingesters for a tenant’s shard, <strong>does not require to watch the overrides</strong> config file (containing tenant shard size override). Watching the file in the querier is problematic because of introduced delays (ConfigMap update and Cortex file polling) which could lead to distributors apply changes before queriers.</li><li>The querier <strong>never uses the current state of the ring</strong> as a source of information to detect which ingesters have data for a specific tenant. This information comes directly from the ingesters themselves, which makes the implementation less likely to be subject to race conditions.</li></ul><p>Cons:</p><ul><li>Each querier needs to open a gRPC connection to each ingester. Given gRPC supports multiplexing, the underlying TCP connection could be the same connection used to fetch samples from ingesters at query time, basically having 1 single TCP connection between a querier and an ingester.</li><li>The “Edge case: queriers notice the ring topology change before distributors” described in attempt #1 can still happen in case of delays in the propagation of the state update from an ingester to queriers:<ul><li>Short delay: a short delay (few seconds) shouldn’t be a real problem. From the final user perspective, there’s no real difference between this edge case and a delay of few seconds in the ingestion path (eg. Prometheus remote write lagging behind few seconds). In the real case of Prometheus remote writing to Cortex, there’s no easy way to know if the latest samples are missing because has not been remote written yet by Prometheus or any delay in the propagation of this information between ingesters and queriers.</li><li>Long delay: in case of networking issue propagating the state update from an ingester to the querier, the gRPC keepalive will trigger (because of failed ping-pong) and the querier will remove the failing ingesters in-memory data, so the ingester will be always tried by the querier for any query, until the state update will be re-established.</li></ul></li></ul><h2 id=ruler-sharding>Ruler sharding</h2><h3 id=introduction-1>Introduction</h3><p>The ruler currently supports rule groups sharding across a pool of rulers. When sharding is enabled, rulers form a hash ring and each ruler uses the ring to check if it should evaluate a specific rule group.</p><p>At a polling interval (defaults to 1 minute), the ruler:</p><ul><li>List all the bucket objects to find all rule groups (listing is done specifying an empty delimiter so it return objects at any depth)</li><li>For each discovered rule group, the ruler hashes the object key and checks if it belongs to the range of tokens assigned to the ruler itself. If not, the rule group is discarded, otherwise it’s kept for evaluation.</li></ul><h3 id=proposal-4>Proposal</h3><p>We propose to introduce shuffle sharding in the ruler as well, leveraging on the already existing hash ring used by the current sharding implementation.</p><p>The <strong>configuration</strong> will be extended to allow to configure:</p><ul><li>Enable/disable shuffle sharding</li><li>Default shard size</li><li>Per-tenant overrides (reloaded at runtime)</li></ul><p>When shuffle sharding is enabled:</p><ul><li>The ruler lists (ListBucketV2) the tenants for which rule groups are stored in the bucket</li><li>The ruler filters out tenants not belonging to its shard</li><li>For each tenant belonging to its shard, the ruler does a ListBucketV2 call with the “<tenant-id>/” prefix and with empty delimiter to find all the rule groups, which are then evaluated in the ruler</li></ul><p>The ruler re-syncs the rule groups from the bucket whenever one of the following conditions happen:</p><ol><li>Periodic interval (configurable)</li><li>Ring topology changes</li><li>The configured shard size of a tenant has changed</li></ol><h3 id=other-notes>Other notes</h3><ul><li>The “subring” implementation is unoptimized. We will optimize it as part of this work to make sure no performance degradation is introduced when using the subring vs the normal ring.</li></ul></div></main></div></div><footer class="row td-box td-box--dark td-box--gradient td-box--height-auto text-white p-5"><div class="col-12 col-sm-4 pb-5 pb-sm-0"><img src=/images/cortex-stacked-white.png width=75px class="img-fluid float-left"></div><div class="col-12 col-sm-4 pb-5 pb-sm-0"><h6 class=font-weight-bold>Community</h6><ul class=list-unstyled><li><a href=https://slack.cncf.io class="text-reset text-white"><i class="fab fa-fw fa-slack"></i> Slack</a></li><li><a href=https://github.com/cortexproject/cortex class="text-reset text-white"><i class="fab fa-fw fa-github"></i> GitHub</a></li><li><a href=https://twitter.com/cortexmetrics class="text-reset text-white"><i class="fab fa-fw fa-twitter"></i> Twitter</a></li></ul></div><div class="col-12 col-sm-4"><h6 class=font-weight-bold>About</h6><p>Cortex is an OSS licensed project as Apache License 2.0</p></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.91798a335c881f1b6b805085ba4aa22d1dbd2b0b18d105d05189fa104ddae350.js integrity="sha256-kXmKM1yIHxtrgFCFukqiLR29KwsY0QXQUYn6EE3a41A=" crossorigin=anonymous></script></body></html>