<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cortex – Guides</title><link>/docs/guides/</link><description>Recent content in Guides on Cortex</description><generator>Hugo -- gohugo.io</generator><atom:link href="/docs/guides/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Running Cortex on Kubernetes</title><link>/docs/guides/running-cortex-on-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/running-cortex-on-kubernetes/</guid><description>
&lt;p>Because Cortex is designed to run multiple instances of each component
(ingester, querier, etc.), you probably want to automate the placement
and shepherding of these instances. Most users choose Kubernetes to do
this, but this is not mandatory.&lt;/p>
&lt;h2 id="configuration">Configuration&lt;/h2>
&lt;h3 id="resource-requests">Resource requests&lt;/h3>
&lt;p>If using Kubernetes, each container should specify resource requests
so that the scheduler can place them on a node with sufficient capacity.&lt;/p>
&lt;p>For example, an ingester might request:&lt;/p>
&lt;pre tabindex="0">&lt;code> resources:
requests:
cpu: 4
memory: 10Gi
&lt;/code>&lt;/pre>&lt;p>The specific values here should be adjusted based on your own
experiences running Cortex - they are very dependent on the rate of data
arriving and other factors such as series churn.&lt;/p>
&lt;h3 id="take-extra-care-with-ingesters">Take extra care with ingesters&lt;/h3>
&lt;p>Ingesters hold hours of timeseries data in memory; you can configure
Cortex to replicate the data, but you should take steps to avoid losing
all replicas at once:&lt;/p>
&lt;ul>
&lt;li>Don&amp;rsquo;t run multiple ingesters on the same node.&lt;/li>
&lt;li>Don&amp;rsquo;t run ingesters on preemptible/spot nodes.&lt;/li>
&lt;li>Spread out ingesters across racks / availability zones / whatever
applies in your datacenters.&lt;/li>
&lt;/ul>
&lt;p>You can ask Kubernetes to avoid running on the same node like this:&lt;/p>
&lt;pre tabindex="0">&lt;code> affinity:
podAntiAffinity:
preferredDuringSchedulingIgnoredDuringExecution:
- weight: 100
podAffinityTerm:
labelSelector:
matchExpressions:
- key: name
operator: In
values:
- ingester
topologyKey: &amp;#34;kubernetes.io/hostname&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Give plenty of time for an ingester to hand over or flush data to
store when shutting down; for Kubernetes, this looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code> terminationGracePeriodSeconds: 2400
&lt;/code>&lt;/pre>&lt;p>Ask Kubernetes to limit rolling updates to one ingester at a time and
signal the old one to stop before the new one is ready:&lt;/p>
&lt;pre tabindex="0">&lt;code> strategy:
rollingUpdate:
maxSurge: 0
maxUnavailable: 1
&lt;/code>&lt;/pre>&lt;p>Ingesters provide an HTTP hook to signal readiness when all is well;
this is valuable because it stops a rolling update at the first
problem:&lt;/p>
&lt;pre tabindex="0">&lt;code> readinessProbe:
httpGet:
path: /ready
port: 80
&lt;/code>&lt;/pre>&lt;p>We do not recommend configuring a liveness probe on ingesters -
killing them is a last resort and should not be left to a machine.&lt;/p></description></item><item><title>Docs: Getting started with gossiped ring</title><link>/docs/guides/getting-started-with-gossiped-ring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/getting-started-with-gossiped-ring/</guid><description>
&lt;p>Cortex requires a Key-Value (KV) store to store the ring. It can use traditional KV stores like Consul or Etcd,
but it can also build its own KV store on top of the memberlist library using a gossip algorithm.&lt;/p>
&lt;p>This short guide shows how to start Cortex in &lt;a href="/docs/architecture/">single-binary mode&lt;/a> with a memberlist-based ring.
To reduce the number of required dependencies in this guide, it will use &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> with no shipping to external stores.
The storage engine and external storage configuration are not dependent on the ring configuration.&lt;/p>
&lt;h2 id="single-binary-two-cortex-instances">Single-binary, two Cortex instances&lt;/h2>
&lt;p>For simplicity and to get started, we&amp;rsquo;ll run it as two instances of Cortex on the local computer.
We will use prepared configuration files (&lt;a href="../../configuration/single-process-config-blocks-gossip-1.yaml">file 1&lt;/a>, &lt;a href="../../configuration/single-process-config-blocks-gossip-2.yaml">file 2&lt;/a>), with no external
dependencies.&lt;/p>
&lt;p>Build Cortex first:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ go build ./cmd/cortex
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Run two instances of Cortex, each one with its own dedicated config file:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ ./cortex -config.file docs/configuration/single-process-config-blocks-gossip-1.yaml
$ ./cortex -config.file docs/configuration/single-process-config-blocks-gossip-2.yaml
&lt;/code>&lt;/pre>&lt;p>Download Prometheus and configure it to use our first Cortex instance for remote writes.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">remote_write&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">url&lt;/span>: &lt;span style="color:#ae81ff">http://localhost:9109/api/v1/push&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After starting Prometheus, it will now start pushing data to Cortex. The Distributor component in Cortex will
distribute incoming samples between the two instances.&lt;/p>
&lt;p>To query that data, you can configure your Grafana instance to use http://localhost:9109/prometheus (first Cortex) as a Prometheus data source.&lt;/p>
&lt;h2 id="how-it-works">How it works&lt;/h2>
&lt;p>The two instances we started earlier should be able to find each other via memberlist configuration (already present in the config files):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">memberlist&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># defaults to hostname&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">node_name&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Ingester 1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">bind_port&lt;/span>: &lt;span style="color:#ae81ff">7946&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">join_members&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">localhost:7947&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">abort_if_cluster_join_fails&lt;/span>: &lt;span style="color:#66d9ef">false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This tells memberlist to listen on port 7946, and connect to localhost:7947, which is the second instance.
Port numbers are reversed in the second configuration file.
We also need to configure &lt;code>node_name&lt;/code> and also ingester ID (&lt;code>ingester.lifecycler.id&lt;/code> field), because it defaults to hostname,
but we are running both Cortex instances on the same host.&lt;/p>
&lt;p>To make sure that both ingesters generate unique tokens, we configure &lt;code>join_after&lt;/code> and &lt;code>observe_period&lt;/code> to 10 seconds.
The first option tells Cortex to wait 10 seconds before joining the ring. This option is normally used to tell Cortex ingester
how long to wait for a potential token and data transfer from leaving ingester, but we also use it here to increase
the chance of finding other gossip peers. When Cortex joins the ring, it generates tokens and writes them to the ring.
If multiple Cortex instances do this at the same time, they can generate conflicting tokens. This can be a problem
when using a gossiped ring (instances may simply not see each other yet), so we use &lt;code>observe_period&lt;/code> to watch the ring for token conflicts.
If conflict is detected, new tokens are generated instead of conflicting tokens, and the observe period is restarted.
If no conflict is detected within the observe period, the ingester switches to the ACTIVE state.&lt;/p>
&lt;p>We are able to observe ring state on &lt;a href="http://localhost:9109/ring">http://localhost:9109/ring&lt;/a> and &lt;a href="http://localhost:9209/ring">http://localhost:9209/ring&lt;/a>.
The two instances may see slightly different views (eg. different timestamps), but they should converge to a common state soon, with both instances
being ACTIVE and ready to receive samples.&lt;/p>
&lt;h2 id="how-to-add-another-instance">How to add another instance?&lt;/h2>
&lt;p>To add another Cortex to the small cluster, copy &lt;code>docs/configuration/single-process-config-blocks-gossip-1.yaml&lt;/code> to a new file,
and make the following modifications. We assume that the third Cortex will run on the same machine again, so we change the node name and ingester ID as well. Here
is the annotated diff:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-diff" data-lang="diff">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">+ # These ports need to be unique.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">&lt;/span>&lt;span style="color:#f92672">- http_listen_port: 9109
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- grpc_listen_port: 9195
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&lt;/span>&lt;span style="color:#a6e22e">+ http_listen_port: 9309
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">+ grpc_listen_port: 9395
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ingester:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lifecycler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # Defaults to hostname, but we run both ingesters in this demonstration on the same machine.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- id: &amp;#34;Ingester 1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&lt;/span>&lt;span style="color:#a6e22e">+ id: &amp;#34;Ingester 3&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memberlist:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # defaults to hostname
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- node_name: &amp;#34;Ingester 1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&lt;/span>&lt;span style="color:#a6e22e">+ node_name: &amp;#34;Ingester 3&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> # bind_port needs to be unique
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- bind_port: 7946
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&lt;/span>&lt;span style="color:#a6e22e">+ bind_port: 7948
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">+# Directory names in the `blocks_storage` &amp;gt; `tsdb` config ending with `...1` to end with `...3`. This is to avoid different instances
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">+# writing in-progress data to the same directories.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">&lt;/span> blocks_storage:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tsdb:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- dir: /tmp/cortex/tsdb-ing1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&lt;/span>&lt;span style="color:#a6e22e">+ dir: /tmp/cortex/tsdb-ing3
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">&lt;/span> bucket_store:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">- sync_dir: /tmp/cortex/tsdb-sync-querier1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&lt;/span>&lt;span style="color:#a6e22e">+ sync_dir: /tmp/cortex/tsdb-sync-querier3
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We don&amp;rsquo;t need to change or add &lt;code>memberlist.join_members&lt;/code> list. This new instance will simply join to the second one (listening on port 7947), and
will discover other peers through it. When using Kubernetes, the suggested setup is to have a headless service pointing to all pods
that want to be part of the gossip cluster, and then point &lt;code>join_members&lt;/code> to this headless service.&lt;/p>
&lt;p>We also don&amp;rsquo;t need to change &lt;code>/tmp/cortex/storage&lt;/code> directory in the &lt;code>blocks_storage.filesystem.dir&lt;/code> field. This is the directory where all ingesters will
&amp;ldquo;upload&amp;rdquo; finished blocks. This can also be an S3 or GCP storage, but for simplicity, we use the local filesystem in this example.&lt;/p>
&lt;p>After these changes, we can start another Cortex instance using the modified configuration file. This instance will join the ring
and will start receiving samples after it enters the ACTIVE state.&lt;/p></description></item><item><title>Docs: Configuring Notification using Cortex Alertmanager</title><link>/docs/guides/alertmanager-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/alertmanager-configuration/</guid><description>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>Cortex Alertmanager notification setup follows mostly the syntax of Prometheus Alertmanager since it is based on the same codebase. The following is a description on how to load the configuration setup so that Alertmanager can use it for notification when an alert event happens.&lt;/p>
&lt;h3 id="configuring-the-cortex-alertmanager-storage-backend">Configuring the Cortex Alertmanager storage backend&lt;/h3>
&lt;p>With the introduction of Cortex 1.8, the storage backend config option shifted to the new pattern &lt;a href="https://github.com/cortexproject/cortex/pull/3888">#3888&lt;/a>. You can find the new configuration &lt;a href="/docs/configuration/configuration-file/#alertmanager_storage_config">here&lt;/a>&lt;/p>
&lt;p>Note that when using &lt;code>-alertmanager.sharding-enabled=true&lt;/code>, the following storage backends are not supported: &lt;code>local&lt;/code>, &lt;code>configdb&lt;/code>.&lt;/p>
&lt;p>When using the new configuration pattern, it is important that any of the old configuration pattern flags are unset (&lt;code>-alertmanager.storage&lt;/code>), as well as &lt;code>-&amp;lt;prefix&amp;gt;.configs.url&lt;/code>. This is because the old pattern still takes precedence over the new one. The old configuration pattern (&lt;code>-alertmanager.storage&lt;/code>) is marked as deprecated and will be removed by Cortex version 1.11. However, this change doesn&amp;rsquo;t apply to &lt;code>-alertmanager.storage.path&lt;/code> and &lt;code>-alertmanager.storage.retention&lt;/code>.&lt;/p>
&lt;h3 id="cortex-alertmanager-configuration">Cortex Alertmanager configuration&lt;/h3>
&lt;p>Cortex Alertmanager can be uploaded via Cortex &lt;a href="/docs/api/#set-alertmanager-configuration">Set Alertmanager configuration API&lt;/a> or using &lt;a href="https://github.com/cortexproject/cortex-tools">Cortex Tools&lt;/a>.&lt;/p>
&lt;p>Follow the instructions at the &lt;code>cortextool&lt;/code> link above to download or update to the latest version of the tool.&lt;/p>
&lt;p>To obtain the full help of how to use &lt;code>cortextool&lt;/code> for all commands and flags, use
&lt;code>cortextool --help-long&lt;/code>.&lt;/p>
&lt;p>The following example shows the steps to upload the configuration to Cortex &lt;code>Alertmanager&lt;/code> using &lt;code>cortextool&lt;/code>.&lt;/p>
&lt;h4 id="1-create-the-alertmanager-configuration-yaml-file">1. Create the Alertmanager configuration YAML file.&lt;/h4>
&lt;p>The following is &lt;code>amconfig.yml&lt;/code>, an example of a configuration for Cortex &lt;code>Alertmanager&lt;/code> to send notifications via email:&lt;/p>
&lt;pre tabindex="0">&lt;code>global:
# The smarthost and SMTP sender used for mail notifications.
smtp_smarthost: &amp;#39;localhost:25&amp;#39;
smtp_from: &amp;#39;alertmanager@example.org&amp;#39;
smtp_auth_username: &amp;#39;alertmanager&amp;#39;
smtp_auth_password: &amp;#39;password&amp;#39;
route:
# A default receiver.
receiver: send-email
receivers:
- name: send-email
email_configs:
- to: &amp;#39;someone@localhost&amp;#39;
&lt;/code>&lt;/pre>&lt;p>&lt;a href="https://grafana.com/blog/2020/02/25/step-by-step-guide-to-setting-up-prometheus-alertmanager-with-slack-pagerduty-and-gmail/#:~:text=To%20set%20up%20alerting%20in,to%20receive%20notifications%20from%20Alertmanager.">Example on how to set up Slack&lt;/a> to support receiving Alertmanager notifications.&lt;/p>
&lt;h4 id="2-upload-the-alertmanager-configuration">2. Upload the Alertmanager configuration&lt;/h4>
&lt;p>In this example, Cortex &lt;code>Alertmanager&lt;/code> is set to be available via localhost on port 8095 with user/org = 100.&lt;/p>
&lt;p>To upload the above configuration &lt;code>.yml&lt;/code> file with &lt;code>--key&lt;/code> to be your Basic Authentication or API key:&lt;/p>
&lt;pre tabindex="0">&lt;code>cortextool alertmanager load ./amconfig.yml \
--address=http://localhost:8095 \
--id=100 \
--key=&amp;lt;yourKey&amp;gt;
&lt;/code>&lt;/pre>&lt;p>If there is no error reported, the upload is successful.&lt;/p>
&lt;p>To upload the configuration for Cortex &lt;code>Alertmanager&lt;/code> using Cortex API and curl - see Cortex &lt;a href="https://cortexmetrics.io/docs/api/#set-alertmanager-configuration">Set Alertmanager configuration API&lt;/a>.&lt;/p>
&lt;h4 id="3-ensure-the-configuration-has-been-uploaded-successfully">3. Ensure the configuration has been uploaded successfully&lt;/h4>
&lt;pre tabindex="0">&lt;code>cortextool alertmanager get \
--address=http://localhost:8095 \
--id=100 \
--key=&amp;lt;yourKey&amp;gt;
&lt;/code>&lt;/pre></description></item><item><title>Docs: Authentication and Authorisation</title><link>/docs/guides/auth/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/auth/</guid><description>
&lt;p>All Cortex components take the tenant ID from a header &lt;code>X-Scope-OrgID&lt;/code>
on each request. A tenant (also called &amp;ldquo;user&amp;rdquo; or &amp;ldquo;org&amp;rdquo;) is the owner of
a set of series written to and queried from Cortex. All Cortex components
trust this value completely: if you need to protect your Cortex installation
from accidental or malicious calls, then you must add an additional layer
of protection.&lt;/p>
&lt;p>Typically, this means you run Cortex behind a reverse proxy, and you must
ensure that all callers, both machines sending data over the &lt;code>remote_write&lt;/code>
interface and humans sending queries from GUIs, supply credentials
which identify them and confirm they are authorised. When configuring the
&lt;code>remote_write&lt;/code> API in Prometheus, the user and password fields of HTTP Basic
auth, or Bearer token, can be used to convey the tenant ID and/or credentials.
See the &lt;a href="#cortex-tenant">Cortex-Tenant&lt;/a> section below for one way to solve this.&lt;/p>
&lt;p>In trusted environments, Prometheus can send the &lt;code>X-Scope-OrgID&lt;/code> header itself
by configuring the &lt;code>headers&lt;/code> field in its &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write">&lt;code>remote_write&lt;/code> configuration&lt;/a>:&lt;/p>
&lt;pre tabindex="0">&lt;code>remote_write:
- url: http://&amp;lt;cortex&amp;gt;/prometheus/api/v1/push
headers:
X-Scope-OrgID: &amp;lt;org&amp;gt;
&lt;/code>&lt;/pre>&lt;p>To disable the multi-tenant functionality, you can pass the argument
&lt;code>-auth.enabled=false&lt;/code> to every Cortex component, which will set the OrgID
to the string &lt;code>fake&lt;/code> for every request.&lt;/p>
&lt;p>Note that the tenant ID that is used to write the series to the datastore
should be the same as the one you use to query the data. If they don&amp;rsquo;t match,
you won&amp;rsquo;t see any data. As of now, you can&amp;rsquo;t see series from other tenants.&lt;/p>
&lt;p>For more information regarding the tenant ID limits, refer to: &lt;a href="/docs/guides/limitations/#tenant-id-naming">Tenant ID limitations&lt;/a>&lt;/p>
&lt;h3 id="cortex-tenant">Cortex-Tenant&lt;/h3>
&lt;p>One way to add &lt;code>X-Scope-OrgID&lt;/code> to Prometheus requests is to use a &lt;a href="https://github.com/blind-oracle/cortex-tenant">cortex-tenant&lt;/a>
proxy which is able to extract the tenant ID from Prometheus labels.&lt;/p>
&lt;p>It can be placed between Prometheus and Cortex and will search for a predefined
label and use its value as &lt;code>X-Scope-OrgID&lt;/code> header when proxying the timeseries to Cortex.&lt;/p>
&lt;p>This can help to run Cortex in a trusted environment where you want to separate your metrics
into distinct namespaces by some criteria (e.g. teams, applications, etc.).&lt;/p>
&lt;p>Be advised that &lt;strong>cortex-tenant&lt;/strong> is a third-party community project and it&amp;rsquo;s not maintained by the Cortex team.&lt;/p></description></item><item><title>Docs: Capacity Planning</title><link>/docs/guides/capacity-planning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/capacity-planning/</guid><description>
&lt;p>&lt;em>This doc is likely out of date. It should be updated for blocks storage.&lt;/em>&lt;/p>
&lt;p>You will want to estimate how many nodes are required, how many of
each component to run, and how much storage space will be required.
In practice, these will vary greatly depending on the metrics being
sent to Cortex.&lt;/p>
&lt;p>Some key parameters are:&lt;/p>
&lt;ol>
&lt;li>The number of active series. If you have Prometheus already, you
can query &lt;code>prometheus_tsdb_head_series&lt;/code> to see this number.&lt;/li>
&lt;li>Sampling rate, e.g. a new sample for each series every minute
(the default Prometheus &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/">scrape_interval&lt;/a>).
Multiply this by the number of active series to get the
total rate at which samples will arrive at Cortex.&lt;/li>
&lt;li>The rate at which series are added and removed. This can be very
high if you monitor objects that come and go - for example, if you run
thousands of batch jobs lasting a minute or so and capture metrics
with a unique ID for each one. &lt;a href="https://www.robustperception.io/using-tsdb-analyze-to-investigate-churn-and-cardinality">Read how to analyse this on
Prometheus&lt;/a>.&lt;/li>
&lt;li>How compressible the time-series data are. If a metric stays at
the same value constantly, then Cortex can compress it very well, so
12 hours of data sampled every 15 seconds would be around 2KB. On
the other hand, if the value jumps around a lot, it might take 10KB.
There are not currently any tools available to analyse this.&lt;/li>
&lt;li>How long you want to retain data for, e.g. 1 month or 2 years.&lt;/li>
&lt;/ol>
&lt;p>Other parameters which can become important if you have particularly
high values:&lt;/p>
&lt;ol start="6">
&lt;li>Number of different series under one metric name.&lt;/li>
&lt;li>Number of labels per series.&lt;/li>
&lt;li>Rate and complexity of queries.&lt;/li>
&lt;/ol>
&lt;p>Now, some rules of thumb:&lt;/p>
&lt;ol>
&lt;li>Each million series in an ingester takes 15GB of RAM. The total number
of series in ingesters is the number of active series times the
replication factor. This is with the default of 12-hour chunks - RAM
required will reduce if you set &lt;code>-ingester.max-chunk-age&lt;/code> lower
(trading off more back-end database I/O).
There are some additional considerations for planning for ingester memory usage.
&lt;ol>
&lt;li>Memory increases during write-ahead log (WAL) replay, &lt;a href="https://github.com/prometheus/prometheus/issues/6934#issuecomment-726039115">See Prometheus issue #6934&lt;/a>. If you do not have enough memory for WAL replay, the ingester will not be able to restart successfully without intervention.&lt;/li>
&lt;li>Memory temporarily increases during resharding since timeseries are temporarily on both the new and old ingesters. This means you should scale up the number of ingesters before memory utilization is too high, otherwise you will not have the headroom to account for the temporary increase.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Each million series (including churn) consumes 15GB of chunk
storage and 4GB of index, per day (so multiply by the retention
period).&lt;/li>
&lt;li>The distributors’ CPU utilization depends on the specific Cortex cluster
setup, while they don&amp;rsquo;t need much RAM. Typically, distributors are capable
of processing between 20,000 and 100,000 samples/sec with 1 CPU core. It&amp;rsquo;s also
highly recommended to configure Prometheus &lt;code>max_samples_per_send&lt;/code> to 1,000
samples, in order to reduce the distributors’ CPU utilization given the same
total samples/sec throughput.&lt;/li>
&lt;/ol>
&lt;p>If you turn on compression between distributors and ingesters (for
example, to save on inter-zone bandwidth charges at AWS/GCP), they will use
significantly more CPU (approx. 100% more for distributor and 50% more
for ingester).&lt;/p></description></item><item><title>Docs: Config for horizontally scaling the Ruler</title><link>/docs/guides/ruler-sharding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/ruler-sharding/</guid><description>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>One option to scale the ruler is by scaling it horizontally. However, with multiple ruler instances running, they will need to coordinate to determine which instance will evaluate which rule. Similar to the ingesters, the rulers establish a hash ring to divide up the responsibilities of evaluating rules.&lt;/p>
&lt;h2 id="config">Config&lt;/h2>
&lt;p>In order to enable sharding in the ruler, the following flag needs to be set:&lt;/p>
&lt;pre tabindex="0">&lt;code> -ruler.enable-sharding=true
&lt;/code>&lt;/pre>&lt;p>In addition, the ruler requires its own ring to be configured, for instance:&lt;/p>
&lt;pre tabindex="0">&lt;code> -ruler.ring.consul.hostname=consul.dev.svc.cluster.local:8500
&lt;/code>&lt;/pre>&lt;p>The only configuration that is required is to enable sharding and configure a key-value store. From there, the rulers will shard and handle the division of rules automatically.&lt;/p>
&lt;p>Unlike ingesters, rulers do not hand over responsibility: all rules are re-sharded randomly every time a ruler is added to or removed from the ring.&lt;/p>
&lt;h2 id="ruler-storage">Ruler Storage&lt;/h2>
&lt;p>The ruler supports six kinds of storage (configdb, Azure, GCS, S3, Swift, local). Most kinds of storage work with the sharded ruler configuration in an obvious way, i.e. configure all rulers to use the same backend.&lt;/p>
&lt;p>The local implementation reads &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/">Prometheus recording rules&lt;/a> off the local filesystem. This is a read-only backend that does not support the creation and deletion of rules through &lt;a href="/docs/api/#ruler">the API&lt;/a>. Despite the fact that it reads the local filesystem, this method can still be used in a sharded ruler configuration if the operator takes care to load the same rules to every ruler. For instance, this could be accomplished by mounting a &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/">Kubernetes ConfigMap&lt;/a> onto every ruler pod.&lt;/p>
&lt;p>A typical local config may look something like:&lt;/p>
&lt;pre tabindex="0">&lt;code> -ruler-storage.backend=local
-ruler-storage.local.directory=/tmp/cortex/rules
&lt;/code>&lt;/pre>&lt;p>With the above configuration, the ruler would expect the following layout:&lt;/p>
&lt;pre tabindex="0">&lt;code>/tmp/cortex/rules/&amp;lt;tenant id&amp;gt;/rules1.yaml
/rules2.yaml
&lt;/code>&lt;/pre>&lt;p>Yaml files are expected to be in the &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/#recording-rules">Prometheus format&lt;/a>.&lt;/p></description></item><item><title>Docs: Config for sending HA Pairs data to Cortex</title><link>/docs/guides/ha-pair-handling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/ha-pair-handling/</guid><description>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>You can have more than a single Prometheus monitoring and ingesting the same metrics for redundancy. Cortex already does replication for redundancy, and it doesn&amp;rsquo;t make sense to ingest the same data twice. So in Cortex, we made sure we can dedupe the data we receive from HA Pairs of Prometheus. We do this via the following:&lt;/p>
&lt;p>Assume that there are two teams, each running their own Prometheus, monitoring different services. Let&amp;rsquo;s call the Prometheus T1 and T2. Now, if the teams are running HA pairs, let&amp;rsquo;s call the individual Prometheus, T1.a, T1.b, and T2.a and T2.b.&lt;/p>
&lt;p>In Cortex, we make sure we only ingest from one of T1.a and T1.b, and only from one of T2.a and T2.b. We do this by electing a leader replica for each cluster of Prometheus. For example, in the case of T1, let it be T1.a. As long as T1.a is the leader, we drop the samples sent by T1.b. And if Cortex sees no new samples from T1.a for a short period (30s by default), it&amp;rsquo;ll switch the leader to be T1.b.&lt;/p>
&lt;p>This means if T1.a goes down for a few minutes, Cortex&amp;rsquo;s HA sample handling will have switched and elected T1.b as the leader. This failover timeout is what enables us to only accept samples from a single replica at a time, but ensure we don&amp;rsquo;t drop too much data in case of issues. Note that with the default scrape period of 15s, and the default timeouts in Cortex, in most cases, you&amp;rsquo;ll only lose a single scrape of data in the case of a leader election failover. For any rate queries, the rate window should be at least 4x the scrape period to account for any of these failover scenarios, for example, with the default scrape period of 15s, then you should calculate rates over at least 1m periods.&lt;/p>
&lt;p>Now we do the same leader election process for T2.&lt;/p>
&lt;h2 id="config">Config&lt;/h2>
&lt;h3 id="client-side">Client Side&lt;/h3>
&lt;p>So for Cortex to achieve this, we need 2 identifiers for each process, one identifier for the cluster (T1 or T2, etc.) and one identifier to identify the replica in the cluster (a or b). The easiest way to do this is by setting external labels; the default labels are &lt;code>cluster&lt;/code> and &lt;code>__replica__&lt;/code>. For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>cluster: prom-team1
__replica__: replica1 (or pod-name)
&lt;/code>&lt;/pre>&lt;p>and&lt;/p>
&lt;pre tabindex="0">&lt;code>cluster: prom-team1
__replica__: replica2
&lt;/code>&lt;/pre>&lt;p>Note: These are external labels and have nothing to do with remote_write config.&lt;/p>
&lt;p>These two label names are configurable per-tenant within Cortex and should be set to something sensible. For example, the cluster label is already used by some workloads, and you should set the label to be something else that uniquely identifies the cluster. Good examples for this label-name would be &lt;code>team&lt;/code>, &lt;code>cluster&lt;/code>, &lt;code>prometheus&lt;/code>, etc.&lt;/p>
&lt;p>The replica label should be set so that the value for each prometheus is unique in that cluster. Note: Cortex drops this label when ingesting data but preserves the cluster label. This way, your timeseries won&amp;rsquo;t change when replicas change.&lt;/p>
&lt;h3 id="server-side">Server Side&lt;/h3>
&lt;p>The minimal configuration requires:&lt;/p>
&lt;ul>
&lt;li>Enabling the HA tracker via &lt;code>-distributor.ha-tracker.enable=true&lt;/code> CLI flag (or its YAML config option)&lt;/li>
&lt;li>Configuring the KV store for the ring (See: &lt;a href="/docs/configuration/arguments/#ringha-tracker-store">Ring/HA Tracker Store&lt;/a>). Only Consul and etcd are currently supported. Multi should be used for migration purposes only.&lt;/li>
&lt;li>Setting the limits configuration to accept samples via &lt;code>-distributor.ha-tracker.enable-for-all-users&lt;/code> (or its YAML config option).&lt;/li>
&lt;/ul>
&lt;p>The following configuration snippet shows an example of the HA tracker config via YAML config file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">accept_ha_samples&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">distributor&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ha_tracker&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">enable_ha_tracker&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kvstore&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">[store&lt;/span>: &lt;span style="color:#ae81ff">&amp;lt;string&amp;gt; | default = &amp;#34;consul&amp;#34;]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">[consul | etcd&lt;/span>: &lt;span style="color:#ae81ff">&amp;lt;config&amp;gt;]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For further configuration file documentation, see the &lt;a href="/docs/configuration/configuration-file/#distributor_config">distributor section&lt;/a> and &lt;a href="/docs/configuration/arguments/#ringha-tracker-store">Ring/HA Tracker Store&lt;/a>.&lt;/p>
&lt;p>For flag configuration, see the &lt;a href="/docs/configuration/arguments/#ha-tracker">distributor flags&lt;/a> having &lt;code>ha-tracker&lt;/code> in them.&lt;/p>
&lt;h2 id="remote-read">Remote Read&lt;/h2>
&lt;p>If you plan to use remote_read, you can&amp;rsquo;t have the &lt;code>__replica__&lt;/code> label in the
external section. Instead, you will need to add it only on the remote_write
section of your prometheus.yml.&lt;/p>
&lt;pre tabindex="0">&lt;code>global:
external_labels:
cluster: prom-team1
remote_write:
- url: https://cortex/api/v1/push
write_relabel_configs:
- target_label: __replica__
replacement: 1
&lt;/code>&lt;/pre>&lt;p>and&lt;/p>
&lt;pre tabindex="0">&lt;code>global:
external_labels:
cluster: prom-team1
remote_write:
- url: https://cortex/api/v1/push
write_relabel_configs:
- target_label: __replica__
replacement: replica2
&lt;/code>&lt;/pre>&lt;p>When Prometheus is executing remote read queries, it will add the external
labels to the query. In this case, if it asks for the &lt;code>__replica__&lt;/code> label,
Cortex will not return any data.&lt;/p>
&lt;p>Therefore, the &lt;code>__replica__&lt;/code> label should only be added for remote write.&lt;/p>
&lt;h2 id="accept-multiple-ha-pairs-in-single-request">Accept multiple HA pairs in single request&lt;/h2>
&lt;p>Let&amp;rsquo;s assume there are two teams (T1 and T2), and each team operates two Prometheus for the HA (T1.a, T1.b for T1 and
T2.a, T2.b for T2).
They want to operate another Prometheus, receiving whole Prometheus requests and sending write request to the
Distributor.&lt;/p>
&lt;p>The write request flow is as follows: T1.a, T1.b, T2.a, T2.b -&amp;gt; Prometheus -&amp;gt; Distributor which means the Distributor&amp;rsquo;s
incoming write request contains time series of T1.a, T1.b, T2.a, and T2.b.
In other words, there are two HA pairs in a single write request, and the expected push result is to accept each
Prometheus leader replicas (example: T1.a, T2.b for each team).&lt;/p>
&lt;h2 id="config-1">Config&lt;/h2>
&lt;h3 id="client-side-1">Client side&lt;/h3>
&lt;p>The client setting is the same as a single HA pair.
For example:&lt;/p>
&lt;p>For T1.a&lt;/p>
&lt;pre tabindex="0">&lt;code>cluster: prom-team1
__replica__: replica1 (or pod-name)
&lt;/code>&lt;/pre>&lt;p>For T1.b&lt;/p>
&lt;pre tabindex="0">&lt;code>cluster: prom-team1
__replica__: replica2 (or pod-name)
&lt;/code>&lt;/pre>&lt;p>For T2.a&lt;/p>
&lt;pre tabindex="0">&lt;code>cluster: prom-team2
__replica__: replica1 (or pod-name)
&lt;/code>&lt;/pre>&lt;p>For T2.b&lt;/p>
&lt;pre tabindex="0">&lt;code>cluster: prom-team2
__replica__: replica2 (or pod-name)
&lt;/code>&lt;/pre>&lt;h3 id="server-side-1">Server side&lt;/h3>
&lt;p>One additional setting is needed to accept multiple HA pairs; it is enabled via
&lt;code>--experimental.distributor.ha-tracker.mixed-ha-samples=true&lt;/code> (or its YAML config option).&lt;/p>
&lt;p>The following configuration snippet shows an example of accepting multiple HA pairs config via the YAML config file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">accept_ha_samples&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">accept_mixed_ha_samples&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">distributor&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ha_tracker&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">enable_ha_tracker&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kvstore&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">[ store&lt;/span>: &lt;span style="color:#ae81ff">&amp;lt;string&amp;gt; | default = &amp;#34;consul&amp;#34; ]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">[ consul | etcd&lt;/span>: &lt;span style="color:#ae81ff">&amp;lt;config&amp;gt; ]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For further configuration file documentation, see
the &lt;a href="/docs/configuration/configuration-file/#limits_config">limits section&lt;/a>.&lt;/p></description></item><item><title>Docs: Encryption at Rest</title><link>/docs/guides/encryption-at-rest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/encryption-at-rest/</guid><description>
&lt;p>Cortex supports data encryption at rest for some storage backends.&lt;/p>
&lt;h2 id="s3">S3&lt;/h2>
&lt;p>The Cortex S3 client supports the following server-side encryption (SSE) modes:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html">SSE-S3&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html">SSE-KMS&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="blocks-storage">Blocks storage&lt;/h3>
&lt;p>The &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> S3 server-side encryption can be configured as follows.&lt;/p>
&lt;h3 id="s3_sse_config">&lt;code>s3_sse_config&lt;/code>&lt;/h3>
&lt;p>The &lt;code>s3_sse_config&lt;/code> configures the S3 server-side encryption.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">sse&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Enable AWS Server Side Encryption. Supported values: SSE-KMS, SSE-S3.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># CLI flag: -&amp;lt;prefix&amp;gt;.s3.sse.type&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">[type&lt;/span>: &lt;span style="color:#ae81ff">&amp;lt;string&amp;gt; | default = &amp;#34;&amp;#34;]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># KMS Key ID used to encrypt objects in S3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># CLI flag: -&amp;lt;prefix&amp;gt;.s3.sse.kms-key-id&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">[kms_key_id&lt;/span>: &lt;span style="color:#ae81ff">&amp;lt;string&amp;gt; | default = &amp;#34;&amp;#34;]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># KMS Encryption Context used for object encryption. It expects JSON formatted&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># string.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># CLI flag: -&amp;lt;prefix&amp;gt;.s3.sse.kms-encryption-context&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">[kms_encryption_context&lt;/span>: &lt;span style="color:#ae81ff">&amp;lt;string&amp;gt; | default = &amp;#34;&amp;#34;]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="ruler">Ruler&lt;/h3>
&lt;p>The ruler S3 server-side encryption can be configured similarly to the blocks storage. The per-tenant overrides are supported when using the storage backend configurable the &lt;code>-ruler-storage.&lt;/code> flag prefix (or their respective YAML config options).&lt;/p>
&lt;h3 id="alertmanager">Alertmanager&lt;/h3>
&lt;p>The alertmanager S3 server-side encryption can be configured similarly to the blocks storage. The per-tenant overrides are supported when using the storage backend configurable the &lt;code>-alertmanager-storage.&lt;/code> flag prefix (or their respective YAML config options).&lt;/p>
&lt;h3 id="per-tenant-config-overrides">Per-tenant config overrides&lt;/h3>
&lt;p>The S3 client used by the blocks storage, ruler, and alertmanager supports S3 SSE config overrides on a per-tenant basis, using the &lt;a href="/docs/configuration/arguments/#runtime-configuration-file">runtime configuration file&lt;/a>.
The following settings can be overridden for each tenant:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;code>s3_sse_type&lt;/code>&lt;/strong>&lt;br />
S3 server-side encryption type. It must be set to enable the SSE config override for a given tenant.&lt;/li>
&lt;li>&lt;strong>&lt;code>s3_sse_kms_key_id&lt;/code>&lt;/strong>&lt;br />
S3 server-side encryption KMS Key ID. Ignored if the SSE type override is not set or the type is not &lt;code>SSE-KMS&lt;/code>.&lt;/li>
&lt;li>&lt;strong>&lt;code>s3_sse_kms_encryption_context&lt;/code>&lt;/strong>&lt;br />
S3 server-side encryption KMS encryption context. If unset and the key ID override is set, the encryption context will not be provided to S3. Ignored if the SSE type override is not set or the type is not &lt;code>SSE-KMS&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="other-storages">Other storages&lt;/h2>
&lt;p>Other storage backends may support encryption at rest, configuring it directly at the storage level.&lt;/p></description></item><item><title>Docs: Ingesters rolling updates</title><link>/docs/guides/ingesters-rolling-updates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/ingesters-rolling-updates/</guid><description>
&lt;p>Cortex &lt;a href="/docs/architecture/#ingester">ingesters&lt;/a> are semi-stateful.
A running ingester holds several hours of time series data in memory before they&amp;rsquo;re flushed to the long-term storage.
When an ingester shuts down because of a rolling update or maintenance, the in-memory data must not be discarded in order to avoid any data loss.&lt;/p>
&lt;p>The Cortex &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> requires ingesters to run with a persistent disk where the TSDB WAL and blocks are stored (eg. a StatefulSet when deployed on Kubernetes).&lt;/p>
&lt;p>During a rolling update, the leaving ingester closes the open TSDBs, synchronizes the data to disk (&lt;code>fsync&lt;/code>), and releases the disk resources.
The new ingester, which is expected to reuse the same disk as the leaving one, will replay the TSDB WAL on startup to load back into memory the time series that have not been compacted into a block yet.&lt;/p></description></item><item><title>Docs: Ingesters scaling up and down</title><link>/docs/guides/ingesters-scaling-up-and-down/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/ingesters-scaling-up-and-down/</guid><description>
&lt;p>This guide explains how to scale up and down ingesters.&lt;/p>
&lt;p>If you&amp;rsquo;re looking for how to run ingesters rolling updates, please refer to the &lt;a href="/docs/guides/ingesters-rolling-updates/">dedicated guide&lt;/a>._&lt;/p>
&lt;h2 id="scaling-up">Scaling up&lt;/h2>
&lt;p>Adding more ingesters to a Cortex cluster is considered a safe operation. When a new ingester starts, it will register to the &lt;a href="/docs/architecture/#the-hash-ring">hash ring&lt;/a> and the distributors will reshard received series accordingly.
Ingesters that were previously receiving those series will see data stop arriving and will consider those series &amp;ldquo;idle&amp;rdquo;.&lt;/p>
&lt;p>If you run with &lt;code>-distributor.shard-by-all-labels=false&lt;/code> (the default), before adding a second ingester, you have to wait until data has migrated from idle series to the back-end store; otherwise, you will see gaps in queries. This will happen after the next &amp;ldquo;head compaction&amp;rdquo; (typically every 2 hours).
If you have set &lt;code>-querier.query-store-after&lt;/code>, then that is also a minimum time you have to wait before adding a second ingester.&lt;/p>
&lt;p>If you run with &lt;code>-distributor.shard-by-all-labels=true&lt;/code>,
no special care is required to take when scaling up ingesters.&lt;/p>
&lt;h2 id="scaling-down">Scaling down&lt;/h2>
&lt;p>A running ingester holds several hours of time series data in memory before they’re flushed to the long-term storage. When an ingester shuts down because of a scale down operation, the in-memory data must not be discarded in order to avoid any data loss.&lt;/p>
&lt;p>Ingesters don’t flush series to blocks at shutdown by default. However, Cortex ingesters expose an API endpoint &lt;a href="/docs/api/#shutdown">&lt;code>/shutdown&lt;/code>&lt;/a> that can be called to flush series to blocks and upload blocks to the long-term storage before the ingester terminates.&lt;/p>
&lt;p>Even if ingester blocks are compacted and shipped to the storage at shutdown, it takes some time for queriers and store-gateways to discover the newly uploaded blocks. This is due to the fact that the blocks storage runs a periodic scanning of the storage bucket to discover blocks. If two or more ingesters are scaled down in a short period of time, queriers may miss some data at query time due to series that were stored in the terminated ingesters but their blocks haven’t been discovered yet.&lt;/p>
&lt;h3 id="new-gradual-scaling-approach-recommended">New Gradual Scaling Approach (Recommended)&lt;/h3>
&lt;p>Starting with Cortex 1.19.0, a new &lt;strong>READONLY&lt;/strong> state for ingesters was introduced that enables gradual, safe scaling down without data loss or performance impact. This approach eliminates the need for complex configuration changes and allows for more flexible scaling operations.&lt;/p>
&lt;h4 id="how-the-readonly-state-works">How the READONLY State Works&lt;/h4>
&lt;p>The READONLY state allows ingesters to:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Stop accepting new writes&lt;/strong> - Push requests will be rejected and redistributed to other ingesters&lt;/li>
&lt;li>&lt;strong>Continue serving queries&lt;/strong> - Existing data remains available for queries, maintaining performance&lt;/li>
&lt;li>&lt;strong>Gradually age out data&lt;/strong> - As time passes, data naturally ages out according to your retention settings&lt;/li>
&lt;li>&lt;strong>Be safely removed&lt;/strong> - Once data has aged out, ingesters can be terminated without any impact&lt;/li>
&lt;/ul>
&lt;h4 id="step-by-step-scaling-process">Step-by-Step Scaling Process&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Set ingesters to READONLY mode&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Transition ingester to READONLY state&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -X POST http://ingester-1:8080/ingester/mode -d &lt;span style="color:#e6db74">&amp;#39;{&amp;#34;mode&amp;#34;: &amp;#34;READONLY&amp;#34;}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -X POST http://ingester-2:8080/ingester/mode -d &lt;span style="color:#e6db74">&amp;#39;{&amp;#34;mode&amp;#34;: &amp;#34;READONLY&amp;#34;}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -X POST http://ingester-3:8080/ingester/mode -d &lt;span style="color:#e6db74">&amp;#39;{&amp;#34;mode&amp;#34;: &amp;#34;READONLY&amp;#34;}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;strong>Monitor data aging&lt;/strong> (Optional but recommended)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Check user statistics and loaded blocks on the ingester&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl http://ingester-1:8080/ingester/all_user_stats
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;strong>Wait for safe removal window&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Immediate removal&lt;/strong> (after step 1): Safe once queries no longer need the ingester&amp;rsquo;s data&lt;/li>
&lt;li>&lt;strong>Conservative approach&lt;/strong>: Wait for &lt;code>querier.query-ingesters-within&lt;/code> duration (e.g., 5 hours)&lt;/li>
&lt;li>&lt;strong>Complete data aging&lt;/strong>: Wait for full retention period to ensure all blocks are removed&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Remove ingesters&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Terminate the ingester processes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl delete pod ingester-1 ingester-2 ingester-3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h4 id="timeline-example">Timeline Example&lt;/h4>
&lt;p>For a cluster with &lt;code>querier.query-ingesters-within=5h&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>T0&lt;/strong>: Set ingesters 5, 6, 7 to READONLY state&lt;/li>
&lt;li>&lt;strong>T1&lt;/strong>: Ingesters stop receiving new data but continue serving queries&lt;/li>
&lt;li>&lt;strong>T2 (T0 + 5h)&lt;/strong>: Ingesters no longer receive query requests (safe to remove)&lt;/li>
&lt;li>&lt;strong>T3 (T0 + retention_period)&lt;/strong>: All blocks naturally removed from ingesters&lt;/li>
&lt;li>&lt;strong>T4&lt;/strong>: Remove ingesters from cluster&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Any time after T2 is safe for removal without service impact.&lt;/strong>&lt;/p>
&lt;h3 id="legacy-approach-for-older-versions">Legacy Approach (For Older Versions)&lt;/h3>
&lt;p>If you’re running an older version of Cortex that doesn’t support the READONLY state, you’ll need to follow the legacy approach.&lt;/p>
&lt;p>The ingesters scale down is deemed an infrequent operation and no automation is currently provided. However, if you need to scale down ingesters, please be aware of the following:&lt;/p>
&lt;ul>
&lt;li>Configure queriers and rulers to always query the storage
&lt;ul>
&lt;li>&lt;code>-querier.query-store-after=0s&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Frequently scan the storage bucket
&lt;ul>
&lt;li>&lt;code>-blocks-storage.bucket-store.sync-interval=5m&lt;/code>&lt;/li>
&lt;li>&lt;code>-compactor.cleanup-interval=5m&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Lower bucket scanning cache TTLs
&lt;ul>
&lt;li>&lt;code>-blocks-storage.bucket-store.metadata-cache.bucket-index-content-ttl=1m&lt;/code>&lt;/li>
&lt;li>&lt;code>-blocks-storage.bucket-store.metadata-cache.tenant-blocks-list-ttl=1m&lt;/code>&lt;/li>
&lt;li>&lt;code>-blocks-storage.bucket-store.metadata-cache.metafile-doesnt-exist-ttl=1m&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ingesters should be scaled down one by one:
&lt;ol>
&lt;li>Call &lt;code>/shutdown&lt;/code> endpoint on the ingester to shutdown&lt;/li>
&lt;li>Wait until the HTTP call returns successfully or &amp;ldquo;finished flushing and shipping TSDB blocks&amp;rdquo; is logged&lt;/li>
&lt;li>Terminate the ingester process (the &lt;code>/shutdown&lt;/code> will not do it)&lt;/li>
&lt;li>Before proceeding to the next ingester, wait 2x the maximum between &lt;code>-blocks-storage.bucket-store.sync-interval&lt;/code> and &lt;code>-compactor.cleanup-interval&lt;/code>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Native Histograms</title><link>/docs/guides/native-histograms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/native-histograms/</guid><description>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>The Prometheus introduces a native histogram, a new sample type, to address several problems that the classic histograms (originally implemented histograms) have.
Please read to &lt;a href="https://prometheus.io/docs/specs/native_histograms/">Prometheus Native Histograms Document&lt;/a> if more detailed information is necessary.&lt;/p>
&lt;p>This guide explains how to configure the native histograms on the Cortex.&lt;/p>
&lt;h2 id="how-to-configure-native-histograms">How to configure native histograms&lt;/h2>
&lt;p>This section explains how to configure native histograms on the Cortex.&lt;/p>
&lt;h3 id="enable-ingestion">Enable Ingestion&lt;/h3>
&lt;p>To ingest native histogram ingestion, set the flag &lt;code>-blocks-storage.tsdb.enable-native-histograms&lt;/code>.&lt;/p>
&lt;p>And via yaml:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">enable_native_histograms&lt;/span>: &lt;span style="color:#ae81ff">&amp;lt;bool&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>By default, it is disabled, so make sure to enable it if you need native histograms for your system monitoring.&lt;/p>
&lt;p>When a &lt;code>write request&lt;/code> contains both &lt;code>samples&lt;/code> and &lt;code>native histogram&lt;/code>, the ingestion behavior is as follows:&lt;/p>
&lt;ul>
&lt;li>When it is enabled, the Cortex ingests samples and native histograms. If ingestion is successful, the Cortex returns a 200 status code.&lt;/li>
&lt;li>When it is disabled, the Cortex ingests samples but discards the native histograms. If sample ingestion is successful, the Cortex returns a 200 status code.&lt;/li>
&lt;/ul>
&lt;p>You can track the discarded number of native histograms via the &lt;code>cortex_discarded_samples_total{reason=&amp;quot;native-histogram-sample&amp;quot;}&lt;/code> promQL.&lt;/p>
&lt;h3 id="set-bucket-limit">Set Bucket limit&lt;/h3>
&lt;p>To limit the number of buckets(= sum of the number of positive and negative buckets) native histograms ingested, set the flag &lt;code>-validation.max-native-histogram-buckets&lt;/code>.&lt;/p>
&lt;p>And via yaml:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_native_histogram_buckets&lt;/span>: &lt;span style="color:#ae81ff">&amp;lt;int&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The default value is 0, which means no limit. If the total number of positive and negative buckets exceeds the limit, the Cortex emits a validation error with status code 400 (Bad Request).&lt;/p>
&lt;p>To limit the number of buckets per tenant, you can utilize a &lt;a href="/docs/configuration/arguments/#runtime-configuration-file">runtime config&lt;/a>.&lt;/p>
&lt;p>For example, the following yaml file specifies the number of bucket limit 160 for &lt;code>user-1&lt;/code> and no limit for &lt;code>user-2&lt;/code>.&lt;/p>
&lt;pre tabindex="0">&lt;code>overrides:
user-1:
max_native_histogram_buckets: 160
user-2:
max_native_histogram_buckets: 0
&lt;/code>&lt;/pre>&lt;h2 id="how-to-enable-out-of-order-native-histograms-ingestion">How to enable out-of-order native histograms ingestion&lt;/h2>
&lt;p>Like samples out-of-order ingestion, the Cortex allows out-of-order ingestion for the native histogram.
It is automatically enabled when &lt;code>-blocks-storage.tsdb.enable-native-histograms=true&lt;/code> and &lt;code>-ingester.out-of-order-time-window &amp;gt; 0&lt;/code>.&lt;/p>
&lt;p>And via yaml:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">enable_native_histograms&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">out_of_order_time_window&lt;/span>: &lt;span style="color:#ae81ff">5m&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Overrides Exporter</title><link>/docs/guides/overrides-exporter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/overrides-exporter/</guid><description>
&lt;p>Since Cortex is a multi-tenant system, it supports applying limits to each tenant to prevent
any single one from using too many resources. In order to help operators understand how close
to their limits tenants are, the &lt;code>overrides-exporter&lt;/code> module can expose limits as Prometheus metrics.&lt;/p>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>To update configuration without restarting, Cortex allows operators to supply a &lt;code>runtime_config&lt;/code>
file that will be periodically reloaded. This file can be specified under the &lt;code>runtime_config&lt;/code> section
of the main &lt;a href="/docs/configuration/arguments/#runtime-configuration-file">configuration file&lt;/a> or using the &lt;code>-runtime-config.file&lt;/code>
command-line flag. This file is used to apply tenant-specific limits.&lt;/p>
&lt;h2 id="example">Example&lt;/h2>
&lt;p>The &lt;code>overrides-exporter&lt;/code> is not enabled by default; it must be explicitly enabled. We recommend
only running a single instance of it in your cluster due to the cardinality of the metrics
emitted.&lt;/p>
&lt;p>With a &lt;code>runtime.yaml&lt;/code> file given below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># file: runtime.yaml&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># In this example, we&amp;#39;re overriding ingestion limits for a single tenant.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">overrides&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;user1&amp;#34;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ingestion_burst_size&lt;/span>: &lt;span style="color:#ae81ff">350000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ingestion_rate&lt;/span>: &lt;span style="color:#ae81ff">350000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_global_series_per_metric&lt;/span>: &lt;span style="color:#ae81ff">300000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_global_series_per_user&lt;/span>: &lt;span style="color:#ae81ff">300000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_series_per_metric&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_series_per_user&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_samples_per_query&lt;/span>: &lt;span style="color:#ae81ff">100000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_series_per_query&lt;/span>: &lt;span style="color:#ae81ff">100000&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>overrides-exporter&lt;/code> is configured to run as follows:&lt;/p>
&lt;pre tabindex="0">&lt;code>cortex -target overrides-exporter -runtime-config.file runtime.yaml -server.http-listen-port=8080
&lt;/code>&lt;/pre>&lt;p>After the &lt;code>overrides-exporter&lt;/code> starts, you can use &lt;code>curl&lt;/code> to inspect the tenant overrides.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>curl -s http://localhost:8080/metrics | grep cortex_overrides
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># HELP cortex_overrides Resource limit overrides applied to tenants
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># TYPE cortex_overrides gauge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_overrides{limit_name=&amp;#34;ingestion_burst_size&amp;#34;,user=&amp;#34;user1&amp;#34;} 350000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_overrides{limit_name=&amp;#34;ingestion_rate&amp;#34;,user=&amp;#34;user1&amp;#34;} 350000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_overrides{limit_name=&amp;#34;max_global_series_per_metric&amp;#34;,user=&amp;#34;user1&amp;#34;} 300000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_overrides{limit_name=&amp;#34;max_global_series_per_user&amp;#34;,user=&amp;#34;user1&amp;#34;} 300000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_overrides{limit_name=&amp;#34;max_local_series_per_metric&amp;#34;,user=&amp;#34;user1&amp;#34;} 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_overrides{limit_name=&amp;#34;max_local_series_per_user&amp;#34;,user=&amp;#34;user1&amp;#34;} 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_overrides{limit_name=&amp;#34;max_samples_per_query&amp;#34;,user=&amp;#34;user1&amp;#34;} 100000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_overrides{limit_name=&amp;#34;max_series_per_query&amp;#34;,user=&amp;#34;user1&amp;#34;} 100000
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With these metrics, you can set up alerts to know when tenants are close to hitting their limits
before they exceed them.&lt;/p></description></item><item><title>Docs: Use Partition Compaction in Cortex</title><link>/docs/guides/partition-compaction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/partition-compaction/</guid><description>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>Compactor is bounded by maximum 64GB of index file size. If compaction failed due to exceeding index file size limit, partition compaction can be enabled to allow compactor compacting into multiple blocks that have index file size stays within limit.&lt;/p>
&lt;h2 id="enable-partition-compaction">Enable Partition Compaction&lt;/h2>
&lt;p>In order to enable partition compaction, the following flag needs to be set:&lt;/p>
&lt;pre tabindex="0">&lt;code>-compactor.sharding-enabled=true # Enable sharding tenants across multiple compactor instances. This is required to enable partition compaction
-compactor.sharding-strategy=shuffle-sharding # Use Shuffle Sharding as sharding strategy. This is required to enable partition compaction
-compactor.compaction-strategy=partitioning # Use Partition Compaction as compaction strategy. To turn if off, set it to `default`
&lt;/code>&lt;/pre>&lt;h3 id="migration">Migration&lt;/h3>
&lt;p>There is no special migration process needed to enable partition compaction. End user could enable it by setting the above configurations all at once.&lt;/p>
&lt;p>Enabling partition compaction would group previously compacted blocks (only those have time range smaller than the largest configured compaction time ranges) with uncompacted blocks and generate new compaction plans. This would group blocks having duplicated series together and those series would be deduped after compaction.&lt;/p>
&lt;p>Disabling partition compaction after enabled it does not need migration either. After disabling partition compaction, compactor would group partitioned result blocks together and compact them into one block.&lt;/p>
&lt;h2 id="configure-partition-compaction">Configure Partition Compaction&lt;/h2>
&lt;p>By default, partition compaction utilizes the following configurations and their values:&lt;/p>
&lt;pre tabindex="0">&lt;code>-compactor.partition-index-size-bytes=68719476736 # 64GB
-compactor.partition-series-count=0 # no limit
&lt;/code>&lt;/pre>&lt;p>The default value should start partitioning result blocks when sum of index files size of parent blocks exceeds 64GB. End user could also change those two configurations. Partition compaction would always calculate partition count based on both configuration and pick the one with higher partition count.&lt;/p>
&lt;p>Both configurations support to be set per tenant.&lt;/p>
&lt;p>Note: &lt;code>compactor.partition-series-count&lt;/code> is using sum of series count of all parent blocks. If parent blocks were not deduped, the result block could have fewer series than the configuration value.&lt;/p>
&lt;h2 id="useful-metrics">Useful Metrics&lt;/h2>
&lt;ul>
&lt;li>&lt;code>cortex_compactor_group_partition_count&lt;/code>: can be used to keep track of how many partitions being compacted for each time range.&lt;/li>
&lt;li>&lt;code>cortex_compactor_group_compactions_not_planned_total&lt;/code>: can be used to alarm any compaction was failed to be planned due to error.&lt;/li>
&lt;li>&lt;code>cortex_compact_group_compaction_duration_seconds&lt;/code>: can be used to monitor compaction duration of each time range compactions.&lt;/li>
&lt;li>&lt;code>cortex_compactor_oldest_partition_offset&lt;/code>: can be used to monitor when was the oldest compaction that is still not completed.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Rule evaluations via query frontend</title><link>/docs/guides/rule-evalutions-via-query-frontend/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/rule-evalutions-via-query-frontend/</guid><description>
&lt;p>This guide explains how to configure the Ruler to evaluate rules via Query Frontends instead of the Ingester/Store Gateway, and the pros and cons of rule evaluation via Query Frontend.&lt;/p>
&lt;h2 id="how-to-enable">How to enable&lt;/h2>
&lt;p>By default, the Ruler queries both Ingesters and Store Gateway depending on the Rule time range for evaluating rules (alerting rules or recording rules). If you have set &lt;code>-ruler.frontend-address&lt;/code>, then the Ruler queries the Query Frontend for evaluation rules.
The address should be the gRPC listen address in host:port format.&lt;/p>
&lt;p>You can configure via args:&lt;/p>
&lt;pre tabindex="0">&lt;code>-ruler.frontend-address=query-frontend.svc.cluster.local:9095
&lt;/code>&lt;/pre>&lt;p>And via yaml:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">ruler&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">frontend_address&lt;/span>: &lt;span style="color:#ae81ff">query-frontend.svc.cluster.local:9095&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In addition, you can configure gRPC client (Ruler -&amp;gt; Query Frontend) config, please refer to frontend_client section in &lt;a href="/docs/configuration/configuration-file/#ruler_config">ruler config&lt;/a>.&lt;/p>
&lt;h2 id="configure-query-response-format">Configure query response format&lt;/h2>
&lt;p>You can configure the query response format via &lt;code>-ruler.query-response-format&lt;/code>. It is used to retrieve query results from the Query Frontend.
The supported values are &lt;code>protobuf&lt;/code> and &lt;code>json&lt;/code>. We recommend using &lt;code>protobuf&lt;/code>(default) because the retrieved query results containing native histograms are only supported on &lt;code>protobuf&lt;/code>.&lt;/p>
&lt;h2 id="how-to-track-query-stat-log-and-metric">How to track query stat log and metric&lt;/h2>
&lt;p>When the &lt;code>-ruler.frontend-address&lt;/code> is configured, the values of query stats for rule evaluation logs and metrics are all &lt;code>0&lt;/code> in the Ruler.
You can look at the logs at the Query Frontend via enabling &lt;code>-frontend.enabled-ruler-query-stats&lt;/code> (disabled by default).&lt;/p>
&lt;p>Also, there are alternative metrics; you can see right-side metrics instead:&lt;/p>
&lt;pre tabindex="0">&lt;code>`cortex_ruler_query_seconds_total` -&amp;gt; `cortex_query_seconds_total{source=&amp;#34;ruler&amp;#34;}`
`cortex_ruler_fetched_series_total` -&amp;gt; `cortex_query_fetched_series_total{source=&amp;#34;ruler&amp;#34;}`
`cortex_ruler_samples_total` -&amp;gt; `cortex_query_samples_total{source=&amp;#34;ruler&amp;#34;}`
`cortex_ruler_fetched_chunks_bytes_total` -&amp;gt; `cortex_query_fetched_chunks_bytes_total{source=&amp;#34;ruler&amp;#34;}`
`cortex_ruler_fetched_data_bytes_total` -&amp;gt; `cortex_query_fetched_data_bytes_total{source=&amp;#34;ruler&amp;#34;}`
&lt;/code>&lt;/pre>&lt;h2 id="pros-and-cons">Pros and Cons&lt;/h2>
&lt;p>If this feature is enabled, the query execute path is as follows:&lt;/p>
&lt;p>Ruler -&amp;gt; Query Frontend -&amp;gt; Query Scheduler -&amp;gt; Querier -&amp;gt; Ingester/Store Gateway&lt;/p>
&lt;p>There are pros and cons regarding query performance as more hops than before (Ruler -&amp;gt; Ingester/Store Gateway).&lt;/p>
&lt;h3 id="pros">Pros&lt;/h3>
&lt;ul>
&lt;li>The rule evaluation performance could be improved in such a situation where the number of Queriers pulling queries from the Query Scheduler is good enough.
If then, the queries in Query Scheduler are fetched in a reasonable time (i.e. a lot of hops are not a defect for query performance). In this environment, query performance could be improved as we can use Query Frontend features like the vertical query sharding.&lt;/li>
&lt;li>The Ruler can use fewer resources as it doesn&amp;rsquo;t need to run a query engine to evaluate rules.&lt;/li>
&lt;/ul>
&lt;h3 id="cons">Cons&lt;/h3>
&lt;ul>
&lt;li>If there are not enough Queriers, adding rule queries to Query Scheduler could cause query starvation problem (queries in Query Scheduler could not be fetched in a reasonable time), so rules cannot be evaluated in time.&lt;/li>
&lt;/ul>
&lt;p>You can utilize the &lt;code>cortex_prometheus_rule_evaluation_duration_seconds&lt;/code> metric whether to use &lt;code>-ruler.frontend-address&lt;/code>.&lt;/p></description></item><item><title>Docs: Securing communication between Cortex components with TLS</title><link>/docs/guides/tls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/tls/</guid><description>
&lt;p>Cortex is a distributed system with significant traffic between its services.
To allow for secure communication, Cortex supports TLS between all its
components. This guide describes the process of setting up TLS.&lt;/p>
&lt;h3 id="generation-of-certs-to-configure-tls">Generation of certs to configure TLS&lt;/h3>
&lt;p>The first step to securing inter-service communication in Cortex with TLS is
generating certificates. A Certifying Authority (CA) will be used for this
purpose, which should be private to the organization, as any certificates signed
by this CA will have permissions to communicate with the cluster.&lt;/p>
&lt;p>We will use the following script to generate self-signed certs for the cluster:&lt;/p>
&lt;pre tabindex="0">&lt;code># keys
openssl genrsa -out root.key
openssl genrsa -out client.key
openssl genrsa -out server.key
# root cert / certifying authority
openssl req -x509 -new -nodes -key root.key -subj &amp;#34;/C=US/ST=KY/O=Org/CN=root&amp;#34; -sha256 -days 100000 -out root.crt
# csrs - certificate signing requests
openssl req -new -sha256 -key client.key -subj &amp;#34;/C=US/ST=KY/O=Org/CN=client&amp;#34; -out client.csr
openssl req -new -sha256 -key server.key -subj &amp;#34;/C=US/ST=KY/O=Org/CN=localhost&amp;#34; -out server.csr
# certificates
openssl x509 -req -in client.csr -CA root.crt -CAkey root.key -CAcreateserial -out client.crt -days 100000 -sha256
openssl x509 -req -in server.csr -CA root.crt -CAkey root.key -CAcreateserial -out server.crt -days 100000 -sha256
&lt;/code>&lt;/pre>&lt;p>Note that the above script generates certificates that are valid for 100,000 days.
This can be changed by adjusting the &lt;code>-days&lt;/code> option in the above commands.
It is recommended that the certs be replaced at least once every 2 years.&lt;/p>
&lt;p>The above script generates keys &lt;code>client.key, server.key&lt;/code> and certs
&lt;code>client.crt, server.crt&lt;/code> for both the client and server. The CA cert is
generated as &lt;code>root.crt&lt;/code>.&lt;/p>
&lt;h3 id="load-certs-into-the-httpgrpc-serverclient">Load certs into the HTTP/GRPC server/client&lt;/h3>
&lt;p>Every HTTP/GRPC link between Cortex components supports TLS configuration
through the following config parameters:&lt;/p>
&lt;h4 id="server-flags">Server flags&lt;/h4>
&lt;pre tabindex="0">&lt;code> # Path to the TLS Cert for the HTTP Server
-server.http-tls-cert-path=/path/to/server.crt
# Path to the TLS Key for the HTTP Server
-server.http-tls-key-path=/path/to/server.key
# Type of Client Auth for the HTTP Server
-server.http-tls-client-auth=&amp;#34;RequireAndVerifyClientCert&amp;#34;
# Path to the Client CA Cert for the HTTP Server
-server.http-tls-ca-path=&amp;#34;/path/to/root.crt&amp;#34;
# Path to the TLS Cert for the GRPC Server
-server.grpc-tls-cert-path=/path/to/server.crt
# Path to the TLS Key for the GRPC Server
-server.grpc-tls-key-path=/path/to/server.key
# Type of Client Auth for the GRPC Server
-server.grpc-tls-client-auth=&amp;#34;RequireAndVerifyClientCert&amp;#34;
# Path to the Client CA Cert for the GRPC Server
-server.grpc-tls-ca-path=/path/to/root.crt
&lt;/code>&lt;/pre>&lt;h4 id="client-flags">Client flags&lt;/h4>
&lt;p>Client flags are component-specific.&lt;/p>
&lt;p>For an HTTP client in the Alertmanager:&lt;/p>
&lt;pre tabindex="0">&lt;code> # Path to the TLS Cert for the HTTP Client
-alertmanager.configs.tls-cert-path=/path/to/client.crt
# Path to the TLS Key for the HTTP Client
-alertmanager.configs.tls-key-path=/path/to/client.key
# Path to the TLS CA for the HTTP Client
-alertmanager.configs.tls-ca-path=/path/to/root.crt
&lt;/code>&lt;/pre>&lt;p>For a GRPC client in the Querier:&lt;/p>
&lt;pre tabindex="0">&lt;code> # Path to the TLS Cert for the GRPC Client
-querier.frontend-client.tls-cert-path=/path/to/client.crt
# Path to the TLS Key for the GRPC Client
-querier.frontend-client.tls-key-path=/path/to/client.key
# Path to the TLS CA for the GRPC Client
-querier.frontend-client.tls-ca-path=/path/to/root.crt
&lt;/code>&lt;/pre>&lt;p>Similarly, for the GRPC Ingester Client:&lt;/p>
&lt;pre tabindex="0">&lt;code> # Path to the TLS Cert for the GRPC Client
-ingester.client.tls-cert-path=/path/to/client.crt
# Path to the TLS Key for the GRPC Client
-ingester.client.tls-key-path=/path/to/client.key
# Path to the TLS CA for the GRPC Client
-ingester.client.tls-ca-path=/path/to/root.crt
&lt;/code>&lt;/pre>&lt;p>TLS can be configured in a similar fashion for other HTTP/GRPC clients in Cortex.&lt;/p></description></item><item><title>Docs: Security</title><link>/docs/guides/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/security/</guid><description>
&lt;p>Cortex must be deployed with due care over system configuration, using principles such as &amp;ldquo;least privilege&amp;rdquo; to limit any exposure due to flaws in the source code.&lt;/p>
&lt;p>You must configure authorisation and authentication externally to Cortex; see &lt;a href="/docs/guides/auth/">this guide&lt;/a>&lt;/p>
&lt;p>Information about security disclosures and mailing lists is &lt;a href="https://github.com/cortexproject/cortex/blob/master/SECURITY.md">in the main repo&lt;/a>&lt;/p></description></item><item><title>Docs: Shuffle Sharding</title><link>/docs/guides/shuffle-sharding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/shuffle-sharding/</guid><description>
&lt;p>Cortex leverages sharding techniques to horizontally scale both single and multi-tenant clusters beyond the capacity of a single node.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>The &lt;strong>default sharding strategy&lt;/strong> employed by Cortex distributes the workload across the entire pool of instances running a given service (eg. ingesters). For example, on the write path, each tenant&amp;rsquo;s series are sharded across all ingesters, regardless of how many active series the tenant has or how many different tenants are in the cluster.&lt;/p>
&lt;p>The default strategy allows for a fair balance on the resources consumed by each instance (ie. CPU and memory) and to maximise these resources across the cluster.&lt;/p>
&lt;p>However, in a &lt;strong>multi-tenant&lt;/strong> cluster, this approach also introduces some &lt;strong>downsides&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>An outage affects all tenants&lt;/li>
&lt;li>A misbehaving tenant (eg. causing out-of-memory) could affect all other tenants.&lt;/li>
&lt;/ol>
&lt;p>The goal of &lt;strong>shuffle sharding&lt;/strong> is to provide an alternative sharding strategy to reduce the blast radius of an outage and better isolate tenants.&lt;/p>
&lt;h2 id="what-is-shuffle-sharding">What is shuffle sharding?&lt;/h2>
&lt;p>Shuffle sharding is a technique used to isolate different tenants’ workloads and to give each tenant a single-tenant experience even if they&amp;rsquo;re running in a shared cluster. This technique has been publicly shared and clearly explained by AWS in their &lt;a href="https://aws.amazon.com/builders-library/workload-isolation-using-shuffle-sharding/">builders&amp;rsquo; library&lt;/a> and a reference implementation has been shown in the &lt;a href="https://github.com/awslabs/route53-infima/blob/master/src/main/java/com/amazonaws/services/route53/infima/SimpleSignatureShuffleSharder.java">Route53 Infima library&lt;/a>.&lt;/p>
&lt;p>The idea is to assign each tenant a shard composed of a subset of the Cortex service instances, aiming to minimize the overlapping instances between two different tenants. Shuffle sharding brings the following &lt;strong>benefits&lt;/strong> over the default sharding strategy:&lt;/p>
&lt;ul>
&lt;li>An outage on some Cortex cluster instances/nodes will only affect a subset of tenants.&lt;/li>
&lt;li>A misbehaving tenant will affect only its shard instances. Due to the low overlap of instances between different tenants, it&amp;rsquo;s statistically quite likely that any other tenant will run on different instances or only a subset of instances will match the affected ones.&lt;/li>
&lt;/ul>
&lt;p>Shuffle sharding requires no more resources than the default sharding strategy, but instances may be less evenly balanced from time to time.&lt;/p>
&lt;h3 id="low-overlapping-instances-probability">Low overlapping instances probability&lt;/h3>
&lt;p>For example, given a Cortex cluster running &lt;strong>50 ingesters&lt;/strong> and assigning &lt;strong>each tenant 4&lt;/strong> out of 50 ingesters, shuffling instances between each tenant, we get &lt;strong>230K possible combinations&lt;/strong>.&lt;/p>
&lt;p>Randomly picking two different tenants, we have the:&lt;/p>
&lt;ul>
&lt;li>71% chance that they will not share any instance&lt;/li>
&lt;li>26% chance that they will share only 1 instance&lt;/li>
&lt;li>2.7% chance that they will share 2 instances&lt;/li>
&lt;li>0.08% chance that they will share 3 instances&lt;/li>
&lt;li>Only a 0.0004% chance that their instances will fully overlap&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="/images/guides/shuffle-sharding-probability.png" alt="Shuffle sharding probability">&lt;/p>
&lt;!-- Chart source at https://docs.google.com/spreadsheets/d/1FXbiWTXi6bdERtamH-IfmpgFq1fNL4GP_KX_yJvbRi4/edit -->
&lt;h2 id="cortex-shuffle-sharding">Cortex shuffle sharding&lt;/h2>
&lt;p>Cortex currently supports shuffle sharding in the following services:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#ingesters-shuffle-sharding">Ingesters&lt;/a>&lt;/li>
&lt;li>&lt;a href="#query-frontend-and-query-scheduler-shuffle-sharding">Query-frontend / Query-scheduler&lt;/a>&lt;/li>
&lt;li>&lt;a href="#store-gateway-shuffle-sharding">Store-gateway&lt;/a>&lt;/li>
&lt;li>&lt;a href="#ruler-shuffle-sharding">Ruler&lt;/a>&lt;/li>
&lt;li>&lt;a href="#compactor-shuffle-sharding">Compactor&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Shuffle sharding is &lt;strong>disabled by default&lt;/strong> and needs to be explicitly enabled in the configuration.&lt;/p>
&lt;h3 id="guaranteed-properties">Guaranteed properties&lt;/h3>
&lt;p>The Cortex shuffle sharding implementation guarantees the following properties:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Stability&lt;/strong>&lt;br />
Given a consistent state of the hash ring, the shuffle sharding algorithm always selects the same instances for a given tenant, even across different machines.&lt;/li>
&lt;li>&lt;strong>Consistency&lt;/strong>&lt;br />
Adding or removing 1 instance from the hash ring leads to only 1 instance changed at most, in each tenant&amp;rsquo;s shard.&lt;/li>
&lt;li>&lt;strong>Shuffling&lt;/strong>&lt;br />
Probabilistically and for a large enough cluster, it ensures that every tenant gets a different set of instances, with a reduced number of overlapping instances between two tenants to improve failure isolation.&lt;/li>
&lt;li>&lt;strong>Zone-awareness&lt;/strong>&lt;br />
When &lt;a href="/docs/guides/zone-aware-replication/">zone-aware replication&lt;/a> is enabled, the subset of instances selected for each tenant contains a balanced number of instances for each availability zone.&lt;/li>
&lt;/ul>
&lt;h3 id="ingesters-shuffle-sharding">Ingesters shuffle sharding&lt;/h3>
&lt;p>By default, the Cortex distributor spreads the received series across all running ingesters.&lt;/p>
&lt;p>When shuffle sharding is &lt;strong>enabled&lt;/strong> for the ingesters, the distributor and ruler on the &lt;strong>write path&lt;/strong> spread each tenant series across &lt;code>-distributor.ingestion-tenant-shard-size&lt;/code> number of ingesters, while on the &lt;strong>read path&lt;/strong>, the querier and ruler queries only the subset of ingesters holding the series for a given tenant.&lt;/p>
&lt;p>&lt;em>The shard size can be overridden on a per-tenant basis in the limits overrides configuration.&lt;/em>&lt;/p>
&lt;h4 id="ingesters-write-path">Ingesters write path&lt;/h4>
&lt;p>To enable shuffle-sharding for ingesters on the write path, you need to configure the following CLI flags (or their respective YAML config options) to &lt;strong>distributor&lt;/strong>, &lt;strong>ingester&lt;/strong>, and &lt;strong>ruler&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>-distributor.sharding-strategy=shuffle-sharding&lt;/code>&lt;/li>
&lt;li>&lt;code>-distributor.ingestion-tenant-shard-size=&amp;lt;size&amp;gt;&lt;/code>&lt;br />
&lt;code>&amp;lt;size&amp;gt;&lt;/code> should be set to the number of ingesters each tenant series should be sharded to. If &lt;code>&amp;lt;size&amp;gt;&lt;/code> is greater than the number of available ingesters in the Cortex cluster, the tenant series are sharded across all ingesters.&lt;/li>
&lt;/ul>
&lt;h4 id="ingesters-read-path">Ingesters read path&lt;/h4>
&lt;p>Assuming shuffle-sharding has been enabled for the write path, to enable shuffle-sharding for ingesters on the read path too, you need to configure the following CLI flags (or their respective YAML config options) to &lt;strong>querier&lt;/strong> and &lt;strong>ruler&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>-distributor.sharding-strategy=shuffle-sharding&lt;/code>&lt;/li>
&lt;li>&lt;code>-distributor.ingestion-tenant-shard-size=&amp;lt;size&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>-querier.shuffle-sharding-ingesters-lookback-period=&amp;lt;period&amp;gt;&lt;/code>&lt;br />
Queriers and rulers fetch in-memory series from the minimum set of required ingesters, selecting only ingesters which may have received series since &amp;rsquo;now - lookback period&amp;rsquo;. The configured lookback &lt;code>&amp;lt;period&amp;gt;&lt;/code> should be greater than or equal to &lt;code>-querier.query-store-after&lt;/code> and &lt;code>-querier.query-ingesters-within&lt;/code> if set, and greater than the estimated minimum time it takes for the oldest samples stored in a block uploaded by ingester to be discovered and available for querying (3h with the default configuration).&lt;/li>
&lt;/ul>
&lt;h4 id="rollout-strategy">Rollout strategy&lt;/h4>
&lt;p>If you&amp;rsquo;re running a Cortex cluster with shuffle-sharding disabled and you want to enable it for ingesters, the following rollout strategy should be used to avoid missing querying any time-series in the ingesters’ memory:&lt;/p>
&lt;ol>
&lt;li>Enable ingesters shuffle-sharding on the &lt;strong>write path&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Wait&lt;/strong> at least &lt;code>-querier.shuffle-sharding-ingesters-lookback-period&lt;/code> time&lt;/li>
&lt;li>Enable ingesters shuffle-sharding on the &lt;strong>read path&lt;/strong>&lt;/li>
&lt;/ol>
&lt;h4 id="limitation-decreasing-the-tenant-shard-size">Limitation: decreasing the tenant shard size&lt;/h4>
&lt;p>The current shuffle-sharding implementation in Cortex has a limitation which prevents safely decreasing the tenant shard size if the ingesters shuffle-sharding is enabled on the read path.&lt;/p>
&lt;p>The problem is that if a tenant’s subring decreases in size, there is currently no way for the queriers and rulers to know how big the tenant subring was previously, and hence they will potentially miss an ingester with data for that tenant. In other words, the lookback mechanism to select the ingesters which may have received series since &amp;rsquo;now - lookback period&amp;rsquo; doesn&amp;rsquo;t work correctly if the tenant shard size is decreased.&lt;/p>
&lt;p>This is deemed an infrequent operation that we considered banning, but a workaround still exists:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Disable&lt;/strong> shuffle-sharding on the read path&lt;/li>
&lt;li>&lt;strong>Decrease&lt;/strong> the configured tenant shard size&lt;/li>
&lt;li>&lt;strong>Wait&lt;/strong> at least &lt;code>-querier.shuffle-sharding-ingesters-lookback-period&lt;/code> time&lt;/li>
&lt;li>&lt;strong>Re-enable&lt;/strong> shuffle-sharding on the read path&lt;/li>
&lt;/ol>
&lt;h3 id="query-frontend-and-query-scheduler-shuffle-sharding">Query-frontend and Query-scheduler shuffle sharding&lt;/h3>
&lt;p>By default, all Cortex queriers can execute received queries for a given tenant.&lt;/p>
&lt;p>When shuffle sharding is &lt;strong>enabled&lt;/strong> by setting &lt;code>-frontend.max-queriers-per-tenant&lt;/code> (or its respective YAML config option) to a value higher than 0 and lower than the number of available queriers, only the specified number of queriers will execute queries for a single tenant.&lt;/p>
&lt;p>Note that this distribution happens in the query-frontend, or query-scheduler if used. When using query-scheduler, the &lt;code>-frontend.max-queriers-per-tenant&lt;/code> option must be set for the query-scheduler component. When not using query-frontend (with or without scheduler), this option is not available.&lt;/p>
&lt;p>&lt;em>The maximum number of queriers can be overridden on a per-tenant basis in the limits overrides configuration.&lt;/em>&lt;/p>
&lt;h4 id="the-impact-of-query-of-death">The impact of &amp;ldquo;query of death&amp;rdquo;&lt;/h4>
&lt;p>In the event a tenant is repeatedly sending a &amp;ldquo;query of death&amp;rdquo; which leads the querier to crash or get killed because of out-of-memory, the crashed querier will get disconnected from the query-frontend or query-scheduler and a new querier will be immediately assigned to the tenant&amp;rsquo;s shard. This practically invalidates the assumption that shuffle-sharding can be used to contain the blast radius in case of a query of death.&lt;/p>
&lt;p>To mitigate it, Cortex allows you to configure a delay between when a querier disconnects because of a crash and when the crashed querier is actually removed from the tenant&amp;rsquo;s shard (and another healthy querier is added as a replacement). A delay of 1 minute may be a reasonable trade-off:&lt;/p>
&lt;ul>
&lt;li>Query-frontend: &lt;code>-query-frontend.querier-forget-delay=1m&lt;/code>&lt;/li>
&lt;li>Query-scheduler: &lt;code>-query-scheduler.querier-forget-delay=1m&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="store-gateway-shuffle-sharding">Store-gateway shuffle sharding&lt;/h3>
&lt;p>The Cortex store-gateway &amp;ndash; used by the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> &amp;ndash; by default spreads each tenant&amp;rsquo;s blocks across all running store-gateways.&lt;/p>
&lt;p>When shuffle sharding is &lt;strong>enabled&lt;/strong> via &lt;code>-store-gateway.sharding-strategy=shuffle-sharding&lt;/code> (or its respective YAML config option), each tenant’s blocks will be sharded across a subset of &lt;code>-store-gateway.tenant-shard-size&lt;/code> store-gateway instances. This configuration needs to be set to &lt;strong>store-gateway&lt;/strong>, &lt;strong>querier&lt;/strong>, and &lt;strong>ruler&lt;/strong>.&lt;/p>
&lt;p>&lt;em>The shard size can be overridden on a per-tenant basis setting &lt;code>store_gateway_tenant_shard_size&lt;/code> in the limits overrides configuration.&lt;/em>&lt;/p>
&lt;p>&lt;em>Please check out the &lt;a href="/docs/blocks-storage/store-gateway/">store-gateway documentation&lt;/a> for more information about how it works.&lt;/em>&lt;/p>
&lt;h3 id="ruler-shuffle-sharding">Ruler shuffle sharding&lt;/h3>
&lt;p>Cortex ruler can run in three modes:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>No sharding at all.&lt;/strong> This is the most basic mode of the ruler. It is activated by using &lt;code>-ruler.enable-sharding=false&lt;/code> (default) and works correctly only if a single ruler is running. In this mode, the Ruler loads all rules for all tenants.&lt;/li>
&lt;li>&lt;strong>Default sharding&lt;/strong>, activated by using &lt;code>-ruler.enable-sharding=true&lt;/code> and &lt;code>-ruler.sharding-strategy=default&lt;/code> (default). In this mode, rulers register themselves into the ring. Each ruler will then select and evaluate only those rules that it &amp;ldquo;owns&amp;rdquo;.&lt;/li>
&lt;li>&lt;strong>Shuffle sharding&lt;/strong>, activated by using &lt;code>-ruler.enable-sharding=true&lt;/code> and &lt;code>-ruler.sharding-strategy=shuffle-sharding&lt;/code>. Similarly to default sharding, rulers use the ring to distribute workload, but rule groups for each tenant can only be evaluated on a limited number of rulers (&lt;code>-ruler.tenant-shard-size&lt;/code>, can also be set per tenant as &lt;code>ruler_tenant_shard_size&lt;/code> in overrides).&lt;/li>
&lt;/ol>
&lt;p>Note that when using sharding strategy, each rule group is evaluated by a single ruler only; there is no replication.&lt;/p>
&lt;h3 id="compactor-shuffle-sharding">Compactor shuffle sharding&lt;/h3>
&lt;p>Cortex compactor can run in three modes:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>No sharding at all.&lt;/strong> This is the most basic mode of the compactor. It is activated by using &lt;code>-compactor.sharding-enabled=false&lt;/code> (default). In this mode, every compactor will run every compaction.&lt;/li>
&lt;li>&lt;strong>Default sharding&lt;/strong>, activated by using &lt;code>-compactor.sharding-enabled=true&lt;/code> and &lt;code>-compactor.sharding-strategy=default&lt;/code> (default). In this mode, compactors register themselves into the ring. One single tenant will belong to only 1 compactor.&lt;/li>
&lt;li>&lt;strong>Shuffle sharding&lt;/strong>, activated by using &lt;code>-compactor.sharding-enabled=true&lt;/code> and &lt;code>-compactor.sharding-strategy=shuffle-sharding&lt;/code>. Similarly to default sharding, but compactions for each tenant can be carried out on multiple compactors (&lt;code>-compactor.tenant-shard-size&lt;/code>, can also be set per tenant as &lt;code>compactor_tenant_shard_size&lt;/code> in overrides).&lt;/li>
&lt;/ol>
&lt;p>With shuffle sharding selected as the sharding strategy, a subset of the compactors will be used to handle a user based on the shard size.&lt;/p>
&lt;p>The idea behind using the shuffle sharding strategy for the compactor is to further enable horizontal scalability and build tolerance for compactions that may take longer than the compaction interval.&lt;/p>
&lt;h2 id="faq">FAQ&lt;/h2>
&lt;h3 id="does-shuffle-sharding-add-additional-overhead-to-the-kv-store">Does shuffle sharding add additional overhead to the KV store?&lt;/h3>
&lt;p>No, shuffle sharding subrings are computed client-side and are not stored in the ring. KV store sizing still depends primarily on the number of replicas (of any component that uses the ring, e.g. ingesters) and tokens per replica.&lt;/p>
&lt;p>However, each tenant&amp;rsquo;s subring is cached in memory on the client-side, which may slightly increase the memory footprint of certain components (mostly the distributor).&lt;/p></description></item><item><title>Docs: Tracing</title><link>/docs/guides/tracing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/tracing/</guid><description>
&lt;p>Cortex uses &lt;a href="https://www.jaegertracing.io/">Jaeger&lt;/a> or &lt;a href="https://opentelemetry.io/">OpenTelemetry&lt;/a> to implement distributed
tracing. We have found tracing invaluable for troubleshooting the behavior of
Cortex in production.&lt;/p>
&lt;h2 id="jaeger">Jaeger&lt;/h2>
&lt;h3 id="dependencies">Dependencies&lt;/h3>
&lt;p>In order to send traces, you will need to set up a Jaeger deployment. A
deployment includes either the Jaeger all-in-one binary or else a distributed
system of agents, collectors, and queriers. If running on Kubernetes, &lt;a href="https://github.com/jaegertracing/jaeger-kubernetes">Jaeger
Kubernetes&lt;/a> is an excellent
resource.&lt;/p>
&lt;h3 id="configuration">Configuration&lt;/h3>
&lt;p>In order to configure Cortex to send traces, you must do two things:&lt;/p>
&lt;ol>
&lt;li>Set the &lt;code>JAEGER_AGENT_HOST&lt;/code> environment variable in all components to point
to your Jaeger agent. This defaults to &lt;code>localhost&lt;/code>.&lt;/li>
&lt;li>Enable sampling in the appropriate components:
&lt;ul>
&lt;li>The Ingester and Ruler self-initiate traces and should have sampling
explicitly enabled.&lt;/li>
&lt;li>Sampling for the Distributor and Query Frontend can be enabled in Cortex
or in an upstream service such as your front door.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>To enable sampling in Cortex components, you can specify either
&lt;code>JAEGER_SAMPLER_MANAGER_HOST_PORT&lt;/code> for remote sampling or
&lt;code>JAEGER_SAMPLER_TYPE&lt;/code> and &lt;code>JAEGER_SAMPLER_PARAM&lt;/code> to manually set sampling
configuration. See the &lt;a href="https://github.com/jaegertracing/jaeger-client-go#environment-variables">Jaeger Client Go
documentation&lt;/a>
for the full list of environment variables you can configure.&lt;/p>
&lt;p>Note that you must specify one of &lt;code>JAEGER_AGENT_HOST&lt;/code> or
&lt;code>JAEGER_SAMPLER_MANAGER_HOST_PORT&lt;/code> in each component for Jaeger to be enabled,
even if you plan to use the default values.&lt;/p>
&lt;h2 id="opentelemetry">OpenTelemetry&lt;/h2>
&lt;h3 id="dependencies-1">Dependencies&lt;/h3>
&lt;p>In order to send traces, you will need to set up an OpenTelemetry Collector. The collector will be able to send traces to
multiple destinations such as &lt;a href="https://aws-otel.github.io/docs/getting-started/x-ray">AWS X-Ray&lt;/a>,
&lt;a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/googlecloudexporter">Google Cloud&lt;/a>,
&lt;a href="https://www.dash0.com/hub/integrations/int_opentelemetry-collector/overview">Dash0&lt;/a>
&lt;a href="https://docs.datadoghq.com/tracing/trace_collection/open_standards/otel_collector_datadog_exporter/">DataDog&lt;/a> and
&lt;a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter">others&lt;/a>. OpenTelemetry Collector
provides a &lt;a href="https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-collector/examples/deployment-otlp-traces">helm chart&lt;/a>
to set up the environment.&lt;/p>
&lt;h3 id="configuration-1">Configuration&lt;/h3>
&lt;p>See the document on the tracing section in the &lt;a href="https://cortexmetrics.io/docs/configuration/configuration-file/">Configuration file&lt;/a>.&lt;/p>
&lt;h3 id="current-state">Current State&lt;/h3>
&lt;p>Cortex is maintaining backward compatibility with Jaeger support. Cortex has not fully migrated from OpenTracing to OpenTelemetry and is currently using the
&lt;a href="https://opentelemetry.io/docs/migration/opentracing/">OpenTracing bridge&lt;/a>.&lt;/p></description></item><item><title>Docs: Use OpenTelemetry Collector to send metrics to Cortex</title><link>/docs/guides/use-opentelemetry-collector-to-send-metrics-to-cortex/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/use-opentelemetry-collector-to-send-metrics-to-cortex/</guid><description>
&lt;p>This guide explains how to configure open-telemetry collector and OTLP(OpenTelemetry Protocol) configurations in the
Cortex.&lt;/p>
&lt;h2 id="context">Context&lt;/h2>
&lt;p>The &lt;a href="https://opentelemetry.io/docs/collector/">open-telemetry collector&lt;/a> can write collected metrics to the Cortex with
the Prometheus and OTLP formats.&lt;/p>
&lt;h2 id="push-with-prometheus-format">Push with Prometheus format&lt;/h2>
&lt;p>To push metrics via the &lt;code>Prometheus&lt;/code> format, we can
use &lt;a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/prometheusremotewriteexporter">prometheusremotewrite&lt;/a>
exporter in the open-telemetry collector.
In the &lt;code>exporters&lt;/code> and &lt;code>service&lt;/code> sections in the open-telemetry collector yaml file, we can add as follows:&lt;/p>
&lt;pre tabindex="0">&lt;code>exporters:
prometheusremotewrite:
endpoint: http://&amp;lt;cortex-endpoint&amp;gt;/api/v1/push
headers:
X-Scope-OrgId: &amp;lt;orgId&amp;gt;
...
service:
pipelines:
metrics:
receivers: [...]
processors: [...]
exporters: [prometheusremotewrite]
&lt;/code>&lt;/pre>&lt;p>Please refer to &lt;a href="/docs/guides/auth/">Authentication and Authorisation&lt;/a> section for the
&lt;code>X-Scope-OrgId&lt;/code> explanation.&lt;/p>
&lt;h2 id="push-with-otlp-format">Push with OTLP format&lt;/h2>
&lt;p>To push metrics via the &lt;code>OTLP&lt;/code> format, we can
use &lt;a href="https://github.com/open-telemetry/opentelemetry-collector/tree/main/exporter/otlphttpexporter">otlphttp&lt;/a> exporter
in the open-telemetry collector.
In the &lt;code>exporters&lt;/code> and &lt;code>service&lt;/code> sections in the open-telemetry collector yaml file, we can add as follows:&lt;/p>
&lt;pre tabindex="0">&lt;code>exporters:
otlphttp:
endpoint: http://&amp;lt;cortex-endpoint&amp;gt;/api/v1/otlp
headers:
X-Scope-OrgId: &amp;lt;orgId&amp;gt;
...
service:
pipelines:
metrics:
receivers: [...]
processors: [...]
exporters: [otlphttp]
&lt;/code>&lt;/pre>&lt;h2 id="cortex-configurations-for-ingesting-otlp-metrics">Cortex configurations for ingesting OTLP metrics&lt;/h2>
&lt;p>You can configure OTLP-related flags in the config file.&lt;/p>
&lt;pre tabindex="0">&lt;code>limits:
promote_resource_attributes: &amp;lt;list of string&amp;gt;
...
distributor:
otlp:
convert_all_attributes: &amp;lt;boolean&amp;gt;
disable_target_info: &amp;lt;boolean&amp;gt;
allow_delta_temporality: &amp;lt;boolean&amp;gt;
enable_type_and_unit_labels: &amp;lt;boolean&amp;gt;
&lt;/code>&lt;/pre>&lt;h3 id="ingest-target_info-metric">Ingest &lt;code>target_info&lt;/code> metric&lt;/h3>
&lt;p>By default,
the &lt;a href="https://github.com/prometheus/OpenMetrics/blob/main/specification/OpenMetrics.md#supporting-target-metadata-in-both-push-based-and-pull-based-systems">target_info&lt;/a>
is enabled to write and can be disabled via &lt;code>-distributor.otlp.disable-target-info=true&lt;/code>.&lt;/p>
&lt;h3 id="resource-attributes-conversion">Resource attributes conversion&lt;/h3>
&lt;p>The conversion of
all &lt;a href="https://opentelemetry.io/docs/specs/semconv/resource/">resource attributes&lt;/a> to labels is
disabled by default and can be enabled via
&lt;code>-distributor.otlp.convert-all-attributes=true&lt;/code>.&lt;/p>
&lt;p>You can specify the attributes converted to labels via &lt;code>-distributor.promote-resource-attributes&lt;/code> flag. It is supported
only if &lt;code>-distributor.otlp.convert-all-attributes=false&lt;/code>.&lt;/p>
&lt;p>These flags can be configured via yaml:&lt;/p>
&lt;pre tabindex="0">&lt;code>limits:
promote_resource_attributes: &amp;lt;list of string&amp;gt;
...
distributor:
otlp:
convert_all_attributes: &amp;lt;boolean&amp;gt;
disable_target_info: &amp;lt;boolean&amp;gt;
&lt;/code>&lt;/pre>&lt;p>These are the yaml examples:&lt;/p>
&lt;ul>
&lt;li>Example 1: All of the resource attributes are converted, and the &lt;code>target_info&lt;/code> metric is disabled to push.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>distributor:
otlp:
convert_all_attributes: true
disable_target_info: true
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Example 2: Only &lt;code>service.name&lt;/code> and &lt;code>service.instance.id&lt;/code> resource attributes are converted to labels and the
&lt;code>target_info&lt;/code> metric is enabled to push.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>limits:
promote_resource_attributes: [&amp;#34;service.name&amp;#34;, &amp;#34;service.instance.id&amp;#34;]
distributor:
otlp:
convert_all_attributes: false
disable_target_info: false
&lt;/code>&lt;/pre>&lt;h3 id="ingest-delta-temporality-otlp-metrics">Ingest delta temporality OTLP metrics&lt;/h3>
&lt;p>OpenTelemetry supports two temporalities, &lt;a href="https://opentelemetry.io/docs/specs/otel/metrics/data-model/#temporality">Delta and Cumulative&lt;/a>.
By default, only the cumulative metrics can be ingested via OTLP endpoint in Cortex.
To enable the ingestion of OTLP metrics with delta temporality, set the &lt;code>distributor.otlp.allow-delta-temporality&lt;/code> flag to &lt;code>true&lt;/code>.&lt;/p>
&lt;h3 id="enable-__type__-and-__unit__-label">Enable &lt;code>__type__&lt;/code> and &lt;code>__unit__&lt;/code> label&lt;/h3>
&lt;p>The &lt;code>__type__&lt;/code> and &lt;code>__unit__&lt;/code> labels are added to OTLP metrics if &lt;code>distributor.otlp.enable-type-and-unit-labels&lt;/code> is set to &lt;code>true&lt;/code>.
This flag is disabled by default.&lt;/p>
&lt;h3 id="configure-promote-resource-attributes-per-tenants">Configure promote resource attributes per tenants&lt;/h3>
&lt;p>The &lt;code>promote_resource_attributes&lt;/code> is a &lt;a href="/docs/guides/overrides-exporter/">runtime config&lt;/a> so you can configure it per tenant.&lt;/p>
&lt;p>For example, this yaml file specifies &lt;code>attr1&lt;/code> being converted to label in both &lt;code>user-1&lt;/code> and &lt;code>user-2&lt;/code>. But, the &lt;code>attr2&lt;/code>
is converted only for &lt;code>user-2&lt;/code>.&lt;/p>
&lt;pre tabindex="0">&lt;code>overrides:
user-1:
promote_resource_attributes: [&amp;#34;attr1&amp;#34;]
user-2:
promote_resource_attributes: [&amp;#34;attr1&amp;#34;, &amp;#34;attr2&amp;#34;]
`
&lt;/code>&lt;/pre></description></item><item><title>Docs: Zone Aware Replication</title><link>/docs/guides/zone-aware-replication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/zone-aware-replication/</guid><description>
&lt;p>Cortex supports data replication for different services. By default, data is transparently replicated across the whole pool of service instances, regardless of whether these instances are all running within the same availability zone (or data center, or rack) or in different ones.&lt;/p>
&lt;p>It is completely possible that all the replicas for the given data are held within the same availability zone, even if the Cortex cluster spans multiple zones. Storing multiple replicas for a given data within the same availability zone poses a risk for data loss if there is an outage affecting various nodes within a zone or a full zone outage.&lt;/p>
&lt;p>For this reason, Cortex optionally supports zone-aware replication. When zone-aware replication is &lt;strong>enabled&lt;/strong>, replicas for the given data are guaranteed to span across different availability zones. This requires the Cortex cluster to run at least in a number of zones equal to the configured replication factor.&lt;/p>
&lt;p>Reads from a zone-aware replication enabled Cortex Cluster can withstand zone failures as long as there are no more than &lt;code>floor(replication factor / 2)&lt;/code> zones with failing instances.&lt;/p>
&lt;p>The Cortex services supporting &lt;strong>zone-aware replication&lt;/strong> are:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="#distributors-and-ingesters-time-series-replication">Distributors and Ingesters&lt;/a>&lt;/strong>&lt;/li>
&lt;li>&lt;strong>&lt;a href="#store-gateways-blocks-replication">Store-gateways&lt;/a>&lt;/strong> (&lt;a href="/docs/blocks-storage/">blocks storage&lt;/a> only)&lt;/li>
&lt;/ul>
&lt;h2 id="distributors--ingesters-time-series-replication">Distributors / Ingesters: time-series replication&lt;/h2>
&lt;p>The Cortex time-series replication is used to hold multiple (typically 3) replicas of each time series in the &lt;strong>ingesters&lt;/strong>.&lt;/p>
&lt;p>&lt;strong>To enable&lt;/strong> the zone-aware replication for the ingesters you should:&lt;/p>
&lt;ol>
&lt;li>Configure the availability zone for each ingester via the &lt;code>-ingester.availability-zone&lt;/code> CLI flag (or its respective YAML config option)&lt;/li>
&lt;li>Rollout ingesters to apply the configured zone&lt;/li>
&lt;li>Enable time-series zone-aware replication via the &lt;code>-distributor.zone-awareness-enabled&lt;/code> CLI flag (or its respective YAML config option). Please be aware that this configuration option should be set to distributors, queriers, and rulers.&lt;/li>
&lt;/ol>
&lt;p>The &lt;code>-distributor.shard-by-all-labels&lt;/code> setting has an impact on read availability. When enabled, a metric is sharded across all ingesters, and the querier needs to fetch series from all ingesters. While, when disabled, a metric is sharded only across &lt;code>&amp;lt;replication factor&amp;gt;&lt;/code> ingesters.&lt;/p>
&lt;p>In the event of a large outage impacting ingesters in more than 1 zone, when &lt;code>-distributor.shard-by-all-labels=true&lt;/code>, all queries will fail, while when disabled, some queries may still succeed if the ingesters holding the required metric are not impacted by the outage. To learn more about this flag, please refer to &lt;a href="/docs/configuration/arguments/#distributor">distributor arguments&lt;/a>.&lt;/p>
&lt;h2 id="store-gateways-blocks-replication">Store-gateways: blocks replication&lt;/h2>
&lt;p>The Cortex &lt;a href="/docs/blocks-storage/store-gateway/">store-gateway&lt;/a> (used only when Cortex is running with the &lt;a href="/docs/blocks-storage/">blocks storage&lt;/a>) supports blocks sharding, used to horizontally scale blocks in a large cluster without hitting any vertical scalability limit.&lt;/p>
&lt;p>To enable the zone-aware replication for the store-gateways, please refer to the &lt;a href="/docs/blocks-storage/store-gateway/#zone-awareness">store-gateway&lt;/a> documentation.&lt;/p>
&lt;h2 id="minimum-number-of-zones">Minimum number of zones&lt;/h2>
&lt;p>For Cortex to function correctly, there must be at least the same number of availability zones as the replication factor. For example, if the replication factor is configured to 3 (default for time-series replication), the Cortex cluster should be spread at least over 3 availability zones.&lt;/p>
&lt;p>It is safe to have more zones than the replication factor, but it cannot be less. Having fewer availability zones than the replication factor causes a replica write to be missed, and in some cases, the write fails if the availability zones count is too low.&lt;/p>
&lt;h2 id="impact-on-unbalanced-zones">Impact on unbalanced zones&lt;/h2>
&lt;p>&lt;strong>Cortex requires that each zone runs the same number of instances&lt;/strong> of a given service for which the zone-aware replication is enabled. This guarantees a fair split of the workload across zones.&lt;/p>
&lt;p>On the contrary, if zones are unbalanced, the zones with a lower number of instances would have a higher pressure on resources utilization (eg. CPU and memory) compared to zones with a higher number of instances.&lt;/p>
&lt;h2 id="impact-on-costs">Impact on costs&lt;/h2>
&lt;p>Depending on the underlying infrastructure being used, deploying Cortex across multiple availability zones may cause an increase in running costs as most cloud providers charge for inter-availability zone networking. The most significant change would be for a Cortex cluster currently running in a single zone.&lt;/p></description></item><item><title>Docs: Parquet Mode</title><link>/docs/guides/parquet-mode/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/parquet-mode/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Parquet mode in Cortex provides an experimental feature that converts TSDB blocks to Parquet format for improved query performance and storage efficiency on older data. This feature is particularly beneficial for long-term storage scenarios where data is accessed less frequently but needs to be queried efficiently.&lt;/p>
&lt;p>The parquet mode consists of two main components:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Parquet Converter&lt;/strong>: Converts TSDB blocks to Parquet format&lt;/li>
&lt;li>&lt;strong>Parquet Queryable&lt;/strong>: Enables querying of Parquet files with fallback to TSDB blocks&lt;/li>
&lt;/ul>
&lt;h2 id="why-parquet-mode">Why Parquet Mode?&lt;/h2>
&lt;p>Traditional TSDB format and Store Gateway architecture face significant challenges when dealing with long-term data storage on object storage:&lt;/p>
&lt;h3 id="tsdb-format-limitations">TSDB Format Limitations&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Random Read Intensive&lt;/strong>: TSDB index relies heavily on random reads, where each read becomes a separate request to object storage&lt;/li>
&lt;li>&lt;strong>Overfetching&lt;/strong>: To reduce object storage requests, data that are close together are merged in a sigle request, leading to higher bandwidth usage and overfetching&lt;/li>
&lt;li>&lt;strong>High Cardinality Bottlenecks&lt;/strong>: Index postings can become a major bottleneck for high cardinality data&lt;/li>
&lt;/ul>
&lt;h3 id="store-gateway-operational-challenges">Store Gateway Operational Challenges&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Resource Intensive&lt;/strong>: Requires significant local disk space for index headers and high memory usage&lt;/li>
&lt;li>&lt;strong>Complex State Management&lt;/strong>: Requires complex data sharding when scaling, which often leads to consistency and availability issues, as well as long startup times&lt;/li>
&lt;li>&lt;strong>Query Inefficiencies&lt;/strong>: Single-threaded block processing leads to high latency for large blocks&lt;/li>
&lt;/ul>
&lt;h3 id="parquet-advantages">Parquet Advantages&lt;/h3>
&lt;p>&lt;a href="https://parquet.apache.org/">Apache Parquet&lt;/a> addresses these challenges through:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Columnar Storage&lt;/strong>: Data organized by columns reduces object storage requests as only specific columns need to be fetched&lt;/li>
&lt;li>&lt;strong>Data Locality&lt;/strong>: Series that are likely to be queried together are co-located to minimize I/O operations&lt;/li>
&lt;li>&lt;strong>Stateless Design&lt;/strong>: Rich file metadata eliminates the need for local state like index headers&lt;/li>
&lt;li>&lt;strong>Advanced Compression&lt;/strong>: Reduces storage costs and improves query performance&lt;/li>
&lt;li>&lt;strong>Parallel Processing&lt;/strong>: Row groups enable parallel processing for better scalability&lt;/li>
&lt;/ul>
&lt;p>For more details on the design rationale, see the &lt;a href="/docs/proposals/parquet-storage/">Parquet Storage Proposal&lt;/a>.&lt;/p>
&lt;h2 id="architecture">Architecture&lt;/h2>
&lt;p>The parquet system works by:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Block Conversion&lt;/strong>: The parquet converter runs periodically to identify TSDB blocks that should be converted to Parquet format&lt;/li>
&lt;li>&lt;strong>Storage&lt;/strong>: Parquet files are stored alongside TSDB blocks in object storage&lt;/li>
&lt;li>&lt;strong>Querying&lt;/strong>: The parquet queryable attempts to query Parquet files first, falling back to TSDB blocks when necessary&lt;/li>
&lt;li>&lt;strong>Marker System&lt;/strong>: Conversion status is tracked using marker files to avoid duplicate conversions&lt;/li>
&lt;/ol>
&lt;h2 id="configuration">Configuration&lt;/h2>
&lt;h3 id="enabling-parquet-converter">Enabling Parquet Converter&lt;/h3>
&lt;p>To enable the parquet converter service, add it to your target list:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">target&lt;/span>: &lt;span style="color:#ae81ff">parquet-converter&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Or include it in a multi-target deployment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">target&lt;/span>: &lt;span style="color:#ae81ff">all,parquet-converter&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="parquet-converter-configuration">Parquet Converter Configuration&lt;/h3>
&lt;p>Configure the parquet converter in your Cortex configuration:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">parquet_converter&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Data directory for caching blocks during conversion&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">data_dir&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;./data&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Frequency of conversion job execution&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">conversion_interval&lt;/span>: &lt;span style="color:#ae81ff">1m&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Maximum rows per parquet row group&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_rows_per_row_group&lt;/span>: &lt;span style="color:#ae81ff">1000000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Number of concurrent meta file sync operations&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">meta_sync_concurrency&lt;/span>: &lt;span style="color:#ae81ff">20&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Enable file buffering to reduce memory usage&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">file_buffer_enabled&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Ring configuration for distributed conversion&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ring&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kvstore&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">store&lt;/span>: &lt;span style="color:#ae81ff">consul&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">consul&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">host&lt;/span>: &lt;span style="color:#ae81ff">localhost:8500&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">heartbeat_period&lt;/span>: &lt;span style="color:#ae81ff">5s&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">heartbeat_timeout&lt;/span>: &lt;span style="color:#ae81ff">1m&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">instance_addr&lt;/span>: &lt;span style="color:#ae81ff">127.0.0.1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">instance_port&lt;/span>: &lt;span style="color:#ae81ff">9095&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="per-tenant-parquet-settings">Per-Tenant Parquet Settings&lt;/h3>
&lt;p>Enable parquet conversion per tenant using limits:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Enable parquet converter for all tenants&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_converter_enabled&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Shard size for shuffle sharding (0 = disabled)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_converter_tenant_shard_size&lt;/span>: &lt;span style="color:#ae81ff">0.8&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Defines sort columns applied during Parquet file generation for specific tenants&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_converter_sort_columns&lt;/span>: [&lt;span style="color:#e6db74">&amp;#34;label1&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;label2&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can also configure per-tenant settings using runtime configuration:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">overrides&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tenant-1&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_converter_enabled&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_converter_tenant_shard_size&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_converter_sort_columns&lt;/span>: [&lt;span style="color:#e6db74">&amp;#34;cluster&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;namespace&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tenant-2&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_converter_enabled&lt;/span>: &lt;span style="color:#66d9ef">false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="enabling-parquet-queryable">Enabling Parquet Queryable&lt;/h3>
&lt;p>To enable querying of Parquet files, configure the querier:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">querier&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Enable parquet queryable with fallback (experimental)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">enable_parquet_queryable&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Cache size for parquet shards&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_queryable_shard_cache_size&lt;/span>: &lt;span style="color:#ae81ff">512&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Default block store: &amp;#34;tsdb&amp;#34; or &amp;#34;parquet&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_queryable_default_block_store&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;parquet&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Disable fallback to TSDB blocks when parquet files are not available&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_queryable_fallback_disabled&lt;/span>: &lt;span style="color:#66d9ef">false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="query-limits-for-parquet">Query Limits for Parquet&lt;/h3>
&lt;p>Configure query limits specific to parquet operations:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Maximum number of rows that can be scanned per query&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_max_fetched_row_count&lt;/span>: &lt;span style="color:#ae81ff">1000000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Maximum chunk bytes per query&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_max_fetched_chunk_bytes&lt;/span>: &lt;span style="color:#ae81ff">100_000_000&lt;/span> &lt;span style="color:#75715e"># 100MB&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Maximum data bytes per query&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_max_fetched_data_bytes&lt;/span>: &lt;span style="color:#ae81ff">1_000_000_000&lt;/span> &lt;span style="color:#75715e"># 1GB&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="cache-configuration">Cache Configuration&lt;/h3>
&lt;p>Parquet mode supports dedicated caching for both chunks and labels to improve query performance. Configure caching in the blocks storage section:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">blocks_storage&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">bucket_store&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Chunks cache configuration for parquet data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">chunks_cache&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">backend&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;memcached&amp;#34;&lt;/span> &lt;span style="color:#75715e"># Options: &amp;#34;&amp;#34;, &amp;#34;inmemory&amp;#34;, &amp;#34;memcached&amp;#34;, &amp;#34;redis&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">subrange_size&lt;/span>: &lt;span style="color:#ae81ff">16000&lt;/span> &lt;span style="color:#75715e"># Size of each subrange for better caching&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_get_range_requests&lt;/span>: &lt;span style="color:#ae81ff">3&lt;/span> &lt;span style="color:#75715e"># Max sub-GetRange requests per GetRange call&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">attributes_ttl&lt;/span>: &lt;span style="color:#ae81ff">168h &lt;/span> &lt;span style="color:#75715e"># TTL for caching object attributes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">subrange_ttl&lt;/span>: &lt;span style="color:#ae81ff">24h &lt;/span> &lt;span style="color:#75715e"># TTL for caching individual chunk subranges&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Memcached configuration (if using memcached backend)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memcached&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">addresses&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;memcached:11211&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">timeout&lt;/span>: &lt;span style="color:#ae81ff">500ms&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_idle_connections&lt;/span>: &lt;span style="color:#ae81ff">16&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_async_concurrency&lt;/span>: &lt;span style="color:#ae81ff">10&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_async_buffer_size&lt;/span>: &lt;span style="color:#ae81ff">10000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_get_multi_concurrency&lt;/span>: &lt;span style="color:#ae81ff">100&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_get_multi_batch_size&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Parquet labels cache configuration (experimental)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">parquet_labels_cache&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">backend&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;memcached&amp;#34;&lt;/span> &lt;span style="color:#75715e"># Options: &amp;#34;&amp;#34;, &amp;#34;inmemory&amp;#34;, &amp;#34;memcached&amp;#34;, &amp;#34;redis&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">subrange_size&lt;/span>: &lt;span style="color:#ae81ff">16000&lt;/span> &lt;span style="color:#75715e"># Size of each subrange for better caching&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_get_range_requests&lt;/span>: &lt;span style="color:#ae81ff">3&lt;/span> &lt;span style="color:#75715e"># Max sub-GetRange requests per GetRange call&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">attributes_ttl&lt;/span>: &lt;span style="color:#ae81ff">168h &lt;/span> &lt;span style="color:#75715e"># TTL for caching object attributes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">subrange_ttl&lt;/span>: &lt;span style="color:#ae81ff">24h &lt;/span> &lt;span style="color:#75715e"># TTL for caching individual label subranges&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Memcached configuration (if using memcached backend)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memcached&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">addresses&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;memcached:11211&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">timeout&lt;/span>: &lt;span style="color:#ae81ff">500ms&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">max_idle_connections&lt;/span>: &lt;span style="color:#ae81ff">16&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="cache-backend-options">Cache Backend Options&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Empty string (&amp;quot;&amp;quot;)&lt;/strong>: Disables caching&lt;/li>
&lt;li>&lt;strong>inmemory&lt;/strong>: Uses in-memory cache (suitable for single-instance deployments)&lt;/li>
&lt;li>&lt;strong>memcached&lt;/strong>: Uses Memcached for distributed caching (recommended for production)&lt;/li>
&lt;li>&lt;strong>redis&lt;/strong>: Uses Redis for distributed caching&lt;/li>
&lt;li>&lt;strong>Multi-level&lt;/strong>: Comma-separated list for multi-tier caching (e.g., &amp;ldquo;inmemory,memcached&amp;rdquo;)&lt;/li>
&lt;/ul>
&lt;h4 id="cache-performance-tuning">Cache Performance Tuning&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>subrange_size&lt;/strong>: Smaller values increase cache hit rates but create more cache entries&lt;/li>
&lt;li>&lt;strong>max_get_range_requests&lt;/strong>: Higher values reduce object storage requests but increase memory usage&lt;/li>
&lt;li>&lt;strong>TTL values&lt;/strong>: Balance between cache freshness and hit rates based on your data patterns&lt;/li>
&lt;li>&lt;strong>Multi-level caching&lt;/strong>: Use &amp;ldquo;inmemory,memcached&amp;rdquo; for L1/L2 cache hierarchy&lt;/li>
&lt;/ul>
&lt;h2 id="block-conversion-logic">Block Conversion Logic&lt;/h2>
&lt;p>The parquet converter determines which blocks to convert based on:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Time Range&lt;/strong>: Only blocks with time ranges larger than the base TSDB block duration (typically 2h) are converted&lt;/li>
&lt;li>&lt;strong>Conversion Status&lt;/strong>: Blocks are only converted once, tracked via marker files&lt;/li>
&lt;li>&lt;strong>Tenant Settings&lt;/strong>: Conversion must be enabled for the specific tenant&lt;/li>
&lt;/ol>
&lt;p>The conversion process:&lt;/p>
&lt;ul>
&lt;li>Downloads TSDB blocks from object storage&lt;/li>
&lt;li>Converts time series data to Parquet format&lt;/li>
&lt;li>Uploads Parquet files (chunks and labels) to object storage&lt;/li>
&lt;li>Creates conversion marker files to track completion&lt;/li>
&lt;/ul>
&lt;h2 id="querying-behavior">Querying Behavior&lt;/h2>
&lt;p>When parquet queryable is enabled:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Block Discovery&lt;/strong>: The bucket index is used to discover available blocks
&lt;ul>
&lt;li>The bucket index now contains metadata indicating whether parquet files are available for querying&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Query Execution&lt;/strong>: Queries prioritize parquet files when available, falling back to TSDB blocks when parquet conversion is incomplete&lt;/li>
&lt;li>&lt;strong>Hybrid Queries&lt;/strong>: Supports querying both parquet and TSDB blocks within the same query operation&lt;/li>
&lt;li>&lt;strong>Fallback Control&lt;/strong>: When &lt;code>parquet_queryable_fallback_disabled&lt;/code> is set to &lt;code>true&lt;/code>, queries will fail with a consistency check error if any required blocks are not available as parquet files, ensuring strict parquet-only querying&lt;/li>
&lt;/ol>
&lt;h2 id="monitoring">Monitoring&lt;/h2>
&lt;h3 id="parquet-converter-metrics">Parquet Converter Metrics&lt;/h3>
&lt;p>Monitor parquet converter operations:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-promql" data-lang="promql">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Blocks converted&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_parquet_converter_blocks_converted_total
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Conversion failures&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_parquet_converter_block_convert_failures_total
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Delay in minutes of Parquet block to be converted from the TSDB block being uploaded to object store&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_parquet_converter_convert_block_delay_minutes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="parquet-queryable-metrics">Parquet Queryable Metrics&lt;/h3>
&lt;p>Monitor parquet query performance:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-promql" data-lang="promql">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Blocks queried by type&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_parquet_queryable_blocks_queried_total
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Query operations&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_parquet_queryable_operations_total
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Cache metrics&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_parquet_queryable_cache_hits_total
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cortex_parquet_queryable_cache_misses_total
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="best-practices">Best Practices&lt;/h2>
&lt;h3 id="deployment-recommendations">Deployment Recommendations&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Dedicated Converters&lt;/strong>: Run parquet converters on dedicated instances for better resource isolation&lt;/li>
&lt;li>&lt;strong>Ring Configuration&lt;/strong>: Use a distributed ring for high availability and load distribution&lt;/li>
&lt;li>&lt;strong>Storage Considerations&lt;/strong>: Ensure sufficient disk space in &lt;code>data_dir&lt;/code> for block processing&lt;/li>
&lt;li>&lt;strong>Network Bandwidth&lt;/strong>: Consider network bandwidth for downloading/uploading blocks&lt;/li>
&lt;/ol>
&lt;h3 id="performance-tuning">Performance Tuning&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Row Group Size&lt;/strong>: Adjust &lt;code>max_rows_per_row_group&lt;/code> based on your query patterns&lt;/li>
&lt;li>&lt;strong>Cache Size&lt;/strong>: Tune &lt;code>parquet_queryable_shard_cache_size&lt;/code> based on available memory&lt;/li>
&lt;li>&lt;strong>Concurrency&lt;/strong>: Adjust &lt;code>meta_sync_concurrency&lt;/code> based on object storage performance&lt;/li>
&lt;li>&lt;strong>Sort Columns&lt;/strong>: Configure &lt;code>parquet_converter_sort_columns&lt;/code> based on your most common query filters to improve query performance&lt;/li>
&lt;/ol>
&lt;h3 id="fallback-configuration">Fallback Configuration&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Gradual Migration&lt;/strong>: Keep &lt;code>parquet_queryable_fallback_disabled: false&lt;/code> (default) during initial deployment to allow queries to succeed even when parquet conversion is incomplete&lt;/li>
&lt;li>&lt;strong>Strict Parquet Mode&lt;/strong>: Set &lt;code>parquet_queryable_fallback_disabled: true&lt;/code> only after ensuring all required blocks have been converted to parquet format&lt;/li>
&lt;li>&lt;strong>Monitoring&lt;/strong>: Monitor conversion progress and query failures before enabling strict parquet mode&lt;/li>
&lt;/ol>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>Experimental Feature&lt;/strong>: Parquet mode is experimental and may have stability issues&lt;/li>
&lt;li>&lt;strong>Storage Overhead&lt;/strong>: Parquet files are stored in addition to TSDB blocks&lt;/li>
&lt;li>&lt;strong>Conversion Latency&lt;/strong>: There&amp;rsquo;s a delay between block creation and parquet availability&lt;/li>
&lt;li>&lt;strong>Shuffle Sharding Requirement&lt;/strong>: Parquet mode only supports shuffle sharding as sharding strategy&lt;/li>
&lt;li>&lt;strong>Bucket Index Dependency&lt;/strong>: The bucket index must be enabled and properly configured as it provides essential metadata for parquet file discovery and query routing&lt;/li>
&lt;/ol>
&lt;h2 id="migration-considerations">Migration Considerations&lt;/h2>
&lt;p>When enabling parquet mode:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Gradual Rollout&lt;/strong>: Enable for specific tenants first&lt;/li>
&lt;li>&lt;strong>Monitor Resources&lt;/strong>: Watch CPU, memory, and storage usage&lt;/li>
&lt;li>&lt;strong>Backup Strategy&lt;/strong>: Ensure TSDB blocks remain available as fallback&lt;/li>
&lt;li>&lt;strong>Testing&lt;/strong>: Thoroughly test query patterns before production deployment&lt;/li>
&lt;/ol></description></item><item><title>Docs: Protecting Cortex from Heavy Queries</title><link>/docs/guides/protecting-cortex-from-heavy-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/protecting-cortex-from-heavy-queries/</guid><description>
&lt;p>PromQL is powerful, and is able to result in query requests that have very wide range of data fetched and samples processed. Heavy queries can cause:&lt;/p>
&lt;ol>
&lt;li>CPU on any query component to be partially exhausted, increasing latency and causing incoming queries to queue up with high chance of time-out.&lt;/li>
&lt;li>CPU on any query component to be fully exhausted, causing GC to slow down leading to the pod being out-of-memory and killed.&lt;/li>
&lt;li>Heap memory on any query component to be exhausted, leading to the pod being out-of-memory and killed.&lt;/li>
&lt;/ol>
&lt;p>It&amp;rsquo;s important to protect Cortex components by setting appropriate limits and throttling configurations based on your infrastructure and data ingested by the customers.&lt;/p>
&lt;h2 id="static-limits">Static limits&lt;/h2>
&lt;p>There are number of static limits that you could configure to block heavy queries from running.&lt;/p>
&lt;h3 id="max-outstanding-requests-per-tenant">Max outstanding requests per tenant&lt;/h3>
&lt;p>See &lt;a href="https://cortexmetrics.io/docs/configuration/configuration-file/#query_frontend_config:~:text=max_outstanding_requests_per_tenant">https://cortexmetrics.io/docs/configuration/configuration-file/#query_frontend_config:~:text=max_outstanding_requests_per_tenant&lt;/a> for details.&lt;/p>
&lt;h3 id="max-data-bytes-fetched-per-sharded-query">Max data bytes fetched per (sharded) query&lt;/h3>
&lt;p>See &lt;a href="https://cortexmetrics.io/docs/configuration/configuration-file/#query_frontend_config:~:text=max_fetched_data_bytes_per_query">https://cortexmetrics.io/docs/configuration/configuration-file/#query_frontend_config:~:text=max_fetched_data_bytes_per_query&lt;/a> for details.&lt;/p>
&lt;h3 id="max-series-fetched-per-sharded-query">Max series fetched per (sharded) query&lt;/h3>
&lt;p>See &lt;a href="https://cortexmetrics.io/docs/configuration/configuration-file/#query_frontend_config:~:text=max_fetched_series_per_query">https://cortexmetrics.io/docs/configuration/configuration-file/#query_frontend_config:~:text=max_fetched_series_per_query&lt;/a> for details.&lt;/p>
&lt;h3 id="max-chunks-fetched-per-sharded-query">Max chunks fetched per (sharded) query&lt;/h3>
&lt;p>See &lt;a href="https://cortexmetrics.io/docs/configuration/configuration-file/#query_frontend_config:~:text=max_fetched_chunk_bytes_per_query">https://cortexmetrics.io/docs/configuration/configuration-file/#query_frontend_config:~:text=max_fetched_chunk_bytes_per_query&lt;/a> for details.&lt;/p>
&lt;h3 id="max-samples-fetched-per-sharded-query">Max samples fetched per (sharded) query&lt;/h3>
&lt;p>See &lt;a href="https://cortexmetrics.io/docs/configuration/configuration-file/#querier_config:~:text=max_samples">https://cortexmetrics.io/docs/configuration/configuration-file/#querier_config:~:text=max_samples&lt;/a> for details.&lt;/p>
&lt;h2 id="resource-based-throttling-experimental">Resource-based throttling (Experimental)&lt;/h2>
&lt;p>Although the static limits are able to protect Cortex components from specific query patterns, they are not generic enough to cover different combinations of bad query patterns. For example, what if the query fetches relatively large postings, series and chunks that are slightly below the individual limits? For a more generic solution, you can enable resource-based throttling by setting CPU and heap utilization thresholds.&lt;/p>
&lt;p>Currently, it only throttles incoming query requests with error code 429 (too many requests) when the resource usage breaches the configured thresholds.&lt;/p>
&lt;p>For example, the following configuration will start throttling query requests if either CPU or heap utilization is above 80%, leaving 20% of room for inflight requests.&lt;/p>
&lt;pre tabindex="0">&lt;code>target: ingester
monitored_resources: cpu,heap
ingester:
query_protection:
rejection:
enabled: true
threshold:
cpu_utilization: 0.8
heap_utilization: 0.8
&lt;/code>&lt;/pre>&lt;p>See &lt;a href="https://cortexmetrics.io/docs/configuration/configuration-file/:~:text=query_protection">https://cortexmetrics.io/docs/configuration/configuration-file/:~:text=query_protection&lt;/a> for details.&lt;/p></description></item><item><title>Docs: Limitations</title><link>/docs/guides/limitations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/limitations/</guid><description>
&lt;h2 id="tenant-id-naming">Tenant ID naming&lt;/h2>
&lt;p>The tenant ID (also called &amp;ldquo;user ID&amp;rdquo; or &amp;ldquo;org ID&amp;rdquo;) is the unique identifier of a tenant within a Cortex cluster. The tenant ID is opaque information to Cortex, which doesn&amp;rsquo;t make any assumptions on its format/content, but its naming has two limitations:&lt;/p>
&lt;ol>
&lt;li>Supported characters&lt;/li>
&lt;li>Length&lt;/li>
&lt;/ol>
&lt;h3 id="supported-characters">Supported characters&lt;/h3>
&lt;p>The following character sets are generally &lt;strong>safe for use in the tenant ID&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Alphanumeric characters
&lt;ul>
&lt;li>&lt;code>0-9&lt;/code>&lt;/li>
&lt;li>&lt;code>a-z&lt;/code>&lt;/li>
&lt;li>&lt;code>A-Z&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Special characters
&lt;ul>
&lt;li>Exclamation point (&lt;code>!&lt;/code>)&lt;/li>
&lt;li>Hyphen (&lt;code>-&lt;/code>)&lt;/li>
&lt;li>Underscore (&lt;code>_&lt;/code>)&lt;/li>
&lt;li>Single Period (&lt;code>.&lt;/code>), but the tenant IDs &lt;code>.&lt;/code> and &lt;code>..&lt;/code> are considered invalid&lt;/li>
&lt;li>Asterisk (&lt;code>*&lt;/code>)&lt;/li>
&lt;li>Single quote (&lt;code>'&lt;/code>)&lt;/li>
&lt;li>Open parenthesis (&lt;code>(&lt;/code>)&lt;/li>
&lt;li>Close parenthesis (&lt;code>)&lt;/code>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>All other characters are not safe to use. In particular, slashes &lt;code>/&lt;/code> and whitespaces (&lt;code> &lt;/code>) are &lt;strong>not supported&lt;/strong>.&lt;/p>
&lt;h3 id="invalid-tenant-ids">Invalid tenant IDs&lt;/h3>
&lt;p>The following tenant IDs are considered invalid in Cortex.&lt;/p>
&lt;ul>
&lt;li>Current directory (&lt;code>.&lt;/code>)&lt;/li>
&lt;li>Parent directory (&lt;code>..&lt;/code>)&lt;/li>
&lt;li>Markers directory (&lt;code>__markers__&lt;/code>)&lt;/li>
&lt;li>User Index File (&lt;code>user-index.json.gz&lt;/code>)&lt;/li>
&lt;/ul>
&lt;h3 id="length">Length&lt;/h3>
&lt;p>The tenant ID length should not exceed 150 bytes/characters.&lt;/p></description></item><item><title>Docs: Glossary</title><link>/docs/guides/glossary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/guides/glossary/</guid><description>
&lt;h3 id="blocks-storage">Blocks storage&lt;/h3>
&lt;p>The blocks storage is a Cortex storage engine based on Prometheus TSDB, which only requires an object store (eg. AWS S3, Google GCS, &amp;hellip;) as backend storage.&lt;/p>
&lt;p>For more information, please refer to the &lt;a href="/docs/blocks-storage/">Cortex blocks storage&lt;/a> documentation.&lt;/p>
&lt;h3 id="chunk">Chunk&lt;/h3>
&lt;p>A chunk is an object containing compressed timestamp-value pairs.&lt;/p>
&lt;p>A single chunk contains timestamp-value pairs for several series.&lt;/p>
&lt;h3 id="churn">Churn&lt;/h3>
&lt;p>Churn is the frequency at which series become idle.&lt;/p>
&lt;p>A series becomes idle once it&amp;rsquo;s not exported anymore by the monitored targets. Typically, series become idle when the monitored target itself disappears (eg. the process or node gets terminated).&lt;/p>
&lt;h3 id="flushing">Flushing&lt;/h3>
&lt;p>Series flushing is the operation run by ingesters to offload time series from memory and store them in the long-term storage.&lt;/p>
&lt;h3 id="ha-tracker">HA Tracker&lt;/h3>
&lt;p>The HA Tracker is a feature of Cortex distributor which is used to deduplicate received series coming from two (or more) Prometheus servers configured in HA pairs.&lt;/p>
&lt;p>For more information, please refer to the guide &amp;ldquo;&lt;a href="/docs/guides/ha-pair-handling/">Config for sending HA Pairs data to Cortex&lt;/a>&amp;rdquo;.&lt;/p>
&lt;h3 id="hash-ring">Hash ring&lt;/h3>
&lt;p>The hash ring is a distributed data structure used by Cortex for sharding, replication, and service discovery. The hash ring data structure gets shared across Cortex replicas via gossip or a key-value store.&lt;/p>
&lt;p>For more information, please refer to the &lt;a href="/docs/architecture/#the-hash-ring">Architecture&lt;/a> documentation.&lt;/p>
&lt;h3 id="org">Org&lt;/h3>
&lt;p>&lt;em>See &lt;a href="#tenant">Tenant&lt;/a>.&lt;/em>&lt;/p>
&lt;h3 id="ring">Ring&lt;/h3>
&lt;p>&lt;em>See &lt;a href="#hash-ring">Hash ring&lt;/a>.&lt;/em>&lt;/p>
&lt;h3 id="sample">Sample&lt;/h3>
&lt;p>A sample is a single timestamped value in a time series.&lt;/p>
&lt;p>For example, given the series &lt;code>node_cpu_seconds_total{instance=&amp;quot;10.0.0.1&amp;quot;,mode=&amp;quot;system&amp;quot;}&lt;/code> its stream of values (samples) could be:&lt;/p>
&lt;pre tabindex="0">&lt;code># Display format: &amp;lt;value&amp;gt; @&amp;lt;timestamp&amp;gt;
11775 @1603812134
11790 @1603812149
11805 @1603812164
11819 @1603812179
11834 @1603812194
&lt;/code>&lt;/pre>&lt;h3 id="series">Series&lt;/h3>
&lt;p>In the Prometheus ecosystem, a series (or time series) is a single stream of timestamped values belonging to the same metric, with the same set of label key-value pairs.&lt;/p>
&lt;p>For example, given a single metric &lt;code>node_cpu_seconds_total&lt;/code> you may have multiple series, each one uniquely identified by the combination of metric name and unique label key-value pairs:&lt;/p>
&lt;pre tabindex="0">&lt;code>node_cpu_seconds_total{instance=&amp;#34;10.0.0.1&amp;#34;,mode=&amp;#34;system&amp;#34;}
node_cpu_seconds_total{instance=&amp;#34;10.0.0.1&amp;#34;,mode=&amp;#34;user&amp;#34;}
node_cpu_seconds_total{instance=&amp;#34;10.0.0.2&amp;#34;,mode=&amp;#34;system&amp;#34;}
node_cpu_seconds_total{instance=&amp;#34;10.0.0.2&amp;#34;,mode=&amp;#34;user&amp;#34;}
&lt;/code>&lt;/pre>&lt;h3 id="tenant">Tenant&lt;/h3>
&lt;p>A tenant (also called &amp;ldquo;user&amp;rdquo; or &amp;ldquo;org&amp;rdquo;) is the owner of a set of series written to and queried from Cortex. Cortex multi-tenancy support allows you to isolate series belonging to different tenants. For example, if you have two tenants &lt;code>team-A&lt;/code> and &lt;code>team-B&lt;/code>, &lt;code>team-A&lt;/code> series will be isolated from &lt;code>team-B&lt;/code>, and each team will be able to query only their own series.&lt;/p>
&lt;p>For more information, please refer to:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="/docs/api/#authentication">HTTP API authentication&lt;/a>&lt;/li>
&lt;li>&lt;a href="/docs/guides/limitations/#tenant-id-naming">Tenant ID limitations&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="time-series">Time series&lt;/h3>
&lt;p>&lt;em>See &lt;a href="#series">Series&lt;/a>.&lt;/em>&lt;/p>
&lt;h3 id="user">User&lt;/h3>
&lt;p>&lt;em>See &lt;a href="#tenant">Tenant&lt;/a>.&lt;/em>&lt;/p>
&lt;h3 id="wal">WAL&lt;/h3>
&lt;p>The Write-Ahead Log (WAL) is an append-only log stored on disk used by ingesters to recover their in-memory state after the process gets restarted, either after a clear shutdown or an abrupt termination.&lt;/p>
&lt;p>For more information, please refer to &lt;a href="/docs/blocks-storage/#the-write-path">Ingesters with WAL&lt;/a>.&lt;/p></description></item></channel></rss>