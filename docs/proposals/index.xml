<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cortex – Proposals</title><link>/docs/proposals/</link><description>Recent content in Proposals on Cortex</description><generator>Hugo -- gohugo.io</generator><atom:link href="/docs/proposals/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Blocks storage bucket index</title><link>/docs/proposals/blocks-storage-bucket-index/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/blocks-storage-bucket-index/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/pracucci">Marco Pracucci&lt;/a>&lt;/li>
&lt;li>Date: November 2020&lt;/li>
&lt;li>Status: draft&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Queriers and store-gateways, at startup and periodically while running, need to scan the entire object store bucket to find tenants and blocks for each tenants.&lt;/p>
&lt;p>For each discovered block, they need to fetch the &lt;code>meta.json&lt;/code> file and check for the existence of &lt;code>deletion-mark.json&lt;/code> (used to signal a block is &amp;ldquo;marked for deletion&amp;rdquo;). The &lt;code>meta.json&lt;/code> file is immutable and gets cached, but &lt;code>deletion-mark.json&lt;/code> can be created at any time and its non-existence can&amp;rsquo;t be safely cached for long (actually our cache is pretty ineffective regarding this).&lt;/p>
&lt;p>The number of blocks in the storage linearly increases with the number of tenants, and so does the number of bucket API calls required to scan the bucket.&lt;/p>
&lt;p>For example, assuming 1 block / day / tenant and a 400d retention, we would have 400 blocks for a single-tenant cluster, but 4M blocks for a 10K tenants cluster (regardless of the number of active series or their QPS).&lt;/p>
&lt;p>We&amp;rsquo;re currently testing a cluster running with 6K tenants and we have faced the following issues:&lt;/p>
&lt;ul>
&lt;li>The cold start of queriers and store-gateways (empty caches) take tens of minutes.&lt;/li>
&lt;li>If we increase the concurrency of bucket operations run to scan the bucket, we hit into object store rate limits and/or the 5xx error rate increases&lt;/li>
&lt;li>The need to run &amp;ldquo;list objects&amp;rdquo; API call for queriers and store-gateways to discover blocks and the check for the existence of &lt;code>deletion-mark.json&lt;/code> (which requires a read operation for each block and can&amp;rsquo;t be cached for a long time) represents a significant % of bucket API calls baseline costs, regardless the tenants QPS (costs we have even if the cluster has 0 queries)&lt;/li>
&lt;/ul>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;p>The goal of this proposal is:&lt;/p>
&lt;ul>
&lt;li>The querier should be up and running without having to scan the bucket at all (zero startup time)&lt;/li>
&lt;li>The querier should not run any &amp;ldquo;list objects&amp;rdquo; operation at anytime&lt;/li>
&lt;li>The querier should require only 1 &amp;ldquo;get object&amp;rdquo; operation to get the entire view of a tenant&amp;rsquo;s blocks&lt;/li>
&lt;/ul>
&lt;h2 id="out-of-scope">Out of scope&lt;/h2>
&lt;p>We believe the same technique described in this proposal could be applied to optimise the store-gateway too (we&amp;rsquo;ve built a &lt;a href="https://github.com/cortexproject/cortex/compare/pracucci:experiment-object-store-based-blocks-index">PoC&lt;/a>), however to keep the design doc easier to discuss we suggest to keep the store-gateway out of scope and address it in a follow-up design doc.&lt;/p>
&lt;h2 id="proposal">Proposal&lt;/h2>
&lt;p>We propose to introduce a per-tenant bucket index. The index is a single JSON file containing two main information: list of all completed (non partial) blocks in the bucket + list of all deletion marks. The bucket index is stored in the bucket within the tenant location (eg. &lt;code>/user-1/bucket-index.json&lt;/code>) and is kept updated by the compactor.&lt;/p>
&lt;p>The querier, at query time, checks whether the bucket index for the tenant has already been loaded in memory. If not, the querier will download it and cache it in memory. Given it&amp;rsquo;s a small file, we expect the lazy download of the bucket index to not significantly impact first query performances.&lt;/p>
&lt;p>While in-memory, a background process will keep it updated at periodic intervals (configurable), so that subsequent queries from the same tenant to the same querier instance will use the cached (and periodically updated) bucket index.&lt;/p>
&lt;p>If a bucket index is unused for a long time (configurable), eg. because that querier instance is not receiving any query from the tenant, the querier will offload it, stopping to keep it updated at regular intervals. This is particularly useful when shuffle sharding is enabled, because a querier will only run queries for a subset of tenants and tenants can be re-sharded across queries in the event of a scale up/down or rolling updates.&lt;/p>
&lt;p>The bucket index will be also cached on memcached for a short period of time, to reduce the number of &amp;ldquo;get object&amp;rdquo; operations in case multiple queriers fetch it in a short period of time (eg. 5 minutes).&lt;/p>
&lt;h3 id="bucket-index-structure">Bucket index structure&lt;/h3>
&lt;p>The &lt;code>bucket-index.json&lt;/code> is a single file containing the following information:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Version&lt;/code>&lt;/li>
&lt;li>List of blocks
&lt;ul>
&lt;li>&lt;code>ID&lt;/code>, &lt;code>MinTime&lt;/code>, &lt;code>MaxTime&lt;/code>, &lt;code>UploadedAt&lt;/code>&lt;/li>
&lt;li>&lt;code>SegmentsFormat&lt;/code> (eg. &amp;ldquo;1-based-6-digits&amp;rdquo;), &lt;code>SegmentsNum&lt;/code> (these two information will be used by the store-gateway)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>List of block deletion marks
&lt;ul>
&lt;li>&lt;code>ID&lt;/code>, &lt;code>DeletionTime&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Timestamp of when the index has been updated&lt;/li>
&lt;/ul>
&lt;p>The bucket index intentionally stores a subset of data for each block&amp;rsquo;s &lt;code>meta.json&lt;/code>, in order to keep the size of the index small. The information in the index is enough to run the querier without having to load the full &lt;code>meta.json&lt;/code> of each block.&lt;/p>
&lt;p>&lt;strong>Size&lt;/strong>: the expected size of the index is about 150 bytes per block. In the case of a tenant with 400 blocks, its bucket index would be 58KB. The size could be further reduced compressing it: experiments show a compression ratio of about 4x using gzip.&lt;/p>
&lt;h3 id="why-the-compactor-writes-the-bucket-index">Why the compactor writes the bucket index&lt;/h3>
&lt;p>There are few reasons why the compactor may be a good candidate to write the bucket index:&lt;/p>
&lt;ul>
&lt;li>The compactor already scans the bucket to have a consistent view of the blocks before running a compaction. Writing the bucket index in the compactor would require no additional bucket API calls.&lt;/li>
&lt;li>Queriers and store-gateways currently read only from the bucket (no writes). We believe that it may be a nice property to preserve.&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-reduce-bucket-api-calls-required-to-discover-deletion-marks">How to reduce bucket API calls required to discover deletion marks&lt;/h3>
&lt;p>The &lt;code>deletion-mark.json&lt;/code> files are stored within each block location. This means that the compactor would still need to run a &amp;ldquo;get object&amp;rdquo; operation for every single block in order to find out which block has it, while updating the bucket index.&lt;/p>
&lt;p>To reduce the number of operations required, we propose to store block deletion marks for all blocks in a per-tenant location (&lt;code>markers/&lt;/code>). Discovering all blocks marked for deletion for a given tenant would only require a single &amp;ldquo;list objects&amp;rdquo; operation on the &lt;code>/&amp;lt;tenant-id&amp;gt;/markers/&lt;/code> prefix. New markers introduced in the future (eg. tenant deletion mark) could be stored in the same location in order to discover all markers with a single &amp;ldquo;list objects&amp;rdquo; operation.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>/user-1/markers/01ER1ZSYHF1FT6RBD8HTVQWX13-deletion-mark.json
/user-1/markers/01ER1ZGSX1Q4B41MK1QQ7RHD33-deletion-mark.json
/user-1/markers/01ER1ZGWKVFT60YMXT8D3XJMDB-deletion-mark.json
&lt;/code>&lt;/pre>&lt;h3 id="what-if-a-bucket-index-gets-stale">What if a bucket index gets stale&lt;/h3>
&lt;p>Queriers and store-gateways don&amp;rsquo;t need a strongly consistent view over the bucket. Even today, given queriers and store-gateways run a periodic scan of the bucket, their view could be stale up to the &amp;ldquo;scan interval&amp;rdquo; (defaults to 15m).&lt;/p>
&lt;p>The maximum delay between a change into the bucket is picked up by queriers and store-gateways depends on the configuration and is the minimum time between:&lt;/p>
&lt;ul>
&lt;li>New blocks uploaded by ingester:
&lt;code>min(-querier.query-ingesters-within, -blocks-storage.tsdb.retention-period)&lt;/code> (default: 6h)&lt;/li>
&lt;li>New blocks uploaded/deleted by compactor:
&lt;code>-compactor.deletion-delay&lt;/code> (default: 12h)&lt;/li>
&lt;/ul>
&lt;p>In order to guarantee consistent query results, we propose to configure the bucket index max stale period in the querier and fail queries if, because of any issue, the bucket index &lt;code>UpdatedAt&lt;/code> age exceeds it.&lt;/p>
&lt;p>The staleness of each bucket index will be tracked via metrics, in order to be alert on it when it&amp;rsquo;s close to expiration (but before it happens).&lt;/p>
&lt;h3 id="object-store-eventual-consistency">Object store eventual consistency&lt;/h3>
&lt;p>Object stores like S3 or GCS do guarantee read-after-create consistency but not read-after-update.&lt;/p>
&lt;p>However, given the analysis done in &amp;ldquo;&lt;a href="#what-if-a-bucket-index-gets-stale">What if a bucket index gets stale&lt;/a>&amp;rdquo; we expect this not to be a real issue, considering we do expect object store reads to not be out of sync for hours.&lt;/p></description></item><item><title>Docs: Blocks storage sharding</title><link>/docs/proposals/blocks-storage-sharding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/blocks-storage-sharding/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/pracucci">Marco Pracucci&lt;/a>&lt;/li>
&lt;li>Date: March 2020&lt;/li>
&lt;li>Status: accepted&lt;/li>
&lt;/ul>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>In Cortex, when using the experimental blocks storage, each querier internally runs the Thanos &lt;a href="https://github.com/thanos-io/thanos/blob/master/pkg/store/bucket.go">&lt;code>BucketStore&lt;/code>&lt;/a>. This means that each querier has a full view over all blocks in the long-term storage and all blocks index headers are loaded in each querier memory. The querier memory usage linearly increase with number and size of all blocks in the storage, imposing a scalability limit to the blocks storage.&lt;/p>
&lt;p>In this proposal we want to solve this. In particular, we want to:&lt;/p>
&lt;ol>
&lt;li>Shard blocks (index headers) across a pool of nodes&lt;/li>
&lt;li>Do not compromise HA on the read path (if a node fails, queries should continue to work)&lt;/li>
&lt;li>Do not compromise correctness (either the query result is correct or it fails)&lt;/li>
&lt;/ol>
&lt;h2 id="proposed-solution">Proposed solution&lt;/h2>
&lt;p>The idea is to introduce a new Cortex service - &lt;code>store-gateway&lt;/code> - internally running the Thanos &lt;a href="https://github.com/thanos-io/thanos/blob/master/pkg/store/bucket.go">&lt;code>BucketStore&lt;/code>&lt;/a>. At query time, a querier will run a query fetching the matching series both from ingesters and the subset of gateways holding the related blocks (based on the query time range). Blocks are replicated across the gateways in order to guarantee query results consistency and HA even in the case of a gateway instance failure.&lt;/p>
&lt;h3 id="ring-based-sharding-and-replication">Ring-based sharding and replication&lt;/h3>
&lt;p>In order to build blocks sharding and replication, the &lt;code>store-gateway&lt;/code> instances form a &lt;a href="/docs/architecture/#the-hash-ring">ring&lt;/a>. Each gateway instance uses a custom &lt;a href="https://github.com/thanos-io/thanos/blob/master/pkg/block/fetcher.go#L108">&lt;code>MetaFetcherFilter&lt;/code>&lt;/a> to filter blocks loaded on the instance itself, keeping only blocks whose &lt;code>hash(block-id)&lt;/code> is within the tokens range assigned to the gateway instance within the ring.&lt;/p>
&lt;p>Within a gateway, the blocks synchronization is triggered in two cases:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Periodically&lt;/strong>&lt;br />
to discover new blocks uploaded by ingesters or compactor, and delete old blocks removed due to retention or by the compactor&lt;/li>
&lt;li>&lt;strong>On-demand&lt;/strong>&lt;br/>
when the ring topology changes (the tokens ranges assigned to the gateway instance have changed)&lt;/li>
&lt;/ol>
&lt;p>It&amp;rsquo;s important to outline that the sync takes time (typically will have to re-scan the bucket and download new blocks index headers) and Cortex needs to guarantee query results consistency at any given time (&lt;em>see below&lt;/em>).&lt;/p>
&lt;h3 id="query-execution">Query execution&lt;/h3>
&lt;p>When a querier executes a query, it will need to fetch series both from ingesters and the store-gateway instances.&lt;/p>
&lt;p>For a given query, the number of blocks to query is expected to be low, especially if the Cortex cluster is running the &lt;code>query-frontend&lt;/code> with a &lt;code>24h&lt;/code> query split interval. In this scenario, whatever is the client&amp;rsquo;s query time range, the &lt;code>query-frontend&lt;/code> will split the client&amp;rsquo;s query into partitioned queries each with up to &lt;code>24h&lt;/code> time range and the querier will likely hit not more than 1 block per partitioned query (except for the last 24h for which blocks may have not been compacted yet).&lt;/p>
&lt;p>Given this assumption, we want to avoid sending every query to every store-gateway instance. The querier should be able to take an informed decision about the minimum subset of store-gateway instances which needs to query given a time range.&lt;/p>
&lt;p>The idea is to run the &lt;a href="https://github.com/thanos-io/thanos/blob/master/pkg/block/fetcher.go#L127">&lt;code>MetaFetcher&lt;/code>&lt;/a> also within the querier, but without any sharding filter (contrary to the store-gateway). At any given point in time, the querier knows the entire list of blocks in the storage. When the querier executes the &lt;code>Select()&lt;/code> (or &lt;code>SelectSorted()&lt;/code>) it does:&lt;/p>
&lt;ol>
&lt;li>Compute the list of blocks by the query time range&lt;/li>
&lt;li>Compute the minimum list of store-gateway instances containing the required blocks (using the information from the ring)&lt;/li>
&lt;li>Fetch series from ingesters and the matching store-gateway instances&lt;/li>
&lt;li>Merge and deduplicate received series
&lt;ul>
&lt;li>Optimization: can be skipped if the querier hits only 1 store-gateway&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="query-results-consistency">Query results consistency&lt;/h3>
&lt;p>When a querier executes a query, it should guarantee that either all blocks matching the time range are queried or the query fails.&lt;/p>
&lt;p>However, due to the (intentional) lack of a strong coordination between queriers and store-gateways, and the ring topology which can change any time, there&amp;rsquo;s no guarantee that the blocks assigned to a store-gateway shard are effectively loaded on the store-gateway itself at any given point in time.&lt;/p>
&lt;p>The idea is introduce a &lt;strong>consistency check in the querier&lt;/strong>. When a store-gateway receives a request from the querier, the store-gateway includes in the response the list of block IDs currently loaded on the store-gateway itself. The querier can then merge the list of block IDs received from all store-gateway hit, and match it against the list of block IDs computed at the beginning of the query execution.&lt;/p>
&lt;p>There are three possible scenarios:&lt;/p>
&lt;ol>
&lt;li>The list match: all good&lt;/li>
&lt;li>All the blocks known by the querier are within the list of blocks returned by store-gateway, but the store-gateway also included blocks unknown to the querier: all good (it means the store-gateways have discovered and loaded new blocks before the querier discovered them)&lt;/li>
&lt;li>Some blocks known by the querier are &lt;strong>not&lt;/strong> within the list of blocks returned by store-gateway: potential consistency issue&lt;/li>
&lt;/ol>
&lt;p>We want to protect from a partial results response which may occur in the case #3. However, there are some legit cases which, if not handled, would lead to frequent false positives. Given the querier and store-gateway instances independently scan the bucket at a regular interval (to find new blocks or deleted blocks), we may be in one of the following cases:&lt;/p>
&lt;p>a. The querier has discovered new blocks before the store-gateway successfully discovered and loaded them
b. The store-gateway has offloaded blocks &amp;ldquo;marked for deletion&amp;rdquo; before the querier&lt;/p>
&lt;p>To protect from case (a), we can exclude the blocks which have been uploaded in the last &lt;code>X&lt;/code> time from the consistency check (same technique already used in other Thanos components). This &lt;code>X&lt;/code> delay time is used to give the store-gateway enough time to discover and load new blocks, before the querier consider them for the consistency check. This value &lt;code>X&lt;/code> should be greater than the &lt;code>-experimental.blocks-storage.bucket-store.consistency-delay&lt;/code>, because we do expect the querier to consider a block for consistency check once it&amp;rsquo;s reasonably safe to assume that its store-gateway already loaded it.&lt;/p>
&lt;p>To protect from case (b) we need to understand how blocks are offloaded. The &lt;code>BucketStore&lt;/code> (running within the store-gateway) offloads a block as soon as it&amp;rsquo;s not returned by the &lt;code>MetaFetcher&lt;/code>. This means we can configure the &lt;code>MetaFetcher&lt;/code> with a &lt;a href="https://github.com/thanos-io/thanos/blob/4bd19b16a752e9ceb1836c21d4156bdeb517fe50/pkg/block/fetcher.go#L648">&lt;code>IgnoreDeletionMarkFilter&lt;/code>&lt;/a> with a delay of &lt;code>X&lt;/code> (could be the same value used for case (a)) and in the querier exclude the blocks which have been marked for deletion more than &lt;code>X&lt;/code> time ago from the consistency check.&lt;/p>
&lt;h2 id="trade-offs">Trade-offs&lt;/h2>
&lt;p>The proposed solution comes with the following trade-offs:&lt;/p>
&lt;ul>
&lt;li>A querier is not ready until it has completed an initial full scan of the bucket, downloading the &lt;code>meta.json&lt;/code> file of every block&lt;/li>
&lt;li>A store-gateway is not ready until it has completed an initial full scan of the bucket, downloading the &lt;code>meta.json&lt;/code> and index header of each block matching its shard&lt;/li>
&lt;li>If a querier hits 2+ store-gateways it may receive duplicated series if the 2+ store-gateways share some blocks due to the replication factor&lt;/li>
&lt;/ul></description></item><item><title>Docs: Cross-Tenant Query Federation</title><link>/docs/proposals/cross-tenant-query-federation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/cross-tenant-query-federation/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/simonswine">Christian Simon&lt;/a>&lt;/li>
&lt;li>Date: October 2020&lt;/li>
&lt;li>Status: Accepted&lt;/li>
&lt;/ul>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>This document aims to describe how to implement the ability to allow queries to cover data from more than a single Cortex tenant.&lt;/p>
&lt;h2 id="reasoning">Reasoning&lt;/h2>
&lt;p>Adopting a tenancy model within an organization with each tenant representing a department comes with the disadvantage that it will prevent queries from spanning multiple departments. This proposal tries to overcome those limitations.&lt;/p>
&lt;h2 id="alternatives-considered">Alternatives considered&lt;/h2>
&lt;h3 id="aggregation-in-promql-api-clients">Aggregation in PromQL API clients&lt;/h3>
&lt;p>In theory PromQL API clients could be aggregating/correlating query results from multiple tenants. For example Grafana could be used with multiple data sources and a cross tenant query could be achieved through using &lt;a href="https://grafana.com/docs/grafana/latest/panels/transformations/">Transformations&lt;/a>.&lt;/p>
&lt;p>As this approach comes with the following disadvantages, it was not considered further:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Every PromQL API client needs to support the aggregation from various sources.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Queries that are written in PromQL can&amp;rsquo;t be used without extra work across tenants.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="multi-tenant-aggregation-in-the-query-frontends">Multi tenant aggregation in the query frontends&lt;/h3>
&lt;p>Another approach to multi tenant query federation could be achieved by aggregation of partial query results within the query frontend. For this a query needs to be split into sub queries per tenant and afterwards the partial results need reduced into the final result.&lt;/p>
&lt;p>The &lt;a href="https://github.com/cortexproject/cortex/blob/f0c81bb59bf202db820403812e8dabcb64347bfd/pkg/querier/astmapper/parallel.go#L27">astmapper&lt;/a> package goes down a similar approach, but it cannot parallelize all query types. Ideally multi-tenant query federation should support the full PromQL language and the algorithms necessary would differ per query functions and operators used. This approach was deemed as a fairly complex way to achieve that tenant query federation.&lt;/p>
&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;h3 id="aggregate-data-without-overlaps">Aggregate data without overlaps&lt;/h3>
&lt;h4 id="challenge">Challenge&lt;/h4>
&lt;p>The series in different tenants might have exactly the same labels and hence potentially collide which each other.&lt;/p>
&lt;h4 id="proposal">Proposal&lt;/h4>
&lt;p>In order to be able to always identify the tenant correctly, queries using multiple tenants should inject a tenant label named &lt;code>__tenant_id__&lt;/code> and its value containing the tenant ID into the results. A potentially existing label with the same name should be stored in a label with the prefix &lt;code>original_&lt;/code>.&lt;/p>
&lt;p>Label selectors containing the tenant label should behave like any other labels. This can be achieved by selecting the tenants used in a multi tenant query.&lt;/p>
&lt;h3 id="exposure-of-feature-to-the-user">Exposure of feature to the user&lt;/h3>
&lt;h4 id="challenge-1">Challenge&lt;/h4>
&lt;p>The tenant ID is currently read from the &lt;code>X-Scope-OrgID&lt;/code> HTTP header. The tenant ID has those &lt;a href="https://cortexmetrics.io/docs/guides/limitations/#tenant-id-naming">documented limitations&lt;/a> of values being allowed.&lt;/p>
&lt;h4 id="proposal-1">Proposal&lt;/h4>
&lt;p>For the query path a user should be able to specify a &lt;code>X-Scope-OrgID&lt;/code> header with multiple tenant IDs. Multiple tenant IDs should then be propagating throughout out the system until it reaches the querier. The &lt;code>Queryable&lt;/code> instance returned by the querier component, is wrapped by a &lt;code>mergeQueryable&lt;/code>, which will aggregate the results from a &lt;code>Queryable&lt;/code> per tenant and hence treated by the downstream components as a single tenant query.&lt;/p>
&lt;p>To allow such queries to be processed we suggest that an experimental configuration flag &lt;code>-querier.tenant-federation.enabled&lt;/code> will be added, which is switched off by default. Once enabled the value of the &lt;code>X-Scope-OrgID&lt;/code> header should be interpreted as &lt;code>|&lt;/code> separated list of tenant ids. Components which are not expecting multiple tenant ids (e.g. the ingress path) must signal an error if multiple are used.&lt;/p>
&lt;h3 id="implementing-limits-fairness-and-observability-for-cross-tenant-queries">Implementing Limits, Fairness and Observability for Cross-Tenant queries&lt;/h3>
&lt;h4 id="challenge-2">Challenge&lt;/h4>
&lt;p>In Cortex the tenant id is used as the primary identifier for those components:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The limits that apply to a certain query.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The query-frontend maintains a per tenant query queue to implement fairness.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Relevant metrics about the query are exposed under a &lt;code>user&lt;/code> label.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Having a query spanning multiple tenants, the existing methods are no longer correct.&lt;/p>
&lt;h4 id="proposal-2">Proposal&lt;/h4>
&lt;p>The identifier for aforementioned features for queries involving more than a single tenant should be derived from: An ordered, distinct list of tenant IDs, which is joined by a &lt;code>|&lt;/code>. This will produce a reproducible identifier for the same set of tenants no matter which order they have been specified.&lt;/p>
&lt;p>While this feature is considered experimental, this provides some insights and ability to limit multi-tenant queries with these short comings:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Cardinality costs to the possible amount of tenant ID combinations.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Query limits applied to single tenants part of a multi tenant query are ignored.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Challenge&lt;/th>
&lt;th>Status&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Aggregate data without overlap&lt;/td>
&lt;td>Implementation in PR &lt;a href="https://github.com/cortexproject/cortex/pull/3250">#3250&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Exposure of feature to the user&lt;/td>
&lt;td>Implementation in PR &lt;a href="https://github.com/cortexproject/cortex/pull/3250">#3250&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Implementing Limits, Fairness and Observability for Cross-Tenant queries&lt;/td>
&lt;td>Implementation in PR &lt;a href="https://github.com/cortexproject/cortex/pull/3250">#3250&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="future-work">Future work&lt;/h3>
&lt;p>Those features are currently out of scope for this proposal, but we can foresee some interest implementing them after this proposal.&lt;/p>
&lt;h4 id="cross-tenant-support-for-the-ruler">Cross Tenant support for the ruler&lt;/h4>
&lt;p>Ability to use multi tenant queries in the ruler.&lt;/p>
&lt;h4 id="allow-the-identifier-for-limits-fairness-and-observability-to-be-switched-out">Allow the identifier for limits, fairness and observability to be switched out&lt;/h4>
&lt;p>It would be great if the source identifier could be made more pluggable. This could allow to for example base all of those features on another dimension (e.g. users rather than tenants)&lt;/p>
&lt;h4 id="allow-customisation-of-the-label-used-to-expose-tenant-ids">Allow customisation of the label used to expose tenant ids&lt;/h4>
&lt;p>As per this proposal the label name &lt;code>__tenant_id__&lt;/code> is fixed, but users might want to be able to modify that through a configuration option.&lt;/p>
&lt;h4 id="retain-overlapping-tenant-id-label-values-recursively">Retain overlapping tenant id label values recursively&lt;/h4>
&lt;p>As per this proposal the tenant label injection retains an existing label value, but this is not implemented recursively. So if the result already contains &lt;code>__tenant_id__&lt;/code> and &lt;code>original__tenant_id__&lt;/code> labels, the value of the latter would be lost.&lt;/p></description></item><item><title>Docs: Deletion of Tenant Data from Blocks Storage</title><link>/docs/proposals/tenant-deletion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/tenant-deletion/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/pstibrany">Peter Stibrany&lt;/a>&lt;/li>
&lt;li>Date: November 2020&lt;/li>
&lt;li>Status: Accepted&lt;/li>
&lt;/ul>
&lt;h2 id="deletion-of-tenant-data">Deletion of tenant data&lt;/h2>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>When a tenant is deleted from the external system that controls access to Cortex, we want to clean up tenants data from Cortex as well.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>When using blocks storage, Cortex stores tenant’s data in several places:&lt;/p>
&lt;ul>
&lt;li>object store for long-term storage of blocks,&lt;/li>
&lt;li>ingesters disks for short-term storage. Ingesters eventually upload data to long-term storage,&lt;/li>
&lt;li>various caches: query-frontend, chunks, index and metadata,&lt;/li>
&lt;li>object store for rules (separate from blocks),&lt;/li>
&lt;li>object store for alert manager configuration,&lt;/li>
&lt;li>state for alertmanager instances (notifications and silences).&lt;/li>
&lt;/ul>
&lt;p>This document expects that there is an external authorization system in place.
Disabling or deleting the tenant in this system will stop tenants data, queries and other API calls from reaching Cortex.
Note that there may be a delay before disabling or deleting the user, until data / queries fully stop, due to eg. caching in authorization proxies.
Cortex endpoint for deleting a tenant data should be only called after this blocking is in place.&lt;/p>
&lt;h2 id="proposal">Proposal&lt;/h2>
&lt;h3 id="api-endpoints">API Endpoints&lt;/h3>
&lt;h4 id="post-purgerdelete_tenant">POST /purger/delete_tenant&lt;/h4>
&lt;p>We propose to introduce an &lt;code>/purger/delete_tenant&lt;/code> API endpoint to trigger data deletion for the tenant.&lt;/p>
&lt;p>While this endpoint works as an “admin” endpoint and should not be exposed directly to tenants, it still needs to know under which tenant it should operate.
For consistency with other endpoints this endpoint would therefore require an &lt;code>X-Scope-OrgID&lt;/code> header and use the tenant from it.&lt;/p>
&lt;p>It is safe to call “delete_tenant” multiple times.&lt;/p>
&lt;h4 id="get-purgerdelete_tenant_status">GET /purger/delete_tenant_status&lt;/h4>
&lt;p>To monitor the state of data deletion, another endpoint will be available: &lt;code>/purger/delete_tenant_status&lt;/code>.
This will return OK if tenant’s data has been fully deleted – no more blocks exist on the long-term storage, alertmanager and rulers are fully stopped and configuration removed.
Similarly to “delete_tenant” endpoint, “delete_tenant_status” will require &lt;code>X-Scope-OrgID&lt;/code> header.&lt;/p>
&lt;h3 id="implementation-asynchronous">Implementation (asynchronous)&lt;/h3>
&lt;p>Purger will implement both API endpoints.
Upon receiving the call to &lt;code>/purger/delete_tenant&lt;/code> endpoint, Purger will initiate the deletion by writing “deletion marker” objects for specific tenant to following buckets:&lt;/p>
&lt;ul>
&lt;li>Blocks bucket&lt;/li>
&lt;li>Ruler configuration bucket&lt;/li>
&lt;li>Alertmanager configuration bucket&lt;/li>
&lt;/ul>
&lt;p>Deletion marker for the tenant will be an object stored under tenant prefix, eg. “/&lt;tenant>/deleted”.
This object will contain the timestamp when it was created, so that we can delete it later based on the specified time period.
We could reuse a subset of the proposed Thanos tombstones format, or use custom format.&lt;/p>
&lt;p>“delete_tenant_status” endpoint will report success, if all of the following are true:&lt;/p>
&lt;ul>
&lt;li>There are no blocks for the tenant in the blocks bucket&lt;/li>
&lt;li>There is a “deletion finished” object in the bucket for the tenant in the ruler configuration bucket.&lt;/li>
&lt;li>There is a “deletion finished” object in the bucket for the tenant in the alertmanager configuration bucket.&lt;/li>
&lt;/ul>
&lt;p>See later sections on Ruler and Alertmanager for explanation of “deletion finished” objects.&lt;/p>
&lt;h3 id="blocks-deletion-marker">Blocks deletion marker&lt;/h3>
&lt;p>Blocks deletion marker will be used by compactor, querier and store-gateway.&lt;/p>
&lt;h4 id="compactor">Compactor&lt;/h4>
&lt;p>Upon discovering the blocks deletion marker, the compactor will start deletion of all blocks that belong to the tenant.
This can take hours to finish, depending on the number of blocks.
Even after deleting all blocks on the storage, ingesters may upload additional blocks for the tenant.
To make sure that the compactor deletes these blocks, the compactor will keep the deletion marker in the bucket.
After a configurable period of time it can delete the deletion marker too.&lt;/p>
&lt;p>To implement deletion, Compactor should use new TenantsCleaner component similar to existing BlocksCleaner (which deletes blocks marked for deletion), and modify UserScanner to ignore deleted tenants (for BlocksCleaner and Compactor itself) or only return deleted tenants (for TenantsCleaner).&lt;/p>
&lt;h4 id="querier-store-gateway">Querier, Store-gateway&lt;/h4>
&lt;p>These two components scan for all tenants periodically, and can use the tenant deletion marker to skip tenants and avoid loading their blocks into memory and caching to disk (store-gateways).
By implementing this, store-gateways will also unload and remove cached blocks.&lt;/p>
&lt;p>Queriers and store-gateways use metadata cache and chunks cache when accessing blocks to reduce the number of API calls to object storage.
In this proposal we don’t suggest removing obsolete entries from the cache – we will instead rely on configured expiration time for cache items.&lt;/p>
&lt;p>Note: assuming no queries will be sent to the system once the tenant is deleted, implementing support for tenant deletion marker in queriers and store-gateways is just an optimization.
These components will unload blocks once they are deleted from the object store even without this optimization.&lt;/p>
&lt;h4 id="query-frontend">Query Frontend&lt;/h4>
&lt;p>While query-frontend could use the tenant blocks deletion marker to clean up the cache, we don’t suggest to do that due to additional complexity.
Instead we will only rely on eventual eviction of cached query results from the cache.
It is possible to configure Cortex to set TTL for items in the frontend cache by using &lt;code>-frontend.default-validity&lt;/code> option.&lt;/p>
&lt;h4 id="ingester">Ingester&lt;/h4>
&lt;p>Ingesters don’t scan object store for tenants.&lt;/p>
&lt;p>To clean up the local state on ingesters, we will implement closing and deletion of local per-tenant data for idle TSDB. (See Cortex PR #3491).
This requires additional configuration for ingesters, specifically how long to wait before closing and deleting TSDB.
This feature needs to work properly in two different scenarios:&lt;/p>
&lt;ul>
&lt;li>Ingester is no longer receiving data due to ring changes (eg. scale up of ingesters)&lt;/li>
&lt;li>Data is received because user has been deleted.&lt;/li>
&lt;/ul>
&lt;p>Ingester doesn’t distinguish between the two at the moment.
To make sure we don’t break queries by accidentally deleting TSDB too early, ingester needs to wait at least &lt;code>-querier.query-ingesters-within&lt;/code> duration.&lt;/p>
&lt;p>Alternatively, ingester could check whether deletion marker exists on the block storage, when it detects idle TSDB.&lt;/p>
&lt;p>&lt;strong>If more data is pushed to the ingester for a given tenant, ingester will open new TSDB, build new blocks and upload them. It is therefore essential that no more data is pushed to Cortex for the tenant after calling the “delete_tenant” endpoint.&lt;/strong>&lt;/p>
&lt;h4 id="ruler-deletion-marker">Ruler deletion marker&lt;/h4>
&lt;p>This deletion marker is stored in the ruler configuration bucket.
When rulers discover this marker during the periodic sync of the rule groups, they will&lt;/p>
&lt;ul>
&lt;li>stop the evaluation of the rule groups for the user,&lt;/li>
&lt;li>delete tenant rule groups (when multiple rulers do this at the same time, they will race for deletion, and need to be prepared to handle possible “object not found” errors)&lt;/li>
&lt;li>delete local state.&lt;/li>
&lt;/ul>
&lt;p>When the ruler is finished with this cleanup, it will ask all other rulers if they still have any data for the tenant.
If all other rulers reply with “no”, ruler can write “deletion finished” marker back to the bucket.
This allows rulers to ignore the ruler completely, and it also communicates the status of the deletion back to purger.&lt;/p>
&lt;p>Note that ruler currently relies on cached local files when using Prometheus Ruler Manager component.
&lt;a href="https://github.com/cortexproject/cortex/issues/3134">This can be avoided now&lt;/a>, and since it makes cleanup simpler, we suggest to modify Cortex ruler implementation to avoid this local copy.&lt;/p>
&lt;p>&lt;strong>Similarly to ingesters, it is necessary to disable access to the ruler API for deleted tenant.
This must be done in external authorization proxy.&lt;/strong>&lt;/p>
&lt;h4 id="alertmanager-deletion-marker">Alertmanager deletion marker&lt;/h4>
&lt;p>Deletion marker for alert manager is stored in the alertmanager configuration bucket. Cleanup procedure for alertmanager data is similar to rulers – when individual alertmanager instances discover the marker, they will:&lt;/p>
&lt;ul>
&lt;li>Delete tenant configuration&lt;/li>
&lt;li>Delete local notifications and silences state&lt;/li>
&lt;li>Ask other alertmanagers if they have any tenant state yet, and if not, write “deletion finished” marker back to the bucket.&lt;/li>
&lt;/ul>
&lt;p>To perform the last step, Alertmanagers need to find other alertmanager instances. This will be implemented by using the ring, which will (likely) be added as per &lt;a href="https://github.com/cortexproject/cortex/pull/3574">Alertmanager scaling proposal&lt;/a>.&lt;/p>
&lt;p>Access to Alertmanager API must be disabled for tenant that is going to be deleted.&lt;/p>
&lt;h2 id="alternatives-considered">Alternatives Considered&lt;/h2>
&lt;p>Another possibility how to deal with tenant data deletion is to make purger component actively communicate with ingester, compactor, ruler and alertmanagers to make data deletion faster. In this case purger would need to understand how to reach out to all those components (with multiple ring configurations, one for each component type), and internal API calls would need to have strict semantics around when the data deletion is complete. This alternative has been rejected due to additional complexity and only small benefit in terms of how fast data would be deleted.&lt;/p></description></item><item><title>Docs: Documentation Versioning</title><link>/docs/proposals/documentation-versioning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/documentation-versioning/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/jaybatra26">Jay Batra&lt;/a>&lt;/li>
&lt;li>Date: March 2020&lt;/li>
&lt;li>Status: proposal&lt;/li>
&lt;/ul>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>In Cortex, currently, we are missing versioning of documentation. The idea is to have version documentation just like Prometheus.&lt;a href="https://prometheus.io/docs/introduction/overview/">&lt;code>Prometheus&lt;/code>&lt;/a>. Documentation is the main source of information for current contributors and first-timers. A properly versioned documentation will help everyone to have a proper place to look for answers before flagging it in the community.&lt;/p>
&lt;p>In this proposal, we want to solve this. In particular, we want to:&lt;/p>
&lt;ol>
&lt;li>Version specific pages of the documentation&lt;/li>
&lt;li>Include links to change version (the version must be in the URL)&lt;/li>
&lt;li>Include the master version and last 3 minor releases. Documentation defaults to the last minor release.&lt;/li>
&lt;/ol>
&lt;h2 id="proposed-solution">Proposed solution&lt;/h2>
&lt;p>Currently, the documentation is residing under the docs/ folder of cortexproject/cortex. It is built by Hugo using the theme &lt;a href="https://www.docsy.dev">&lt;code>docsy&lt;/code>&lt;/a>. It will have a proper &lt;a href="https://www.docsy.dev/docs/adding-content/versioning/#adding-a-version-drop-down-menu">&lt;code>drop-down menu&lt;/code>&lt;/a> which will enable proper versioning. It has a section &lt;a href="https://www.docsy.dev/docs/adding-content/versioning/#adding-a-version-drop-down-menu">&lt;code>params.version&lt;/code>&lt;/a> in config.toml which will allow us to map URLs with proper versions. We will have to change all the occurrences of older doc links with new links. We will keep &lt;code>master&lt;/code> version with 3 latest &lt;code>release&lt;/code> versions. Each release is a minor version expressed as &lt;code>1.x&lt;/code>. The document would default to latest minor version.&lt;/p>
&lt;p>From the current doc, the following paths (and all their subpages) should be versioned for now:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://cortexmetrics.io/docs/apis/">https://cortexmetrics.io/docs/apis/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cortexmetrics.io/docs/configuration/">https://cortexmetrics.io/docs/configuration/&lt;/a> (moving v1.x Guarantees outside of the tree, because these shouldn&amp;rsquo;t be versioned)&lt;/li>
&lt;/ol>
&lt;p>The above should be versioned under a single URL path (&lt;code>/docs/running-cortex/&lt;/code> in the following example, but final prefix is still to be decided).&lt;/p>
&lt;h3 id="example">Example:&lt;/h3>
&lt;p>For &lt;code>master&lt;/code> version we would be able to use the above links via the following path&lt;/p>
&lt;pre tabindex="0">&lt;code>/docs/running-cortex/master/configuration/
/docs/running-cortex/master/api/
&lt;/code>&lt;/pre>&lt;p>And for a minor version like &lt;code>1.x&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>/docs/running-cortex/1.0/configuration/
/docs/running-cortex/1.0/apis/
&lt;/code>&lt;/pre>&lt;p>we&amp;rsquo;ll have versioned documentation only under the /docs/running-cortex/ prefix and, as a starting point, all versioned pages should go there.&lt;/p></description></item><item><title>Docs: Generalize Modules Service to make it extensible</title><link>/docs/proposals/generalize-modules/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/generalize-modules/</guid><description>
&lt;ul>
&lt;li>Author: @annanay25&lt;/li>
&lt;li>Reviewers: @jtlisi, @pstibrany, @cyriltovena, @pracucci&lt;/li>
&lt;li>Date: April 2020&lt;/li>
&lt;li>Status: Accepted&lt;/li>
&lt;/ul>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Cortex uses modules to start and operate services with dependencies. Inter-service dependencies are specified in a map and passed to a module manager which ensures that they are initialised in the right order of dependencies. While this works really well, the implementation is tied in specifically to the Cortex struct and is not flexible for use with other projects like Loki, which also require similar forms of dependency management.&lt;/p>
&lt;p>We would like to extend modules in cortex to a generic dependency management framework, that can be used by any project with no ties to cortex.&lt;/p>
&lt;h2 id="specific-goals">Specific goals&lt;/h2>
&lt;ul>
&lt;li>Framework should allow for reusing cortex modules and allow us to:
&lt;ul>
&lt;li>Add new modules&lt;/li>
&lt;li>Overwrite the implementation of a current module&lt;/li>
&lt;li>Manage dependencies&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Framework should allow for building an application from scratch using the &lt;code>modules&lt;/code> package, with no dependencies on Cortex. For ex: Remove code from Loki that was copied from &lt;code>pkg/cortex/cortex.go&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="proposed-design">Proposed Design&lt;/h2>
&lt;h3 id="modules-package">Modules package&lt;/h3>
&lt;p>To make the modules package extensible, we need to abstract away any Cortex specific details from the module manager. The proposed design is to:&lt;/p>
&lt;ul>
&lt;li>Make a new component &lt;code>Manager&lt;/code>, which is envisioned to be a central manager for all modules of the application. It stores modules &amp;amp; dependencies, and will be housed under a new package &lt;code>pkg/util/modules&lt;/code>. &lt;code>Manager&lt;/code> has the following methods for interaction:&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code> func (m *Manager) RegisterModule(name string, initFn func() (Service, error))
func (m *Manager) AddDependency(name string, dependsOn... string) error
func (m *Manager) InitModuleServices(target string) (map[string]services.Service, error)
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;p>Modules can be created by the application and registered with &lt;code>modules.Manager&lt;/code> using &lt;code>RegisterModule&lt;/code>. The parameters are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>name&lt;/code>: Name of the module&lt;/li>
&lt;li>&lt;code>initFn&lt;/code>: A function that will be used to start the module. If it returns nil, and other modules depend on it, &lt;code>InitModuleServices&lt;/code> will return an error.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Dependencies between modules can be added using &lt;code>AddDependency&lt;/code>. The parameters to the function are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>name&lt;/code>: Name of the module&lt;/li>
&lt;li>&lt;code>dependsOn&lt;/code>: A variadic list of modules that the module depends on.&lt;/li>
&lt;/ul>
&lt;p>These need to be added before the call to &lt;code>InitModuleServices&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The application can be initialized by running &lt;code>initFn&lt;/code>&amp;rsquo;s of all the modules in the right order of dependencies by invoking &lt;code>InitModuleServices&lt;/code> with the target module name.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="changes-to-pkgcortex">Changes to &lt;code>pkg/cortex&lt;/code>:&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;code>WrappedService&lt;/code> present in the current &lt;code>module&lt;/code> design will be deprecated. All &lt;code>initFn&lt;/code>&amp;rsquo;s will be wrapped into &lt;code>WrappedService&lt;/code> by default.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>While the process of loading modules into &lt;code>modules.Manager&lt;/code> should be remain as part of the &lt;code>Cortex.New()&lt;/code> function, &lt;code>InitModuleServices&lt;/code> should be part of &lt;code>Cortex.Run()&lt;/code> and to enable this, &lt;code>modules.Manager&lt;/code> would be made a member of the &lt;code>Cortex&lt;/code> struct.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="usage">Usage&lt;/h2>
&lt;p>Following these changes, the Modules package will be a generic dependency management framework that can be used by any project.&lt;/p>
&lt;h4 id="to-use-the-modules-framework">To use the modules framework:&lt;/h4>
&lt;ul>
&lt;li>Import the &lt;code>pkg/util/modules&lt;/code> package, and initialize a new instance of the &lt;code>Manager&lt;/code> using &lt;code>modules.NewManager()&lt;/code>&lt;/li>
&lt;li>Create components in the system that implement the services interface (present in &lt;code>pkg/util/services&lt;/code>).&lt;/li>
&lt;li>Register each of these components as a module using &lt;code>Manager.RegisterModule()&lt;/code> by passing name of the module and &lt;code>initFn&lt;/code> for the module.&lt;/li>
&lt;li>To add dependencies between modules, use &lt;code>Manager.AddDependency()&lt;/code>&lt;/li>
&lt;li>Once all modules are added into &lt;code>modules.Manager&lt;/code>, initialize the application by calling &lt;code>Manager.InitModuleServices()&lt;/code> which initializes modules in the right order of dependencies.&lt;/li>
&lt;/ul>
&lt;h2 id="future-work">Future work&lt;/h2>
&lt;ul>
&lt;li>Extend the module manager to allow specifying multiple targets as opposed to a single target name supported currently.&lt;/li>
&lt;li>Factor out &lt;code>Run()&lt;/code> method to make it independent of Cortex. This will help reduce replicated code in the Loki project as well as help manage &lt;code>modules.Manager&lt;/code> outside of the Cortex struct.&lt;/li>
&lt;/ul></description></item><item><title>Docs: HTTP API Design</title><link>/docs/proposals/http-api-design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/http-api-design/</guid><description>
&lt;ul>
&lt;li>Author: @jtlisi&lt;/li>
&lt;li>Reviewers: @pracucci, @pstibrany, @khaines, @gouthamve&lt;/li>
&lt;li>Date: March 2020&lt;/li>
&lt;li>Status: Accepted&lt;/li>
&lt;/ul>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>The purpose of this design document is to propose a set of standards that should be the basis of the Cortex HTTP API. This document will outline the current state of the Cortex http api and describe limitations that result from the current approach. It will also outline a set of paradigms on how http routes should be created within Cortex.&lt;/p>
&lt;h2 id="current-design">Current Design&lt;/h2>
&lt;p>As things currently stand, the majority of HTTP API calls exist under the &lt;code>/api/prom&lt;/code> path prefix. This prefix is configurable. However, since this prefix is shared between all the modules which leads to conflicts if the Alertmanager is attempted to be run as as part of the single binary (#1722).&lt;/p>
&lt;h2 id="proposed-design">Proposed Design&lt;/h2>
&lt;h3 id="module-based-routing">Module-Based Routing&lt;/h3>
&lt;p>Cortex incorporates three separate APIs: Alertmanager, Prometheus, and Cortex. Each of these APIs should use a separate route prefix that accurately describes the API. Currently, all of the api calls in Cortex reside under the configured http prefix. Instead the following routing tree is proposed:&lt;/p>
&lt;h4 id="prometheus">&lt;code>/prometheus/*&lt;/code>&lt;/h4>
&lt;p>Under this path prefix, Cortex will act as a Prometheus web server. It will host all of the required Prometheus api endpoints. For example to query cortex the endpoint &lt;code>/prometheus/api/v1/query_range&lt;/code> will be used.&lt;/p>
&lt;h4 id="alertmanager">&lt;code>/alertmanager/*&lt;/code>&lt;/h4>
&lt;p>Under this path prefix, Cortex will act as a Alertmanager web server. In this case, it will forward requests to the alertmanager and support the alertmanager API. This means for a user to access their Alertmanager UI, they will use the &lt;code>/alertmanager&lt;/code> path of cortex.&lt;/p>
&lt;h4 id="apiv1----the-cortex-api-will-exist-under-this-path-prefix">&lt;code>/api/v1/*&lt;/code> &amp;ndash; The cortex API will exist under this path prefix.&lt;/h4>
&lt;ul>
&lt;li>&lt;code>/push&lt;/code>&lt;/li>
&lt;li>&lt;code>/chunks&lt;/code>&lt;/li>
&lt;li>&lt;code>/rules/*&lt;/code>&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Current&lt;/th>
&lt;th>Proposed&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>/api/prom/push&lt;/code>&lt;/td>
&lt;td>&lt;code>/api/v1/push&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/api/prom/chunks&lt;/code>&lt;/td>
&lt;td>&lt;code>/api/v1/chunks&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/api/prom/rules/*&lt;/code>&lt;/td>
&lt;td>&lt;code>/api/v1/rules/*&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="service-endpoints">Service Endpoints&lt;/h4>
&lt;p>A number of endpoints currently exist that are not under the &lt;code>/api/prom&lt;/code> prefix that provide basic web interfaces and trigger operations for cortex services. These endpoints will all be placed under a url with their service name as a prefix if it is applicable.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Current&lt;/th>
&lt;th>Proposed&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>/status&lt;/code>&lt;/td>
&lt;td>&lt;code>/multitenant-alertmanager/status&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/config&lt;/code>&lt;/td>
&lt;td>&lt;code>/config&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/ring&lt;/code>&lt;/td>
&lt;td>&lt;code>/ingester/ring&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/ruler_ring&lt;/code>&lt;/td>
&lt;td>&lt;code>/ruler/ring&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/compactor/ring&lt;/code>&lt;/td>
&lt;td>&lt;code>/compactor/ring&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/store-gateway/ring&lt;/code>&lt;/td>
&lt;td>&lt;code>/store-gateway/ring&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/ha-tracker&lt;/code>&lt;/td>
&lt;td>&lt;code>/distributor/ha_tracker&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/all_user_stats&lt;/code>&lt;/td>
&lt;td>&lt;code>/distributor/all_user_stats&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/user_stats&lt;/code>&lt;/td>
&lt;td>&lt;code>/distributor/user_stats&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/flush&lt;/code>&lt;/td>
&lt;td>&lt;code>/ingester/flush&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>/shutdown&lt;/code>&lt;/td>
&lt;td>&lt;code>/ingester/shutdown&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="path-versioning">Path Versioning&lt;/h3>
&lt;p>Cortex will utilize path based versioning similar to both Prometheus and Alertmanager. This will allow future versions of the API to be released with changes over time.&lt;/p>
&lt;h3 id="backwards-compatibility">Backwards-Compatibility&lt;/h3>
&lt;p>The new API endpoints and the current http prefix endpoints can be maintained concurrently. The flag to configure these endpoints will be maintained as &lt;code>http.prefix&lt;/code>. This will allow us to roll out the new API without disrupting the current routing schema. The original http prefix endpoints can maintained indefinitely or be phased out over time. Deprecation warnings can be added to the current API either when initialized or utilized. This can be accomplished by injecting a middleware that logs a warning whenever a legacy API endpoint is used.&lt;/p>
&lt;p>In cases where Cortex is run as a single binary, the Alertmanager module will only be accesible using the new API.&lt;/p>
&lt;h3 id="implementation">Implementation&lt;/h3>
&lt;p>This will be implemented by adding an API module to the Cortex service. This module will handle setting up all the required HTTP routes with Cortex. It will be designed around a set of interfaces required to fulfill the API. This is similar to how the &lt;code>v1&lt;/code> Prometheus API is implemented.&lt;/p>
&lt;h3 id="style">Style&lt;/h3>
&lt;ul>
&lt;li>All new paths will utilize &lt;code>_&lt;/code> instead of &lt;code>-&lt;/code> for their url to conform with Prometheus and its use of the underscore in the &lt;code>query_range&lt;/code> endpoint. This applies to all operations endpoints. Component names in the path can still contain dashes. For example: &lt;code>/store-gateway/ring&lt;/code>.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Migrating ingesters from chunks to blocks and back.</title><link>/docs/proposals/ingesters-migration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/ingesters-migration/</guid><description>
&lt;ul>
&lt;li>Author: @pstibrany&lt;/li>
&lt;li>Reviewers:&lt;/li>
&lt;li>Date: June 2020&lt;/li>
&lt;li>Status: Replaced with &lt;a href="https://github.com/cortexproject/cortex/blob/v1.11.1/docs/blocks-storage/migrate-from-chunks-to-blocks.md">migration guide (now removed from this site)&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="warning">Warning&lt;/h2>
&lt;p>Suggestions from this proposal were implemented, but general procedure outlined here doesn&amp;rsquo;t quite work in
Kubernetes environment. Please see &lt;a href="https://github.com/cortexproject/cortex/blob/v1.11.1/docs/blocks-storage/migrate-from-chunks-to-blocks.md">chunks to blocks migration guide (now removed from this site)&lt;/a>
instead.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>This short document describes the first step in full migration of the Cortex cluster from using chunks storage to using blocks storage, specifically switching ingesters to using blocks, and modification of queriers to query both chunks and blocks storage.&lt;/p>
&lt;h2 id="ingesters">Ingesters&lt;/h2>
&lt;p>When switching ingesters from chunks to blocks, we need to consider the following:&lt;/p>
&lt;ul>
&lt;li>Ingesting of new data, and querying should work during the switch.&lt;/li>
&lt;li>Ingesters are rolled out with new configuration over time. There is overlap: ingesters of both kinds (chunks, blocks) are running at the same time.&lt;/li>
&lt;li>Ingesters using WAL don’t flush in-memory chunks to storage on shutdown.&lt;/li>
&lt;li>Rollout should be as automated as possible.&lt;/li>
&lt;/ul>
&lt;p>How do we handle ingesters with WAL (non-WAL ingesters are discussed below)? There are several possibilities, but the simplest option seems to be adding a new flag to ingesters to flush chunks on shutdown. This is trivial change to ingester, and allows us to do automated migration by:&lt;/p>
&lt;ol>
&lt;li>Enabling this flag on each ingester (first rollout).&lt;/li>
&lt;li>Turn off chunks, enable TSDB (second rollout). During the second rollout, as the ingester shuts down, it will flush all chunks in memory, and when it restarts, it will start using TSDB.&lt;/li>
&lt;/ol>
&lt;p>Benefit of this approach is that it is trivial to add the flag, and then rollout in both steps can be fully automated.
In this scenario, we will reconfigure existing statefulset of ingesters to use blocks in step 2.&lt;/p>
&lt;p>Notice that querier can ask only ingesters for most recent data and not consult the store, but during the rollout (and some time after), ingesters that are already using blocks will &lt;strong>not&lt;/strong> have the most recent chunks in memory. To make sure queries work correctly, &lt;code>-querier.query-store-after&lt;/code> needs to be set to 0, in order for queriers to not rely on ingesters only for most recent data. After couple of hours after rollout, this value can be increased again, depending on how much data ingesters keep. (&lt;code>-experimental.blocks-storage.tsdb.retention-period&lt;/code> for blocks, &lt;code>-ingester.retain-period&lt;/code> for chunks)
During the rollout, chunks and blocks ingesters share the ring and use the same statefulset.&lt;/p>
&lt;p>Other alternatives considered for flushing chunks / handling WAL:&lt;/p>
&lt;ul>
&lt;li>Replay chunks-WAL into TSDB head on restart. In this scenario, chunks-ingester shuts down, and block ingester starts. It can detect existing chunks WAL, and replay it into TSDB head (and then delete old WAL). Issue here is that current chunks-WAL is quite specific to ingester code, and would require some refactoring to make this happen. Deployment is trivial: just reconfigure ingesters to start using blocks, and replay chunks WAL if found. Required change seems like a couple of days of coding work, but it is essentially only used once (for each cluster). Doesn&amp;rsquo;t seem like good time investment.&lt;/li>
&lt;li>Shutdown single chunks-ingester, run flusher in its place, and when done start new blocks ingester. This is similar to the procedure we did during the introduction of WAL. Flusher can be run via initContainer support in pods. This still requires two-step deployment: 1) enable flusher and reconfigure ingesters to use blocks, 2) remove flusher.&lt;/li>
&lt;/ul>
&lt;p>When not using WAL, ingesters using chunks cannot transfer those chunks to new ingesters that start with blocks support, so old ingesters need to be configured to disable transfers (using &lt;code>-ingester.max-transfer-retries=0&lt;/code>), and to flush chunks on shutdown instead.
As ingesters without WAL are typically deployed using Kubernetes deployment, while blocks ingesters need to use statefulset, and there is no chunks transfer happening, it is possible to configure and start blocks-ingesters and then stop old deployment.&lt;/p>
&lt;p>After all ingesters are converted to blocks, we can set cut-off time for querying chunks storage on queriers.&lt;/p>
&lt;p>For rollback from blocks to chunks, we need to be able to flush data from ingesters to the blocks storage, and then switch ingesters back to chunks.
Ingesters are currently not able to flush blocks to storage, but adding flush-on-shutdown option, support for &lt;code>/shutdown&lt;/code> endpoint and support in flusher component similar to chunks is doable, and should be part of this work.&lt;/p>
&lt;p>With this ability, rollback would follow the same process, just in reverse: 1) redeploy with flush flag enabled, 2a) redeploy with config change from blocks to chunks (when using WAL) or 2b) scale down statefulset with blocks-ingesters, and start deployment with chunk-ingesters again.
Note that this isn&amp;rsquo;t a &lt;em>full&lt;/em> rollback to chunks-only solution, as generated blocks still need to be queried after the rollback, otherwise samples pushed to blocks would be missing.
This means running store-gateways and queriers that can query both chunks and blocks store.&lt;/p>
&lt;p>Alternative plan could be to use a separate Cortex cluster configured to use blocks, and redirect incoming traffic to both chunks and blocks cluster.
When one is confident about the blocks cluster running correctly, old chunks cluster can be shutdown.
In this plan, there is an overlap where both clusters are ingesting same data.
Blocks cluster needs to be configured to be able to query chunks storage as well, with cut-off time based on when clusters were configured (at latest, to minimize amount of duplicated samples that need to be processed during queries.)&lt;/p>
&lt;h2 id="querying">Querying&lt;/h2>
&lt;p>To be able to query both old and new data, querier needs to be modified to be able to query both blocks (on object store only) and chunks store (NoSQL + object store) at the same time, and merge results from both.&lt;/p>
&lt;p>For querying chunks storage, we have two options:&lt;/p>
&lt;ul>
&lt;li>Always query the chunks store – useful during ingesters switch, or after rollback from blocks to chunks.&lt;/li>
&lt;li>Query chunk store only for queries that ask for data after specific cut-off time. This is useful after all ingesters have switched, and we know the timestamp since ingesters are only writing blocks.&lt;/li>
&lt;/ul>
&lt;p>Querier needs to support both modes of querying chunks store.
Which one of these two modes is used depends on single timestamp flag passed to the querier.
If timestamp is configured, chunks store is only used for queries that ask for data older than timestamp.
If timestamp is not configured, chunks store is always queried.&lt;/p>
&lt;p>For blocks, we don&amp;rsquo;t need to use the timestamp flag. Queriers can always query blocks – each querier knows about existing blocks and their timeranges, so it can quickly determine whether there are any blocks with relevant data.
Always querying blocks is also useful when there is some background process converting chunks to blocks.
As new blocks with old data appear on the store as a result of conversion, they get queried if necessary.&lt;/p>
&lt;p>While we could use runtime-config for on-the-fly switch without restarts, queriers restart quickly and so switching via configuration or command line option seems enough.&lt;/p>
&lt;h2 id="work-to-do">Work to do&lt;/h2>
&lt;ul>
&lt;li>Ingester: Add flags for always flushing on shutdown, even when using WAL or blocks.&lt;/li>
&lt;li>Querier: Add support for querying both chunk store and blocks at the same time and test the support for querying both chunks and blocks from ingesters works correctly&lt;/li>
&lt;li>Querier: Add cut-off time support to querier to query chunk the store only if needed, based on query time.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Parallel Compaction by Time Interval</title><link>/docs/proposals/parallel-compaction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/parallel-compaction/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/roystchiang">Roy Chiang&lt;/a>&lt;/li>
&lt;li>Date: May 2021&lt;/li>
&lt;li>Status: Proposed&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>As a part of pushing Cortex’s scaling capability at AWS, we have done performance testing with Cortex and found the compactor to be one of the main limiting factors for higher active timeseries limit per tenant. The documentation &lt;a href="https://cortexmetrics.io/docs/blocks-storage/compactor/#how-compaction-works">Compactor&lt;/a> describes the responsibilities of a compactor, and this proposal focuses on the limitations of the current compactor architecture. In the current architecture, compactor has simple sharding, meaning that a single tenant is sharded to a single compactor. The compactor generates compaction groups, which are groups of Prometheus TSDB blocks that can be compacted together, independently of another group. However, a compactor currnetly handles compaction groups of a single tenant iteratively, meaning that blocks belonging non-overlapping times are not compacted in parallel.&lt;/p>
&lt;p>Cortex ingesters are responsible for uploading TSDB blocks with data emitted by a tenant. These blocks are considered as level-1 blocks, as they contain duplicate timeseries for the same time interval, depending on the replication factor. &lt;a href="https://cortexmetrics.io/docs/blocks-storage/compactor/#how-compaction-works">Vertical compaction&lt;/a> is done to merge all the blocks with the same time interval and deduplicate the samples. These merged blocks are level-2 blocks. Subsequent compactions such as horizontal compaction can happen, further increasing the compaction level of the blocks.&lt;/p>
&lt;h3 id="problem-and-requirements">Problem and Requirements&lt;/h3>
&lt;p>Currently, a compactor is able to compact up to 20M timeseries within 2 hours for a level-2 compaction, including the time to download blocks, compact, and upload the newly compacted block. We would like to increase the timeseries limit per tenant, and compaction is one of the limiting factors. In addition, we would like to achieve the following:&lt;/p>
&lt;ul>
&lt;li>Compact multiple non-overlapping time intervals concurrently, so we can achieve higher throughput for the compaction of a single tenant&lt;/li>
&lt;li>We should be able to scale up, down compactor as needed, depending on how many compactions are pending&lt;/li>
&lt;li>Insight into the compaction progress of a tenant, such as the number of compactions required in order to catch up to the newest blocks&lt;/li>
&lt;/ul>
&lt;h2 id="design">Design&lt;/h2>
&lt;p>We accept the fact that a single compaction can potentially take more than 2 hours to compact, and we achieve higher compaction throughput through horizontally scaling the compactor. To compact more blocks in parallel for a single tenant, we distribute the compaction groups to compactors, instead of introducing more parallelism within a compactor.&lt;/p>
&lt;h3 id="parallelize-work">Parallelize Work&lt;/h3>
&lt;p>This proposal builds heavily on top of the &lt;a href="https://github.com/cortexproject/cortex/pull/2616">GrafanaLabs approach of introducing parallelism via time intervals&lt;/a>. The difference being that a single tenant is now sharded across multiple compactors instead of just a single compactor. The initial approach will be to work on distinct time intervals, but the compactor planner can be later extended to introduce parallelism within a time interval as well.&lt;/p>
&lt;p>The following is an example of parallelize work at each level:&lt;/p>
&lt;p>&lt;img src="/images/proposals/parallel-compaction-grouping.png" alt="Parallel Compaction Grouping">&lt;/p>
&lt;p>Compactors are shuffle-sharded, meaning that 1 tenant can belong to multiple compactors, and these subset of compactors determine which blocks should be compacted together. Compactors determine amongst themselves the responsibility of the compaction blocks, by using a hash of time interval and tenant id, and putting it on the sharding ring.&lt;/p>
&lt;p>The benefit of this approach is that this aligns with what Cortex currently does in Ruler. The downside is that a compaction job can only be assigned to a single compactor, rather than all of the compactors sharded for the tenant. If a compaction job takes forever, other tenants sharded to the same compactor will be blocked until the issue is resolved. With the scheduler approach, any compactor assigned to a given tenant can pick up any work required.&lt;/p>
&lt;p>&lt;img src="/images/proposals/parallel-compaction-without-scheduler.png" alt="Parallel Compaction Without Scheduler">&lt;/p>
&lt;h2 id="scenarios">Scenarios&lt;/h2>
&lt;h3 id="bad-block-resulting-in-non-ideal-compaction-groups">Bad block resulting in non-ideal compaction groups&lt;/h3>
&lt;p>A Cortex operator configures the compaction block range as 2h and 6h. If a full 6-hour block cannot be compacted due to compaction failures, the compactor should not split up the group into subgroups, as this may cause suboptimal grouping of block. Cortex has full information regarding all the available blocks, so we should utilize this information to achieve the best compaction group possible.&lt;/p>
&lt;h2 id="alternatives">Alternatives&lt;/h2>
&lt;h3 id="shard-compaction-jobs-amongst-compactors-with-a-scheduler">Shard compaction jobs amongst compactors with a scheduler&lt;/h3>
&lt;p>&lt;img src="/images/proposals/parallel-compaction-design.png" alt="Parallel Compaction Architecture">&lt;/p>
&lt;p>We add a new component Compactor Scheduler, which is responsible for calculating the compaction plan, and distributing compaction groups to compactors. The planner is sharded by tenant id, so that we can horizontally scale the planner as needed in order to accept more tenants in the cortex cluster. A tenant will have two queues inside the planner, a compaction queue and a clean up queue, similar to how the query frontend currently holds queues of pending queries.&lt;/p>
&lt;p>Once a compactor scheduler pushes a job to a compactor, the job is no longer available. Every set interval, or once the compaction is done, a compactor will update the compactor schedule the current status of the compaction job. If a compactor does not provide an update to the scheduler within a timeout, the compaction job becomes available to be assigned to other compactors.&lt;/p>
&lt;h4 id="concurrency">Concurrency&lt;/h4>
&lt;p>To achieve concurrency within a single tenant, compactor scheduler will push jobs to compactors. Compactors are shuffle-sharded by tenant id, to prevent a large tenant from impacting the compaction of other tenants. Compactor will download blocks from long term storage, compact, and upload. Compactor will also pull from the clean up queues from scheduler, and delete blocks marked for deletion.&lt;/p>
&lt;h4 id="consistency">Consistency&lt;/h4>
&lt;p>On resharding of compactor schedulers, a tenant might move to a different scheduler. We can either drop the current compactor job in order to prevent duplicate compaction jobs, or continue compaction. I propose that the compactor drops the compaction job if the compaction group no longer belongs to the original compactor scheduler. This way, we do not have duplicate compactions happening, and we can minimize work wasted.&lt;/p>
&lt;h3 id="contribute-to-thanos-for-a-more-scalable-compactor">Contribute to Thanos for a more scalable compactor&lt;/h3>
&lt;p>Instead of introducing parallelism on the Cortex compactor level, we move the parallelism to the Thanos compactor itself. Thanos has a &lt;a href="https://docs.google.com/document/d/1xi0V8DB0hE54XgkogJRnNL6yH7C5JThJywlLFoC6dCQ/">proposal to make compactor more scalable&lt;/a>, and a &lt;a href="https://github.com/thanos-io/thanos/pull/3807">PR&lt;/a>. Cortex will enjoy higher throughput per tenant if Thanos is able to speed up the compaction, and we can keep the Cortex architecture the same. However, this approach means that a single tenant is still sharded to a single compactor. In order to compact more groups at once, we must scale up compactor vertically. Although vertical scaling can get us far, we should scale horizontally where we can.&lt;/p></description></item><item><title>Docs: Retention of Tenant Data from Blocks Storage</title><link>/docs/proposals/tenant-retention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/tenant-retention/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/Allenzhli">Allenzhli&lt;/a>&lt;/li>
&lt;li>Date: January 2021&lt;/li>
&lt;li>Status: Accepted, Implemented in &lt;a href="https://github.com/cortexproject/cortex/pull/3879">PR #3879&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="retention-of-tenant-data">Retention of tenant data&lt;/h2>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Metric data is growing over time per-tenant, at the same time, the value of data decreases. We want to have a retention policy like prometheus does. In Cortex, data retention is typically achieved via a bucket policy. However, this has two main issues:&lt;/p>
&lt;ol>
&lt;li>Not every backend storage support bucket policies&lt;/li>
&lt;li>Bucket policies don&amp;rsquo;t easily allow a per-tenant custom retention&lt;/li>
&lt;/ol>
&lt;h2 id="background">Background&lt;/h2>
&lt;h3 id="tenants">tenants&lt;/h3>
&lt;p>When using blocks storage, Cortex stores tenant’s data in object store for long-term storage of blocks, tenant id as part of the object store path. We discover all tenants via scan the root dir of bucket.&lt;/p>
&lt;h3 id="runtime-config">runtime config&lt;/h3>
&lt;p>Using the &amp;ldquo;overrides&amp;rdquo; mechanism (part of runtime config) already allows for per-tenant settings. See &lt;a href="https://cortexmetrics.io/docs/configuration/arguments/#runtime-configuration-file">runtime-configuration-file&lt;/a> for more details. Using it for tenant retention would fit nicely. Admin could set per-tenant retention here, and also have a single global value for tenants that don&amp;rsquo;t have custom value set.&lt;/p>
&lt;h2 id="proposal">Proposal&lt;/h2>
&lt;h3 id="retention-period-field">retention period field&lt;/h3>
&lt;p>We propose to introduce just one new field &lt;code>RetentionPeriod&lt;/code> in the Limits struct(defined at pkg/util/validation/limits.go).&lt;/p>
&lt;p>&lt;code>RetentionPeriod&lt;/code> setting how long historical metric data retention period per-tenant. &lt;code>0&lt;/code> is disable.&lt;/p>
&lt;p>Runtime config is reloaded periodically (defaults to 10 seconds), so we can update the retention settings on-the-fly.&lt;/p>
&lt;p>For each tenant, if a tenant-specific &lt;em>runtime_config&lt;/em> value exists, it will be used directly, otherwise, if a default &lt;em>limits_config&lt;/em> value exists, then the default value will be used; If neither exists, do nothing.&lt;/p>
&lt;h3 id="implementation">Implementation&lt;/h3>
&lt;p>A BlocksCleaner within the Compactor run periodically (which defaults to 15 minutes) and the retention logic will insert into it. The logic should compare retention value to block &lt;code>maxTime&lt;/code> and blocks that match &lt;code>maxTime &amp;lt; now - retention&lt;/code> will be marked for delete.&lt;/p>
&lt;p>Blocks deletion is not immediate, but follows a two steps process. See &lt;a href="https://cortexmetrics.io/docs/blocks-storage/compactor/#soft-and-hard-blocks-deletion">soft-and-hard-blocks-deletion&lt;/a>&lt;/p></description></item><item><title>Docs: Ring Multikey</title><link>/docs/proposals/ring-multikey/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/ring-multikey/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/danielblando">Daniel Blando&lt;/a>&lt;/li>
&lt;li>Date: August 2022&lt;/li>
&lt;li>Status: Proposed&lt;/li>
&lt;/ul>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Cortex implements a ring structure to share information of registered pods for each
service. The data stored and used by the ring need to be implemented via Codec interface. Currently, the only supported
Codec to the ring is &lt;a href="https://github.com/cortexproject/cortex/blob/c815b3cb61e4d0a3f01e9947d44fa111bc85aa08/pkg/ring/ring.proto#L10">Desc&lt;/a>.
Desc is a proto.Message with a list of instances descriptions. It is used to store the data for each pod and
saved on a supported KV store. Currently, Cortex supports memberlist, consul and etcd as KV stores. Memberlist works
implementing a gossip protocol while consul and etcd are a KV store service.&lt;/p>
&lt;p>The ring is used by different services using a different ring key to store and receive the values from the KV store.
For example, ingester service uses the key &amp;ldquo;ingester&amp;rdquo; to save and load data from KV. As the saved data is a Desc
struct only one key is used for all the information.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Each service using a single key to save and load information creates a concurrency issue when multiple pods are saving
the same key. When using memberlist, the issue is mitigate as the information is owned by all pods and timestamp is used
to confirm latest data. For consul and etcd, all pods compete to update the key at the same time causing an increase on
latency and failures direct related to number of pods running. Cortex and etcd implementation use a version tag to
make sure no data is being overridden causing the problem of write failures.&lt;/p>
&lt;p>On a test running cortex with etcd, distributor was scaled to 300 pods and latency increase was noticed coming from etcd usage.
We can also notice 5xx happening when etcd was running.
&lt;img src="/images/proposals/ring-multikey-latency.png" alt="Latency using etcd">&lt;/p>
&lt;p>17:14 - Running memberlist, p99 around 5ms
17:25 - Running etcd, p99 around 200ms
17:25 to 17:34 migrating to multikey
After running etcd multikey poc, p99 around 25ms&lt;/p>
&lt;h2 id="proposal">Proposal&lt;/h2>
&lt;h3 id="multikey-interface">Multikey interface&lt;/h3>
&lt;p>The proposal is separate the current Desc struct which contains a list of key value in multiple keys. Instead of saving one
&amp;ldquo;ingester&amp;rdquo; key, the KV store will have &amp;ldquo;ingester-1&amp;rdquo;, &amp;ldquo;ingester-2&amp;rdquo; keys saved.&lt;/p>
&lt;p>Current:&lt;/p>
&lt;pre tabindex="0">&lt;code>Key: ingester/ring/ingester
Value:
{
&amp;#34;ingesters&amp;#34;: {
&amp;#34;ingester-0&amp;#34;: {
&amp;#34;addr&amp;#34;: &amp;#34;10.0.0.1:9095&amp;#34;,
&amp;#34;timestamp&amp;#34;: 1660760278,
&amp;#34;tokens&amp;#34;: [
1,
2
],
&amp;#34;zone&amp;#34;: &amp;#34;us-west-2b&amp;#34;,
&amp;#34;registered_timestamp&amp;#34;: 1660708390
},
&amp;#34;ingester-1&amp;#34;: ...
}
}
&lt;/code>&lt;/pre>&lt;p>Proposal:&lt;/p>
&lt;pre tabindex="0">&lt;code>Key: ingester/ring/ingester-0
Value:
{
&amp;#34;addr&amp;#34;: &amp;#34;10.0.0.1:9095&amp;#34;,
&amp;#34;timestamp&amp;#34;: 1660760278,
&amp;#34;tokens&amp;#34;: [
1,
15
],
&amp;#34;zone&amp;#34;: &amp;#34;us-west-2b&amp;#34;,
&amp;#34;registered_timestamp&amp;#34;: 1660708390
}
Key: ingester/ring/ingester-1
Value:
{
&amp;#34;addr&amp;#34;: &amp;#34;10.0.0.2:9095&amp;#34;,
&amp;#34;timestamp&amp;#34;: 1660760378,
&amp;#34;tokens&amp;#34;: [
5,
28
],
&amp;#34;zone&amp;#34;: &amp;#34;us-west-2b&amp;#34;,
&amp;#34;registered_timestamp&amp;#34;: 1660708572
}
&lt;/code>&lt;/pre>&lt;p>The proposal is to create an interface called MultiKey. The interface allows KV store to request the codec to split and
join the values is separated keys.&lt;/p>
&lt;pre tabindex="0">&lt;code>type MultiKey interface {
SplitById() map[string]interface{}
JoinIds(map[string]interface{}) Multikey
GetChildFactory() proto.Message
FindDifference(MultiKey) (Multikey, []string, error)
}
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>SplitById - responsible to split the codec in multiple keys and interface.&lt;/li>
&lt;li>JoinIds - responsible to receive multiple keys and interface creating the codec objec&lt;/li>
&lt;li>GetChildFactory - Allow the kv store to know how to serialize and deserialize the interface returned by “SplitById”.
The interface returned by SplitById need to be a proto.Message&lt;/li>
&lt;li>FindDifference - optimization used to know what need to be updated or deleted from a codec. This avoids updating all keys every
time the coded change. First parameter returns a subset of the Multikey to be updated. Second is a list of keys to
be deleted.&lt;/li>
&lt;/ul>
&lt;p>The codec implementation will change to support multiple keys. Currently, the codec interface for KV store supports
only Encode and Decode. New methods will be added which would be used only by the KV stores implementing the multi
key functionality.&lt;/p>
&lt;pre tabindex="0">&lt;code>type Codec interface {
//Existen
Decode([]byte) (interface{}, error)
Encode(interface{}) ([]byte, error)
CodecID() string
//Proposed
DecodeMultiKey(map[string][]byte) (interface{}, error)
EncodeMultiKey(interface{}) (map[string][]byte, error)
}
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>DecodeMultiKey - called by KV store to decode data downloaded. This function will use the JoinIds method.&lt;/li>
&lt;li>EncodeMultiKey - called by KV store to encode data to be saved. This function will use the SplitById method.&lt;/li>
&lt;/ul>
&lt;p>The new KV store will know the data being saved is a reference for multikey. It will use the FindDifference
to know which keys need to be updated. The codec implementation for the new methods will use the JoinIds and SplitById
to know how to separate the codec in multiple keys. The DecodeMultiKey will also use GetChildFactory to know how to
decode the data stored in the kv store.&lt;/p>
&lt;p>Example of CAS being used with multikey design:
&lt;img src="/images/proposals/ring-multikey-sequence.png" alt="Sequence diagram">&lt;/p></description></item><item><title>Docs: Ruler HA</title><link>/docs/proposals/ruler-ha/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/ruler-ha/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/soonping-amzn">Soon-Ping Phang&lt;/a>&lt;/li>
&lt;li>Date: June 2022&lt;/li>
&lt;li>Status: Proposed&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>This proposal consolidates multiple existing PRs from the AWS team working on this feature, as well as future work needed to complete support. The hope is that a more holistic view will make for more productive discussion and review of the individual changes, as well as provide better tracking of overall progress.&lt;/p>
&lt;p>The original issue is &lt;a href="https://github.com/cortexproject/cortex/issues/4435">#4435&lt;/a>.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Rulers in Cortex currently run with a replication factor of 1, wherein each RuleGroup is assigned to exactly 1 ruler. This lack of redundancy creates the following risks:&lt;/p>
&lt;ul>
&lt;li>Rule group evaluation
&lt;ul>
&lt;li>Missed evaluations due to a ruler outage, possibly caused by a deployment, noisy neighbour, hardware failure, etc.&lt;/li>
&lt;li>Missed evaluations due to a ruler brownout due to other tenant rule groups sharing the same ruler (noisy neighbour)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>API
-inconsistent API results during resharding (e.g. due to a deployment) when rulers are in a transition state loading rule groups&lt;/li>
&lt;/ul>
&lt;p>This proposal attempts to mitigate the above risks by enabling a ruler replication factor of greater than 1, allowing multiple rulers to evaluate the same rule group — effectively, the ruler equivalent of ingester HA already supported in Cortex.&lt;/p>
&lt;h2 id="proposal">Proposal&lt;/h2>
&lt;h3 id="make-replicationfactor-configurable">Make ReplicationFactor configurable&lt;/h3>
&lt;p>ReplicationFactor in Ruler is currently hardcoded to 1. Making this a configurable parameter is the first step to enabling HA in ruler, and would also be the mechanism for the user to turn the feature on. The parameter value will be 1 by default, equating to the feature being turned off by default.&lt;/p>
&lt;p>A replication factor greater than 1 will result in the same rules being evaluated by multiple ruler instances.&lt;/p>
&lt;p>This redundancy will allow for missed or skipped rule evaluations from single ruler outages to be covered by other instances evaluating the same rules.&lt;/p>
&lt;p>There is also the question of duplicate metrics generated by replicated evaluation. We do not expect this to be a problem, as all replicas must use the same &lt;a href="https://github.com/prometheus/prometheus/blob/b878527151e6503d24ac5b667b86e8794eb79ff7/rules/manager.go#L509">slotted intervals&lt;/a> when evaluating rules, which should result in the same timestamp applying to metrics generated by each replica, with the samples being deduplicated in the ingestion path.&lt;/p>
&lt;p>&lt;a href="https://github.com/cortexproject/cortex/pull/4712">PR #4712&lt;/a> [open]&lt;/p>
&lt;h3 id="adjustments-for-alerts_for_state">Adjustments for ALERTS_FOR_STATE&lt;/h3>
&lt;p>When an alert fires in a rulegroup shared across multiple rulers, the alert on each ruler will have slightly different timestamps due to the differences in evaluation time on each ruler. This results in the ALERTS_FOR_STATE metric being deduplicated with the &amp;ldquo;duplicate sample for timestamp&amp;rdquo; error. To prevent this, we synchronize Alert.activeAt for the same alerts in different rulers using a post-processing hook.&lt;/p>
&lt;p>&lt;img src="/images/proposals/ruler-ha-alerts-for-state-graph.png" alt="ALERTS_FOR_STATE interference between replicas">&lt;/p>
&lt;p>PRs:&lt;/p>
&lt;ul>
&lt;li>Prometheus PR &lt;a href="https://github.com/prometheus/prometheus/pull/9665">#9665&lt;/a> [merged]&lt;/li>
&lt;li>Prometheus PR &lt;a href="https://github.com/prometheus/prometheus/pull/10070">#10070&lt;/a> [merged]&lt;/li>
&lt;li>Cortex PR &lt;a href="https://github.com/cortexproject/cortex/pull/4712">#4712&lt;/a> [open]&lt;/li>
&lt;/ul>
&lt;h3 id="weak-and-strong-quorum-in-listrules-and-listalerts">Weak and Strong Quorum in ListRules and ListAlerts&lt;/h3>
&lt;p>ListRules/ListAlerts will return inconsistent responses while a new configuration propagates to multiple ruler HA instances. For most users, this is an acceptable side-effect of an eventually consistent HA architecture. However, some use-cases have stronger consistency requirements and are willing to sacrifice some availability for those use-cases. For example, an alerts client might want to verify that a change has propagated to a majority of instances before informing the user that the update succeeded. To enable this, we propose adding an optional quorum API parameter with the following behaviour:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>quorum=weak (default)&lt;/p>
&lt;ul>
&lt;li>Biased towards availability, ListRules will perform a best-effort merge of the results from at least $i = tenantShardSize - replicationFactor + 1$ instances. If a rulegroup definition does not satisfy a quorum of $q = \lfloor{replicationFactor / 2}\rfloor + 1$ copies, it will choose the most recently evaluated version of that rulegroup for the final result set. An error response will be returned only if all instances return an error. Note that a side-effect of this rule is that the API will return a result even if all but one ruler instances in the tenant&amp;rsquo;s shard is unavailable.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>quorum=strong&lt;/p>
&lt;ul>
&lt;li>Biased towards consistency, ListRules will query at least $i = tenantShardSize - \lfloor{replicationFactor / 2}\rfloor + 1$ instances. If any rulegroup does not satisfy a quorum of $q = \lfloor{replicationFactor / 2}\rfloor + 1$ copies, a 503 error will be returned.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/cortexproject/cortex/pull/4768">PR #4768&lt;/a> [open]&lt;/p>
&lt;h4 id="alternatives-to-a-quorum-api-parameter">Alternatives to a quorum API parameter&lt;/h4>
&lt;h5 id="weak-or-strong-quorum-by-default">Weak or strong quorum by default&lt;/h5>
&lt;p>Another option for making ListRules work in HA mode is to implement one of the quorum rules (weak or strong) as the default, with no control to select between the two, outside of maybe a configuration parameter. AWS itself runs multitenant Cortex instances, and we have an internal use-case for the strong quorum implementation, but we do not want to impose the subsequent availability hit on our customers, particularly given that replication_factor is not currently a tenant-specific parameter in Cortex, for ingesters, alert manager, or ruler.&lt;/p>
&lt;p>Making HA availability the default, while giving users the choice to knowingly request for more consistency at the cost of more error-handling seems like a good balance.&lt;/p></description></item><item><title>Docs: Ruler Tenant Federation</title><link>/docs/proposals/ruler-tenant-federation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/ruler-tenant-federation/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/rdooley">Rees Dooley&lt;/a>&lt;/li>
&lt;li>Date: November 2021&lt;/li>
&lt;li>Status: Accepted&lt;/li>
&lt;/ul>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>This document aims to describe how to implement the ability to allow rules to cover data from more than a single Cortex tenant, here after referred to as federated rules. Since currently rules are owned by, query data from and save resulting series in the same tenant, this document aims to provide clear delineation of who owns a federated rule, what tenants the federated rule queries data from and where the federated rule saves resulting series.&lt;/p>
&lt;p>A federated rule is any rule which contains the &lt;code>src_tenants&lt;/code> field.&lt;/p>
&lt;h2 id="reasoning">Reasoning&lt;/h2>
&lt;p>The primary use case for allowing federated rules which query data from multiple tenants is the administration of cortex.&lt;/p>
&lt;p>In the case of the administration of cortex, when running Cortex within a large organization, there may be metrics spanning across tenants which might be desired to be monitored e.g. administrative metrics of the cortex system like &lt;code>prometheus_rule_evaluation_failures_total&lt;/code> aggregated by &lt;code>__tenant_id__&lt;/code>. In this case, a team e.g. &lt;code>infra&lt;/code> may wish to be able to create a rule, owned by &lt;code>infra&lt;/code> which queries multiple tenants &lt;code>t0|t1|...|ti&lt;/code> and stores resulting series in &lt;code>infra&lt;/code>.&lt;/p>
&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;h3 id="allow-federated-rules-behind-feature-flag">Allow federated rules behind feature flag&lt;/h3>
&lt;h4 id="challenge">Challenge&lt;/h4>
&lt;p>Federated tenant rules and alerts will not be a good fit for organization and should be behind a feature flag.&lt;/p>
&lt;h4 id="proposal">Proposal&lt;/h4>
&lt;p>For federated rules, creation of federated rules (those sourcing data from multiple tenants) should be blocked behind the feature flag &lt;code>ruler.enable-federated-rules&lt;/code>&lt;/p>
&lt;p>If tenant federation is enabled, then ruler should use a &lt;code>mergeQueryable&lt;/code> to aggregate the results of querying multiple tenants.&lt;/p>
&lt;h3 id="allow-federated-rules-only-for-select-tenants">Allow federated rules only for select tenants&lt;/h3>
&lt;h4 id="challenge-1">Challenge&lt;/h4>
&lt;p>For many organizations, the ability for any tenant to write a rule querying any other tenant is not acceptable and more fine grained control is required&lt;/p>
&lt;h4 id="proposal-1">Proposal&lt;/h4>
&lt;p>Since the current default is that a tenant should only be able to write rules against itself, we suggest a config option &lt;code>ruler.allowed-federated-tenants&lt;/code>, a string slice of OrgIDs like &lt;code>infra&lt;/code> or &lt;code>0|1|2|3|4&lt;/code> which are allowed to write rules against all tenants. If a tenant &lt;code>bar&lt;/code> attempts to create a federated rule, an error should be returned by the ruler api. Similarly an option &lt;code>ruler.disallowed-federated-tenants&lt;/code> explicitly states a list of tenants for which federated rules are not allowed. Combining these in a &lt;code>util.AllowedTenants&lt;/code> should allow one to quickly determine if federation is enabled or disabled for a given tenant at rule creation.&lt;/p>
&lt;h3 id="where-to-store-resulting-series-of-federated-rule">Where to store resulting series of federated rule&lt;/h3>
&lt;h4 id="challenge-2">Challenge&lt;/h4>
&lt;p>A single tenant rule always stores produced series in the tenant where the rule exists. This 1 -&amp;gt; 1 mapping becomes a many -&amp;gt; 1 mapping for federated rules.&lt;/p>
&lt;h4 id="proposal-2">Proposal&lt;/h4>
&lt;p>Tenants owning a federated rule the resulting series is saved in the tenant which owns the rule.&lt;/p>
&lt;h3 id="which-tenants-to-query-from-for-federated-rules">Which tenants to query from for federated rules&lt;/h3>
&lt;h4 id="challenge-3">Challenge&lt;/h4>
&lt;p>A single tenant rule always queries the tenant which owns the rule. This 1 -&amp;gt; 1 mapping becomes a 1 -&amp;gt; many mapping for federated rules.&lt;/p>
&lt;h4 id="proposal-3">Proposal&lt;/h4>
&lt;p>As some use cases will demand that a specific federated rule, querying tenant B and C, is stored in the owning teams tenant A, an option to allow explicit assignment of source tenants for a federated rule is needed.&lt;/p>
&lt;p>To support this we suggest an additional field &lt;code>src_tenants&lt;/code> on the rule group containing an array of OrgIDs e.g. &lt;code>[t0,t1,...,ti]&lt;/code> which when present determines which tenants to query for the given rule. Rule group is chosen as it reduces repetition between rules.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Challenge&lt;/th>
&lt;th>Status&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Allow federated rules behind feature flag&lt;/td>
&lt;td>Planned but not yet implemented&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allow federated rules only for select tenants&lt;/td>
&lt;td>Planned but not yet implemented&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Where to store resulting series of federated rules&lt;/td>
&lt;td>Planned but not yet implemented&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Which tenants to query from for federated rules&lt;/td>
&lt;td>Planned but not yet implemented&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Docs: Scalable Alertmanager</title><link>/docs/proposals/scalable-alertmanager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/scalable-alertmanager/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/gotjosh">Josh Abreu&lt;/a>&lt;/li>
&lt;li>Date: December 2020&lt;/li>
&lt;li>Status: Accepted&lt;/li>
&lt;/ul>
&lt;h2 id="context-and-background">Context and Background&lt;/h2>
&lt;p>The Cortex Alertmanager at its current state supports high-availability by using the same technique as the upstream Prometheus Alertmanager: we gossip silences and notifications between replicas to achieve eventual consistency. This allows us to tolerate machine failure without interruption of service. The caveat here is traffic between the ruler and Alertmanagers must not be load balanced as alerts themselves are not gossiped.&lt;/p>
&lt;p>By itself it is not horizontally scalable; it is recommended to run a maximum of 3 replicas (for high availability). Each alertmanager replica will contain an internal instance for every tenant. The Alertmanager uses roughly ~3.7MB of memory and ~0.001 CPU cores per tenant, with an average tenant having ~10 active alerts and ~1 silences. These stats were measured from a cluster with ~80 QPS (Alerts received Per Second), ~40 NPS (Notifications Per Second), and ~700 configurations over a single replica.&lt;/p>
&lt;h2 id="problem-and-requirements">Problem and Requirements&lt;/h2>
&lt;p>Current numbers show a reasonably sized machine can handle 2000 tenants in the current service. We would like to be able to scale this up to 10x without increasing the machine size; i.e. we would like to make Cortex Alertmanager service horizontally scalable.&lt;/p>
&lt;p>Furthermore, we desire to preserve the following characteristics:&lt;/p>
&lt;ul>
&lt;li>A single Cortex Alertmanager replica crashing or exiting abruptly should not cause externally-visible downtime or failure to deliver notifications.&lt;/li>
&lt;li>Users should have an eventually consistent view of all the alerts current firing and under active notification, favouring availability of the Cortex Alertmanager over consistency.&lt;/li>
&lt;li>We should be able to scale up, down and roll out new versions of the service without any service interruption or data loss.&lt;/li>
&lt;/ul>
&lt;h2 id="design">Design&lt;/h2>
&lt;p>This is a big design, and as such we have divided the problem up into 4 areas: Routing &amp;amp; Sharding, Persistence &amp;amp; State, Replication &amp;amp; Consistency, and Architecture. The options and solutions I propose are divided along these lines to aid clarity.&lt;/p>
&lt;h3 id="routing--sharding">Routing &amp;amp; Sharding&lt;/h3>
&lt;p>To achieve horizontal scalability, we need to distribute the workload among replicas of the service. We need to choose an appropriate field to use to distribute the workload. The field must be present on all the API requests to the Alertmanager service.&lt;/p>
&lt;p>&lt;strong>We propose the sharding on Tenant ID&lt;/strong>. The simplicity of this implementation, would allow us to get up and running relatively quickly whilst helping us validate assumptions. We intend to use the existing ring code to manage this. Other options such as tenant ID + receiver or Tenant ID + route are relatively complex as distributor components (in this case the Ruler) would need to be aware of Alertmanager configuration.&lt;/p>
&lt;h3 id="persistence--state">Persistence &amp;amp; State&lt;/h3>
&lt;p>Alertmanager is a stateful service; it stores the notification state and configured silences. By default, Alertmanager persists its state to disk every 15mins. In the horizontally scalable Alertmanager service, we need to move this state around as the number of replicas grows and shrinks. We also need to persist this state across restarts and rolling upgrades.&lt;/p>
&lt;p>&lt;strong>We propose&lt;/strong> making each Alertmanager replica flush the state to object storage, under its own key that’s a combination of tenant ID + replica periodically. This state on durable storage will only be used when cold-starting the cluster.&lt;/p>
&lt;p>This mechanism covers multiple challenges (scaling up &amp;amp; down, rolling restarts, total cluster failure). &lt;strong>To top this off, in the implementation, we’ll always try to request state from other replicas in the cluster before ever trying to go to object storage.&lt;/strong>&lt;/p>
&lt;h3 id="replication--consistency">Replication &amp;amp; Consistency&lt;/h3>
&lt;p>Upstream Alertmanager replicates notification state between replicas to ensure notifications are not sent more than once. Cortex Alertmanager does the same. When we move to a model where AM instances for a given tenant only live on a subset of replicas, we have to decide how we will keep these replicas in sync.&lt;/p>
&lt;p>&lt;strong>We have an option of doing nothing&lt;/strong> and use the existing gossip mechanism and gossip all tenants state to all replicas. The AM Router will then drop state updates for tenants which don’t shard to a given replica. I think this will be easy to implement as it requires few changes but probably won’t scale.&lt;/p>
&lt;p>&lt;strong>However, I propose we Synchronized state over gRPC.&lt;/strong> The upstream Alertmanager notification dispatcher uses a timeout-based approach for its notifications. Using the “peer order”, it’ll wait a certain amount of time before letting other replicas know if this notification succeeded or not. If it didn’t, the next replica in line will try to send the notification. We propose to communicate this via gRPC calls.&lt;/p>
&lt;p>While I propose the use of gRPC, is good to note that doing nothing is still a solid candidate for consideration but not having to maintain and operate two separate replication patterns feels like a win.&lt;/p>
&lt;h3 id="architecture">Architecture&lt;/h3>
&lt;p>To implement the sharding strategy we need a component in charge of handling incoming alerts and API requests, distributing it to the corresponding shard.&lt;/p>
&lt;p>The idea here is to have an “Alertmanager Distributor” as a first stop in the reception of alerts. Once alerts are received, the component is in charge of validating the alerts against the limits. Validated alerts are then sent to multiple managers in parallel.&lt;/p>
&lt;p>The individual pieces of this component (sharding, limits) cannot be optional - the optional part of it is where we decide to run it.&lt;/p>
&lt;p>We can either run it as a separate service or embed it. &lt;strong>I propose we simply embed it&lt;/strong>. At its core it’ll be simpler to operate. With future work making it possible to run as a separate service so that operators can scale when/if needed.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Under the assumption we implement the options proposed above, our architecture looks like this:&lt;/p>
&lt;p>&lt;img src="/images/proposals/scalable-am.png" alt="Scalable Alertmanager Architecture">&lt;/p>
&lt;p>&lt;strong>POST /api/v1/alerts (from the ruler) can go to any Alertmanager replica.&lt;/strong> The AM distributor uses the ring to write alerts to a quorum of AM managers (reusing the existing code). We continue to use the same in-memory data structure from the upstream Alertmanager to save alerts and notify other pieces&lt;/p>
&lt;p>&lt;strong>GET /api/v1/alerts and /api/v1/alerts/group&lt;/strong> uses the ring to find the right alertmanager replicas for the given tenant. They read from a quorum of alertmanager replicas and return an union of the results.&lt;/p>
&lt;p>&lt;strong>Alertmanager state is replicated between instances to keep them in sync&lt;/strong>. Where the state is replicated to to is controlled by the ring.&lt;/p></description></item><item><title>Docs: Scalable Query Frontend</title><link>/docs/proposals/scalable-query-frontend/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/scalable-query-frontend/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/joe-elliott">Joe Elliott&lt;/a>&lt;/li>
&lt;li>Date: April 2020&lt;/li>
&lt;li>Status: Proposed&lt;/li>
&lt;/ul>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>This document aims to describe the &lt;a href="#query-frontend-role">role&lt;/a> that the Cortex Query Frontend plays in running multitenant Cortex at scale. It also describes the &lt;a href="#challenges-and-proposals">challenges&lt;/a> of horizontally scaling the query frontend component and includes several recommendations and options for creating a reliably scalable query-frontend. Finally, we conclude with a discussion of the overall philosophy of the changes and propose an &lt;a href="#alternative">alternative&lt;/a>.&lt;/p>
&lt;p>For the original design behind the query frontend, you should read &lt;a href="https://docs.google.com/document/d/1lsvSkv0tiAMPQv-V8vI2LZ8f4i9JuTRsuPI_i-XcAqY">Cortex Query Optimisations design doc from 2018-07&lt;/a>.&lt;/p>
&lt;h2 id="reasoning">Reasoning&lt;/h2>
&lt;p>Query frontend scaling is becoming increasingly important for two primary reasons.&lt;/p>
&lt;p>The Cortex team is working toward a scalable single binary solution. Recently the query-frontend was &lt;a href="https://github.com/cortexproject/cortex/pull/2437">added&lt;/a> to the Cortex single binary mode and, therefore, needs to seamlessly scale. Technically, nothing immediately breaks when scaling the query-frontend, but there are a number of concerns detailed in &lt;a href="#challenges-and-proposals">Challenges And Proposals&lt;/a>.&lt;/p>
&lt;p>As the query-frontend continues to &lt;a href="https://github.com/cortexproject/cortex/pull/1878">support additional features&lt;/a> it will start to become a bottleneck of the system. Current wisdom is to run very few query-frontends in order to maximize &lt;a href="#tenancy-fairness">Tenancy Fairness&lt;/a> but as more features are added scaling horizontally will become necessary.&lt;/p>
&lt;h2 id="query-frontend-role">Query Frontend Role&lt;/h2>
&lt;h3 id="load-shedding">Load Shedding&lt;/h3>
&lt;p>The query frontend maintains a queue per tenant of configurable length (default 100) in which it stores a series of requests from that tenant. If this queue fills up then the frontend will return 429’s thus load shedding the rest of the system.&lt;/p>
&lt;p>This is particularly effective due to the “pull” based model in which queriers pull requests from query frontends.&lt;/p>
&lt;h3 id="query-retries">Query Retries&lt;/h3>
&lt;p>The query frontend is capable of retrying a query on another querier if the first should fail due to OOM or network issues.&lt;/p>
&lt;h3 id="shardingparallelization">Sharding/Parallelization&lt;/h3>
&lt;p>The query frontend shards requests by interval and &lt;a href="https://github.com/cortexproject/cortex/pull/1878">other factors&lt;/a> to concurrently run a single query across multiple queriers.&lt;/p>
&lt;h3 id="query-alignmentcaching">Query Alignment/Caching&lt;/h3>
&lt;p>Queries are aligned to their own step and then stored/retrieved from cache.&lt;/p>
&lt;h3 id="tenancy-fairness">Tenancy Fairness&lt;/h3>
&lt;p>By maintaining one queue per tenant, a low demand tenant will have the same opportunity to have a query serviced as a high demand tenant. See &lt;a href="#dilutes-tenant-fairness">Dilutes Tenant Fairness&lt;/a> for additional discussion.&lt;/p>
&lt;p>For clarity, tenancy fairness only comes into play when queries are actually being queued in the query frontend. Currently this rarely occurs, but as &lt;a href="https://github.com/cortexproject/cortex/pull/1878">query sharding&lt;/a> becomes more aggressive this may become the norm.&lt;/p>
&lt;h2 id="challenges-and-proposals">Challenges And Proposals&lt;/h2>
&lt;h3 id="dynamic-querier-concurrency">Dynamic Querier Concurrency&lt;/h3>
&lt;h4 id="challenge">Challenge&lt;/h4>
&lt;p>For every query frontend the querier adds a &lt;a href="https://github.com/cortexproject/cortex/blob/50f53dba8f8bd5f62c0e85cc5d85684234cd1c1c/pkg/querier/frontend/worker.go#L146">configurable number of goroutines&lt;/a> which are each capable of executing a query. Therefore, scaling the query frontend impacts the amount of work each individual querier is attempting to do at any given time.&lt;/p>
&lt;p>Scaling up may cause a querier to attempt more work than they are configured for due to restrictions such as memory and cpu limits. Additionally, the promql engine itself is limited in the number of queries it can do as configured by the &lt;code>-querier.max-concurrent&lt;/code> parameter. Attempting more queries concurrently than this value causes the queries to queue up in the querier itself.&lt;/p>
&lt;p>For similar reasons scaling down the query frontend may cause a querier to not use its allocated memory and cpu effectively. This will lower effective resource utilization. Also, because individual queriers will be doing less work, this may cause increased queueing in the query frontends.&lt;/p>
&lt;h4 id="proposal">Proposal&lt;/h4>
&lt;p>Currently queriers are configured to have a &lt;a href="https://github.com/cortexproject/cortex/blob/50f53dba8f8bd5f62c0e85cc5d85684234cd1c1c/pkg/querier/frontend/worker.go#L146">max parallelism per query frontend&lt;/a>. An additional “total max concurrency” flag should be added.&lt;/p>
&lt;p>Total Max Concurrency would then be evenly divided amongst all available query frontends. This would decouple the amount of work a querier is attempting to do with the number of query frontends that happen to exist at this moment. Consequently this would allow allocated resources (e.g. k8s cpu/memory limits) to remain balanced with the work the querier was attempting as the query frontend is scaled up or down.&lt;/p>
&lt;p>A &lt;a href="https://github.com/cortexproject/cortex/pull/2456">PR&lt;/a> has already been merged to address this.&lt;/p>
&lt;h3 id="overwhelming-promql-concurrency">Overwhelming PromQL Concurrency&lt;/h3>
&lt;h4 id="challenge-1">Challenge&lt;/h4>
&lt;p>If #frontends &amp;gt; promql concurrency then the queriers are incapable of devoting even a single worker to each query frontend without risking queueing in the querier. Queuing in the querier is a highly undesirable state and one of the primary reasons the query frontend was originally created.&lt;/p>
&lt;h4 id="proposal-1">Proposal&lt;/h4>
&lt;p>When #frontends &amp;gt; promql concurrency then each querier will maintain &lt;a href="https://github.com/cortexproject/cortex/blob/8fb86155a7c7c155b8c4d31b91b267f9631b60ba/pkg/querier/frontend/worker.go#L194-L200">exactly one connection&lt;/a> to every frontend. As the query frontend is &lt;a href="https://github.com/cortexproject/cortex/blob/8fb86155a7c7c155b8c4d31b91b267f9631b60ba/pkg/querier/frontend/frontend.go#L279-L332">currently coded&lt;/a> it will attempt to use every open GRPC connection to execute a query in the attached queriers. Therefore, in this situation where #frontends &amp;gt; promql concurrency, the querier is exposing itself to more work then it is actually configured to perform.&lt;/p>
&lt;p>To prevent this we will add “flow control” information to the &lt;a href="https://github.com/cortexproject/cortex/blob/master/pkg/querier/frontend/frontend.proto#L21">ProcessResponse message&lt;/a> that is used to return query results from the querier to the query frontend. In an active system this message is passed multiple times per second from the queriers to the query frontends and would be a reliable way for the frontends to track the state of queriers and balance load.&lt;/p>
&lt;p>There are a lot of options for an exact implementation of this idea. An effective solution should be determined and chosen by modeling a set of alternatives. The details of this would be included in another design doc. A simple implementation would look something like the following:&lt;/p>
&lt;p>Add two new fields to &lt;a href="https://github.com/cortexproject/cortex/blob/master/pkg/querier/frontend/frontend.proto#L21">ProcessResponse&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-protobuf" data-lang="protobuf">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">message&lt;/span> &lt;span style="color:#000">ProcessResponse&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>&lt;span style="color:#a40000">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a40000">&lt;/span> &lt;span style="color:#000">httpgrpc.HTTPResponse&lt;/span> &lt;span style="color:#000">httpResponse&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">;&lt;/span>&lt;span style="color:#a40000">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a40000">&lt;/span> &lt;span style="color:#000">currentConcurrency&lt;/span> &lt;span style="color:#000">int&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>&lt;span style="color:#000;font-weight:bold">;&lt;/span>&lt;span style="color:#a40000">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a40000">&lt;/span> &lt;span style="color:#000">desiredConcurrency&lt;/span> &lt;span style="color:#000">int&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">3&lt;/span>&lt;span style="color:#000;font-weight:bold">;&lt;/span>&lt;span style="color:#a40000">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a40000">&lt;/span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>&lt;span style="color:#a40000">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>currentConcurrency&lt;/strong> - The current number of queries being executed by the querier.&lt;/p>
&lt;p>&lt;strong>desiredConcurrency&lt;/strong> - The total number of queries that a querier is capable of executing.&lt;/p>
&lt;p>Add a short backoff to the main frontend &lt;a href="https://github.com/cortexproject/cortex/blob/8fb86155a7c7c155b8c4d31b91b267f9631b60ba/pkg/querier/frontend/frontend.go#L288-L331">processing loop&lt;/a>. This would cause the frontend to briefly back off of any querier that was overloaded but continue to send queries to those that were capable of doing work.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">if&lt;/span> &lt;span style="color:#000">current&lt;/span> &lt;span style="color:#000;font-weight:bold">&amp;gt;&lt;/span> &lt;span style="color:#000">desired&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">zzz&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">:=&lt;/span> &lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">current&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span> &lt;span style="color:#000">desired&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span> &lt;span style="color:#000">backoffDuration&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">zzz&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">*=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">+&lt;/span> &lt;span style="color:#000">rand&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Float64&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">.1&lt;/span> &lt;span style="color:#8f5902;font-style:italic">// jitter
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic">&lt;/span> &lt;span style="color:#000">time&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Sleep&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">zzz&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Passing flow control information from the querier to the frontend would also open up additional future work for more sophisticated load balancing across queriers. For example by simply comparing and choosing &lt;a href="https://www.nginx.com/blog/nginx-power-of-two-choices-load-balancing-algorithm/">the least congested of two&lt;/a> queriers we could dramatically improve how well work is distributed.&lt;/p>
&lt;h3 id="increased-time-to-failure">Increased Time To Failure&lt;/h3>
&lt;h4 id="challenge-2">Challenge&lt;/h4>
&lt;p>Scaling the query frontend also increases the per tenant queue length by creating more queues. This could result in increased latencies where failing fast (429) would have been preferred.&lt;/p>
&lt;p>The operator could reduce the queue length per query frontend in response to scaling out, but then they would run the risk of unnecessarily failing a request due to unbalanced distribution across query frontends. Also, shorter queues run the risk of failing to properly service heavily sharded queries.&lt;/p>
&lt;p>Another concern is that a system with more queues will take longer to recover from an production event as it will have queued up more work.&lt;/p>
&lt;h4 id="proposal-2">Proposal&lt;/h4>
&lt;p>Currently we are not proposing any changes to alleviate this concern. We believe this is solvable operationally. This can be revisited as more information is gathered.&lt;/p>
&lt;h3 id="querier-discovery-lag">Querier Discovery Lag&lt;/h3>
&lt;h4 id="challenge-3">Challenge&lt;/h4>
&lt;p>Queriers have a configurable parameter that controls how often they refresh their query frontend list. The default value is 10 seconds. After a new query frontend is added the average querier will take 5 seconds (after DNS is updated) to become aware of it and begin requesting queries from it.&lt;/p>
&lt;h4 id="proposal-3">Proposal&lt;/h4>
&lt;p>It is recommended to add a readiness/health check to the query frontend to prevent it from receiving queries while it is waiting for queriers to connect. HTTP health checks are supported by &lt;a href="https://www.envoyproxy.io/learn/health-check">envoy&lt;/a>, &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">k8s&lt;/a>, &lt;a href="https://docs.nginx.com/nginx/admin-guide/load-balancer/http-health-check/">nginx&lt;/a>, and basically any commodity load balancer. The query frontend would not indicate healthy on its health check until at least one querier had connected.&lt;/p>
&lt;p>In a k8s environment this will require two services. One service for discovery with &lt;code>publishNotReadyAddresses&lt;/code> set to true and one service for load balancing which honors the healthcheck/readiness probe. After a new query-frontend instance is created the &amp;ldquo;discovery service&amp;rdquo; would immediately have the ip of the new instance which would allow queriers to discover and attach to it. After queriers had connected it would then raise its readiness probe and appear on the &amp;ldquo;load balancing&amp;rdquo; service and begin receiving traffic.&lt;/p>
&lt;h3 id="dilutes-tenant-fairness">Dilutes Tenant Fairness&lt;/h3>
&lt;h4 id="challenge-4">Challenge&lt;/h4>
&lt;p>Given &lt;code>f&lt;/code> query frontends, &lt;code>n&lt;/code> tenants and an average of &lt;code>q&lt;/code> queries in the frontend per tenant. The following assumes that queries are perfectly distributed across query frontends. The number of tenants per instance would be:&lt;/p>
&lt;img src="https://render.githubusercontent.com/render/math?math=m = floor(n * \frac{min(q,f)}{f})">
&lt;p>The chance that a query by a tenant with &lt;code>Q&lt;/code> queries in the frontend is serviced next is:&lt;/p>
&lt;img src="https://render.githubusercontent.com/render/math?math=min(Q,f)* \frac{1}{min(q * n %2b Q,f)}*\frac{1}{m %2b 1}">
&lt;p>Note that fewer query frontends caps the impact of the number of active queries per tenant. If there is only one query frontend then the equation reduces to:&lt;/p>
&lt;img src="https://render.githubusercontent.com/render/math?math=\frac{1}{n}">
&lt;p>and every tenant has an equal chance of being serviced regardless of the number of queued queries.&lt;/p>
&lt;p>Adding more query frontends favors high volume tenants by giving them more slots to be picked up by the next available querier. Fewer query frontends allows for an even playing field regardless of the number of active queries.&lt;/p>
&lt;p>For clarity, it should be noted that tenant fairness is only impacted if queries are being queued in the frontend. Under normal operations this is currently not occurring although this may change with increased sharding.&lt;/p>
&lt;h4 id="proposal-4">Proposal&lt;/h4>
&lt;p>Tenancy fairness is complex and is currently &lt;em>not&lt;/em> impacting our system. Therefore we are proposing a very simple improvement to the query frontend. If/when frontend queuing becomes more common this can be revisited as we will understand the problem better.&lt;/p>
&lt;p>Currently the query frontend &lt;a href="https://github.com/cortexproject/cortex/blob/50f53dba8f8bd5f62c0e85cc5d85684234cd1c1c/pkg/querier/frontend/frontend.go#L362-L367">picks a random tenant&lt;/a> to service when a querier requests a new query. This can increase long tail latency if a tenant gets “unlucky” and is also exacerbated for low volume tenants by scaling the query frontend. Instead the query frontend could use a round robin approach to choose the next tenant to service. Round robin is a commonly used algorithm to increase fairness in scheduling.&lt;/p>
&lt;p>This would be a very minor improvement, but would give some guarantees to low volume tenants that their queries would be serviced. This has been proposed in this &lt;a href="https://github.com/cortexproject/cortex/issues/2431">issue&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Pros:&lt;/strong> Requires local knowledge only. Easier to implement than weighted round robin.&lt;/p>
&lt;p>&lt;strong>Cons:&lt;/strong> Improvement is minor.&lt;/p>
&lt;p>&lt;strong>Alternatives to Round Robin&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Do Nothing&lt;/strong>&lt;/p>
&lt;p>As is noted above tenancy fairness only comes into play when queries start queueing up in the query frontend. Internal Metrics for multi-tenant Cortex at Grafana show that this has only happened 5 times in the past week significantly enough to have been caught by Prometheus.&lt;/p>
&lt;p>Right now doing nothing is a viable option that will, almost always, fairly serve our tenants. There is, however, some concern that as sharding becomes more commonplace queueing will become more common and QOS will suffer due to reasons outlined in &lt;a href="#dilutes-tenant-fairness">Dilutes Tenant Fairness&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Pros:&lt;/strong> Easy!&lt;/p>
&lt;p>&lt;strong>Cons:&lt;/strong> Nothing happens!&lt;/p>
&lt;p>&lt;strong>Weighted Round Robin&lt;/strong>&lt;/p>
&lt;p>The query frontends could maintain a local record of throughput or work per tenant. Tenants could then be sorted in QOS bands. In its simplest form there would be two QOS bands. The band of low volume tenants would be serviced twice for every one time the band of high volume tenants would be serviced. The full details of this approach would require a separate proposal.&lt;/p>
&lt;p>This solution would also open up interesting future work. For instance, we could allow operators to manually configure tenants into QOS bands.&lt;/p>
&lt;p>&lt;strong>Pros:&lt;/strong> Requires local knowledge only. Can be extended later to allow tenants to be manually sorted into QOS tiers.&lt;/p>
&lt;p>&lt;strong>Cons:&lt;/strong> Improvement is better than Round Robin only. Relies on even distribution of queries across frontends. Increased complexity and difficulty in reasoning about edge cases.&lt;/p>
&lt;p>&lt;strong>Weighted Round Robin With Gossiped Traffic&lt;/strong>&lt;/p>
&lt;p>This approach would be equivalent to Weighted Round Robin proposed above but with tenant traffic volume gossiped between query frontends.&lt;/p>
&lt;p>&lt;strong>Pros:&lt;/strong> Benefits of Weighted Round Robin without the requirement of even query distribution. Even though it requires distributed information a failure in gossip means it gracefully degrades to Weighted Round Robin.&lt;/p>
&lt;p>&lt;strong>Cons:&lt;/strong> Requires cross instance communication. Increased complexity and difficulty in reasoning about edge cases.&lt;/p>
&lt;h2 id="alternative">Alternative&lt;/h2>
&lt;p>The proposals in this document have preferred augmenting existing components to make decisions with local knowledge. The unstated goal of these proposals is to build a distributed queue across a scaled query frontend that reliably and fairly serves our tenants.&lt;/p>
&lt;p>Overall, these proposals will create a robust system that is resistant to network partitions and failures of individual pieces. However, it will also create a complex system that could be difficult to reason about, contain hard to ascertain edge cases and nuanced failure modes.&lt;/p>
&lt;p>The alternative is, instead of building a distributed queue, to add a new cortex queueing service that sits in between the frontends and the queriers. This queueing service would pull from the frontends and distribute to the queriers. It would decouple the stateful queue from the stateless elements of the query frontend and allow us to easily scale the query frontend while keeping the queue itself a singleton. In a single binary HA mode one (or few) of the replicas would be leader elected to serve this role.&lt;/p>
&lt;p>Having a singleton queue is attractive because it is simple to reason about and gives us a single place to make fair cross tenant queueing decisions. It does, however, create a single point of failure and add another network hop to the query path.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In this document we reviewed the &lt;a href="#query-frontend-role">reasons the frontend exists&lt;/a>, &lt;a href="#challenges-and-proposals">challenges and proposals to scaling the frontend&lt;/a> and &lt;a href="#alternative">an alternative architecture that avoids most problems but comes with its own challenges.&lt;/a>&lt;/p>
&lt;table>
&lt;tr>
&lt;td>&lt;strong>Challenge&lt;/strong>
&lt;/td>
&lt;td>&lt;strong>Proposal&lt;/strong>
&lt;/td>
&lt;td>&lt;strong>Status&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dynamic Querier Concurrency
&lt;/td>
&lt;td>Add Max Total Concurrency in Querier
&lt;/td>
&lt;td>&lt;a href="https://github.com/cortexproject/cortex/pull/2456">Pull Request&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Overwhelming PromQL Concurrency
&lt;/td>
&lt;td>Queriers Coordinate Concurrency with Frontends
&lt;/td>
&lt;td>Proposed
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Increased Time to Failure
&lt;/td>
&lt;td>Operational/Configuration Issue. No Changes Proposed.
&lt;/td>
&lt;td>
N/A
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Querier Discovery Lag
&lt;/td>
&lt;td>Query Frontend HTTP Health Checks
&lt;/td>
&lt;td>&lt;a href="https://github.com/cortexproject/cortex/pull/2733">Pull Request&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dilutes Tenant Fairness
&lt;/td>
&lt;td>Round Robin with additional alternatives proposed
&lt;/td>
&lt;td>&lt;a href="https://github.com/cortexproject/cortex/pull/2553">Pull Request&lt;/a>
&lt;/td>
&lt;/tr>
&lt;/table></description></item><item><title>Docs: Shuffle sharding and zone awareness</title><link>/docs/proposals/shuffle-sharding-and-zone-awareness/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/shuffle-sharding-and-zone-awareness/</guid><description>
&lt;ul>
&lt;li>Author: @pracucci, @tomwilkie, @pstibrany&lt;/li>
&lt;li>Reviewers:&lt;/li>
&lt;li>Date: August 2020&lt;/li>
&lt;li>Status: Accepted, implemented in &lt;a href="https://github.com/cortexproject/cortex/pull/3090">PR #3090&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="shuffle-sharding-and-zone-awareness">Shuffle sharding and zone awareness&lt;/h2>
&lt;h3 id="background">Background&lt;/h3>
&lt;p>Cortex shards the received series across all available ingesters. In a multi-tenant cluster, each tenant series are sharded across all ingesters. This allows to horizontally scale the series across the pool of ingesters but also suffers some issues:&lt;/p>
&lt;ol>
&lt;li>Given every tenant writes series to all ingesters, there’s no isolation between tenants - a single misbehaving tenant can affect the whole cluster.&lt;/li>
&lt;li>Each ingester needs an open TSDB per tenant per ingester - which has significant memory overhead. The larger the number of tenants, the higher the TSDB memory overhead, regardless of the number of series stored in each TSDB.&lt;/li>
&lt;li>Similarly, the number of uploaded blocks to the storage every 2 hours is a function of the number of TSDBs open for each ingester. A cluster with a large number of small tenants will upload a very large number of blocks to the storage, each block being very small, increasing the number of API calls against the storage bucket.&lt;/li>
&lt;/ol>
&lt;p>Cortex currently supports sharding a tenant to a subset of the ingesters on the write path &lt;a href="https://github.com/cortexproject/cortex/pull/1947">PR&lt;/a>, using a feature called “&lt;strong>subring&lt;/strong>”. However, the current subring implementation suffers two issues:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>No zone awareness:&lt;/strong> it doesn’t guarantee selected instances are balanced across availability zones&lt;/li>
&lt;li>&lt;strong>No shuffling:&lt;/strong> the implementation is based on the hash ring and it selects N consecutive instances in the ring. This means that, instead of minimizing the likelihood that two tenants share the same instances, it emphasises it. In order to provide a good isolation between tenants, we want to minimize the chances that two tenants share the same instances.&lt;/li>
&lt;/ol>
&lt;h3 id="goal">Goal&lt;/h3>
&lt;p>The goal of this work is to fix “shuffling” and “zone-awareness” when building the subring for a given tenant, honoring the following properties:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Stability:&lt;/strong> given the same ring, the algorithm always generates the same subring for a given tenant, even across different machines&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Consistent_hashing">&lt;strong>Consistency:&lt;/strong>&lt;/a> when the ring is resized, only n/m series are remapped on average (where n is the number of series and m is the number of replicas).&lt;/li>
&lt;li>&lt;strong>Shuffling:&lt;/strong> probabilistically and for a large enough cluster, ensure every tenant gets a different set of instances, with a reduced number of overlapping instances between two tenants to improve failure isolation.&lt;/li>
&lt;li>&lt;strong>Zone-awareness (balanced):&lt;/strong> the subring built for each tenant contains a balanced number of instances for each availability zone. Selecting the same number of instances in each zone is an important property because we want to preserve the balance of in-memory series across ingesters. Having less replicas in one zone will mean more load per node in this zone, which is something we want to avoid.&lt;/li>
&lt;/ul>
&lt;h3 id="proposal">Proposal&lt;/h3>
&lt;p>This proposal is based on &lt;a href="https://aws.amazon.com/builders-library/workload-isolation-using-shuffle-sharding/">Amazon’s Shuffle Sharding article&lt;/a> and the algorithm has been inspired by shuffle sharding implementation in the &lt;a href="https://github.com/awslabs/route53-infima/blob/master/src/main/java/com/amazonaws/services/route53/infima/SimpleSignatureShuffleSharder.java">AWS Route53 infima library&lt;/a>.&lt;/p>
&lt;p>Given a tenant and a shard size S (number of instances to which tenant data/workload should be sharded to), we build a subring selecting N instances from each zone, where N = ceil(S / num of zones). The shard size S is required to be a multiple of the number of zones, in order to select an equal number of instances from each zone.&lt;/p>
&lt;p>To do it, we &lt;strong>treat each zone as a separate ring&lt;/strong> and select N unique instances from each zone. The instances selection process works as follow:&lt;/p>
&lt;ol>
&lt;li>Generate a seed based on the tenant ID&lt;/li>
&lt;li>Initialise a pseudo random number generator with the tenant’s seed. The random generator must guarantee predictable numbers given the same input seed.&lt;/li>
&lt;li>Generate a sequence of N random numbers, where N is the number of instances to select from the zone. Each random number is used as a “token” to look up instances in the ring. For each random number:&lt;/li>
&lt;li>Lookup the instance holding that token in the ring&lt;/li>
&lt;li>If the instance has not been previously selected, then pick it&lt;/li>
&lt;li>If the instance was previously selected (we call this a “collision”), then continue walking the ring clockwise until we find an instance which has not been selected yet&lt;/li>
&lt;/ol>
&lt;h3 id="guaranteed-properties">Guaranteed properties&lt;/h3>
&lt;h4 id="stability">Stability&lt;/h4>
&lt;p>The same tenant ID always generates the same seed. Given the same seed, the pseudo number random generator always generates the same sequence of numbers.&lt;/p>
&lt;p>This guarantees that, given the same ring, we generate the same exact subring for a given tenant.&lt;/p>
&lt;h4 id="consistency">Consistency&lt;/h4>
&lt;p>The consistency property is honored by two aspects of the algorithm:&lt;/p>
&lt;ol>
&lt;li>The quantity of random numbers generated is always equal to the shard size S, even in case of “collisions”. A collision is when the instance holding the random token has already been picked and we need to select a different instance which has not been picked yet.&lt;/li>
&lt;li>In case of collisions, we select the “next” instance continuing walking the ring instead of generating another random number&lt;/li>
&lt;/ol>
&lt;h5 id="example-adding-an-instance-to-the-ring">Example adding an instance to the ring&lt;/h5>
&lt;p>Let’s consider an initial ring with 3 instances and 1 zone (for simplicity):&lt;/p>
&lt;ul>
&lt;li>I1 - Tokens: 1, 8, 15&lt;/li>
&lt;li>I2 - Tokens: 5, 11, 19&lt;/li>
&lt;li>I3 - Tokens: 7, 13, 21&lt;/li>
&lt;/ul>
&lt;p>With a replication factor = 2, the random sequence looks up:&lt;/p>
&lt;ul>
&lt;li>3 (I2)&lt;/li>
&lt;li>6 (I1)&lt;/li>
&lt;/ul>
&lt;p>Then we add a new instance and the &lt;strong>updated ring&lt;/strong> is:&lt;/p>
&lt;ul>
&lt;li>I1 - Tokens: 1, 8, 15&lt;/li>
&lt;li>I2 - Tokens: 5, 11, 19&lt;/li>
&lt;li>I3 - Tokens: 7, 13, 21&lt;/li>
&lt;li>I4 - Tokens: 4, 7, 17&lt;/li>
&lt;/ul>
&lt;p>Now, let’s compare two different algorithms to solve collisions:&lt;/p>
&lt;ul>
&lt;li>Using the random generator:&lt;br />
Random sequence = 3 (&lt;strong>I4&lt;/strong>), 6 (I4 - collision), 12 (&lt;strong>I3&lt;/strong>)&lt;br />
&lt;strong>all instances are different&lt;/strong> (I4, I3)&lt;/li>
&lt;li>Walking the ring:&lt;br />
Random sequence = 3 (&lt;strong>I4&lt;/strong>), 6 (I4 - collision, next is &lt;strong>I1&lt;/strong>)&lt;br />
&lt;strong>only 1 instance is different&lt;/strong> (I4, I1)&lt;/li>
&lt;/ul>
&lt;h4 id="shuffling">Shuffling&lt;/h4>
&lt;p>Unless when resolving collisions, the algorithm doesn’t walk the ring to find the next instances, but uses a sequence of random numbers. This guarantees instances are shuffled, between different tenants, when building the subring.&lt;/p>
&lt;h4 id="zone-awareness">Zone-awareness&lt;/h4>
&lt;p>We treat each zone as a separate ring and select an equal number of instances from each zone. This guarantees a fair balance of instances between zones.&lt;/p>
&lt;h3 id="proof-of-concept">Proof of concept&lt;/h3>
&lt;p>We’ve built a &lt;a href="https://github.com/cortexproject/cortex/pull/3090">reference implementation&lt;/a> of the proposed algorithm, to test the properties described above.&lt;/p>
&lt;p>In particular, we’ve observed that the &lt;a href="https://github.com/cortexproject/cortex/pull/3090/files#diff-121ffce90aa9932f6b87ffd138e0f36aR281">actual distribution&lt;/a> of matching instances between different tenants is very close to the &lt;a href="https://docs.google.com/spreadsheets/d/1FXbiWTXi6bdERtamH-IfmpgFq1fNL4GP_KX_yJvbRi4/edit">theoretical one&lt;/a>, as well as consistency and stability properties are both honored.&lt;/p></description></item><item><title>Docs: Shuffle sharding on the read path</title><link>/docs/proposals/shuffle-sharding-on-the-read-path/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/shuffle-sharding-on-the-read-path/</guid><description>
&lt;ul>
&lt;li>Author: @pracucci, @tomwilkie, @pstibrany&lt;/li>
&lt;li>Reviewers:&lt;/li>
&lt;li>Date: August 2020&lt;/li>
&lt;li>Status: Proposed, partially implemented&lt;/li>
&lt;/ul>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Cortex currently supports sharding of tenants to a subset of the ingesters on the write path &lt;a href="https://github.com/cortexproject/cortex/pull/1947">PR&lt;/a>.&lt;/p>
&lt;p>This feature is called “subring”, because it computes a subset of nodes registered to the hash ring. The aim of this feature is to improve isolation between tenants and reduce the number of tenants impacted by an outage.&lt;/p>
&lt;p>This approach is similar to the techniques described in &lt;a href="https://aws.amazon.com/builders-library/workload-isolation-using-shuffle-sharding/">Amazon’s Shuffle Sharding article&lt;/a>, but currently suffers from a non random selection of nodes (&lt;em>proposed solution below&lt;/em>).&lt;/p>
&lt;p>Cortex can be &lt;strong>configured&lt;/strong> with a default subring size, and then it can be &lt;a href="https://cortexmetrics.io/docs/configuration/configuration-file/#limits_config">customized on a per-tenant basis&lt;/a>. The per-tenant configuration is live reloaded during runtime and applied without restarting the Cortex process.&lt;/p>
&lt;p>The subring sharding currently supports only the write-path. The read-path is not shuffle sharding aware. For example, an outage of more than one ingester with RF=3 will affect all tenants, or a particularly noisy tenant wrt queries has the ability to affect all tenants.&lt;/p>
&lt;h2 id="goals">Goals&lt;/h2>
&lt;p>The Cortex &lt;strong>read path should support shuffle sharding to isolate&lt;/strong> the impact of an outage in the cluster. The shard size must be dynamically configurable on a per-tenant basis during runtime.&lt;/p>
&lt;p>This deliverable involves introducing shuffle sharding in:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Query-frontend → Querier&lt;/strong> (for queries sharding) &lt;a href="https://github.com/cortexproject/cortex/pull/3113">PR #3113&lt;/a>&lt;/li>
&lt;li>&lt;strong>Querier → Store-gateway&lt;/strong> (for blocks sharding) &lt;a href="https://github.com/cortexproject/cortex/pull/3069">PR #3069&lt;/a>&lt;/li>
&lt;li>&lt;strong>Querier→ Ingesters&lt;/strong> (for queries on recent data)&lt;/li>
&lt;li>&lt;strong>Ruler&lt;/strong> (for rule and alert evaluation)&lt;/li>
&lt;/ul>
&lt;h3 id="prerequisite-fix-subring-shuffling">Prerequisite: fix subring shuffling&lt;/h3>
&lt;p>The solution is implemented in &lt;a href="https://github.com/cortexproject/cortex/pull/3090">https://github.com/cortexproject/cortex/pull/3090&lt;/a>.&lt;/p>
&lt;h4 id="the-problem">The problem&lt;/h4>
&lt;p>The subring is a subset of nodes that should be used for a specific tenant.&lt;/p>
&lt;p>The current subring implementation doesn’t shuffle tenants across nodes. Given a tenant ID, it finds the first node owning the hash(tenant ID) token and then it picks N distinct consecutive nodes walking the ring clockwise.&lt;/p>
&lt;p>For example, in a cluster with 6 nodes (numbered 1-6) and a replication factor of 3, three tenants (A, B, C) could have the following shards:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Tenant ID&lt;/th>
&lt;th>Node 1&lt;/th>
&lt;th>Node 2&lt;/th>
&lt;th>Node 3&lt;/th>
&lt;th>Node 4&lt;/th>
&lt;th>Node 5&lt;/th>
&lt;th>Node 6&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>A&lt;/td>
&lt;td>x&lt;/td>
&lt;td>x&lt;/td>
&lt;td>x&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>B&lt;/td>
&lt;td>&lt;/td>
&lt;td>x&lt;/td>
&lt;td>x&lt;/td>
&lt;td>x&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>x&lt;/td>
&lt;td>x&lt;/td>
&lt;td>x&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="proposal">Proposal&lt;/h4>
&lt;p>We propose to build the subring picking N distinct and random nodes registered in the ring, using the following algorithm:&lt;/p>
&lt;ol>
&lt;li>SID = tenant ID&lt;/li>
&lt;li>SID = hash(SID)&lt;/li>
&lt;li>Look for the node owning the token range containing FNV-1a(SID)&lt;/li>
&lt;li>Loop to (2) until we’ve found N distinct nodes (where N is the shard size)&lt;/li>
&lt;/ol>
&lt;p>&lt;em>hash() function to be decided. The required property is to be strong enough to not generate loops across multiple subsequent hashing of the previous hash.&lt;/em>&lt;/p>
&lt;h3 id="query-frontend--queriers-shuffle-sharding">Query-frontend → Queriers shuffle sharding&lt;/h3>
&lt;p>Implemented in &lt;a href="https://github.com/cortexproject/cortex/pull/3113">https://github.com/cortexproject/cortex/pull/3113&lt;/a>.&lt;/p>
&lt;h3 id="how-querier-runs-query-frontend-jobs">How querier runs query-frontend jobs&lt;/h3>
&lt;p>Today &lt;strong>each&lt;/strong> querier connects to &lt;strong>each&lt;/strong> query-frontend instance, and calls a single “Process” method via gRPC.&lt;/p>
&lt;p>“Process” is a bi-directional streaming gRPC method – using the server-to-client stream for sending requests from query-frontend to the querier, and client-to-server stream for returning results from querier to the query-frontend. NB this is the opposite of what might be considered normal. Query-frontend scans all its queues with pending query requests, and picks a query to execute based on a fair schedule between tenants.&lt;/p>
&lt;p>The query request is then sent to an idle querier worker over the stream opened in the Process method, and the query-frontend then waits for a response from querier. This loop repeats until querier disconnects.&lt;/p>
&lt;h3 id="proposal-1">Proposal&lt;/h3>
&lt;p>To support shuffle sharding, Query-Frontends will keep a list of connected Queriers, and randomly (but consistently between query-frontends) choose N of them to distribute requests to. When Query-Frontend looks for the next request to send to a given querier, it will only consider tenants that “belong” to the Querier.&lt;/p>
&lt;p>To choose N Queriers for a tenant, we propose to use a simple algorithm:&lt;/p>
&lt;ol>
&lt;li>Sort all Queriers by their ID&lt;/li>
&lt;li>SID = tenant ID&lt;/li>
&lt;li>SID = hash(SID)&lt;/li>
&lt;li>Pick the querier from the list of sorted queries with:&lt;br />
index = FNV-1a(SID) % number of Queriers&lt;/li>
&lt;li>Loop to (3) until we’ve found N distinct queriers (where N is the shard size) and stop early if there aren’t enough queriers&lt;/li>
&lt;/ol>
&lt;p>&lt;em>hash() function to be decided. The required property is to be strong enough to not generate loops across multiple subsequent hashing of the previous hash.&lt;/em>&lt;/p>
&lt;h3 id="properties">Properties&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Stability:&lt;/strong> this will produce the same result on all query-frontends as long as all queriers are connected to all query-frontends.&lt;/li>
&lt;li>&lt;strong>Simplicity:&lt;/strong> no external dependencies.&lt;/li>
&lt;li>&lt;strong>No consistent hashing:&lt;/strong> adding/removing queriers will cause “resharding” of tenants between queriers. While in general that’s not desirable property, queriers are stateless so it doesn’t seem to matter in this case.&lt;/li>
&lt;/ul>
&lt;h3 id="implementation-notes">Implementation notes&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Caching:&lt;/strong> once this list of queriers to use for a tenant is computed in the query-frontend, it is cached in memory until queriers are added or removed. Per-tenant cache entries will have a TTL to discard tenants not “seen” since a while.&lt;/li>
&lt;li>&lt;strong>Querier ID:&lt;/strong> Query-frontends currently don’t have any identity for queriers. We need to introduce sending of a unique ID (eg. hostname) by querier to query-frontend when it calls “Process” method.&lt;/li>
&lt;li>&lt;strong>Backward-compatibility:&lt;/strong> when querier shuffle sharding is enabled, the system expects that both query-frontend and querier will run a compatible version. Cluster version upgrade will require to rollout new query-frontends and queriers first, and then enable shuffle sharding.&lt;/li>
&lt;li>&lt;strong>UI:&lt;/strong> we propose to expose the current state of the query-frontend through a new endpoint which should display:
&lt;ul>
&lt;li>Which querier are connected to the query-frontend&lt;/li>
&lt;li>Are there any “old” queriers, that are receiving requests from all tenants?&lt;/li>
&lt;li>Mapping of tenants to queriers. Note that this mapping may only be available for tenants with pending requests on given query-frontend, and therefore be very dynamic.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="configuration">Configuration&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Shard size&lt;/strong> will be configurable on a per-tenant basis via existing “runtime-configuration” mechanism (limits overrides). Changing a value for a tenant needs to invalidate cached per-tenant queriers.&lt;/li>
&lt;li>Queriers shard size will be a different setting than then one used for writes.&lt;/li>
&lt;/ul>
&lt;h3 id="evaluated-alternatives">Evaluated alternatives&lt;/h3>
&lt;h4 id="use-the-subring">Use the subring&lt;/h4>
&lt;p>An alternative option would be using the subring. This implies having queriers registering to the hash ring and query-frontend instances using the ring client to find the queriers subring for each tenant.&lt;/p>
&lt;p>This solution looks adding more complexity without any actual benefit.&lt;/p>
&lt;h4 id="change-query-frontend--querier-architecture">Change query-frontend → querier architecture&lt;/h4>
&lt;p>Completely different approach would be to introduce a place where starting queriers would register (eg. DNS-based service discovery), and let query-frontends discover queriers from this central registry.&lt;/p>
&lt;p>Possible benefit would be that queriers don’t need to initiate connection to all query-frontends, but query-frontends would only connect to queriers for which they have actual pending requests. However this would be a significant redesign of how query-frontend / querier communication works.&lt;/p>
&lt;h2 id="querier--store-gateway-shuffle-sharding">Querier → Store-gateway shuffle sharding&lt;/h2>
&lt;p>Implemented in &lt;a href="https://github.com/cortexproject/cortex/pull/3069">https://github.com/cortexproject/cortex/pull/3069&lt;/a>.&lt;/p>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>As of today, the store-gateway supports blocks sharding with customizable replication factor (defaults to 3). Blocks of a single tenant are sharded across all store-gateway instances and so to execute a query the querier may touch any store-gateway in the cluster.&lt;/p>
&lt;p>The current sharding implementation is based on a &lt;strong>hash ring&lt;/strong> formed by store-gateway instances.&lt;/p>
&lt;h3 id="proposal-2">Proposal&lt;/h3>
&lt;p>The proposed solution to add shuffle sharding support to the store-gateway is to &lt;strong>leverage on the existing hash ring&lt;/strong> to build a per-tenant &lt;strong>subring&lt;/strong>, which is then used both by the querier and store-gateway to know to which store-gateway a block belongs to.&lt;/p>
&lt;h3 id="configuration-1">Configuration&lt;/h3>
&lt;ul>
&lt;li>Shuffle sharding can be enabled in the &lt;strong>store-gateway configuration.&lt;/strong> It supports a &lt;strong>default sharding factor,&lt;/strong> which is &lt;strong>overridable on a per-tenant basis&lt;/strong> and live reloaded during runtime (using the existing limits config).&lt;/li>
&lt;li>The querier already requires the store-gateway configuration when the blocks sharding is enabled. Similarly, when shuffle sharding is enabled the querier will require the store-gateway shuffle sharding configuration as well.&lt;/li>
&lt;/ul>
&lt;h3 id="implementation-notes-1">Implementation notes&lt;/h3>
&lt;p>When shuffle sharding is enabled:&lt;/p>
&lt;ul>
&lt;li>The &lt;strong>store-gateway&lt;/strong> &lt;code>syncUsersBlocks()&lt;/code> will build a tenant’s subring for each tenant found scanning the bucket and will skip any tenant not belonging to its shard.&lt;br />
Likewise, ShardingMetadataFilter will first build a &lt;strong>tenant’s subring&lt;/strong> and then will use the existing logic to filter out blocks not belonging to store-gateway instance itself. The tenant ID can be read from the block’s meta.json.&lt;/li>
&lt;li>The &lt;strong>querier&lt;/strong> &lt;code>blocksStoreReplicationSet.GetClientsFor()&lt;/code> will first build a &lt;strong>tenant’s subring&lt;/strong> and then will use the existing logic to find out to which store-gateway instance each requested block belongs to.&lt;/li>
&lt;/ul>
&lt;h3 id="evaluated-alternatives-1">Evaluated alternatives&lt;/h3>
&lt;p>&lt;em>Given the store-gateways already form a ring and building the shuffle sharding based on the ring (like in the write path) doesn’t introduce extra operational complexity, we haven’t discussed alternatives.&lt;/em>&lt;/p>
&lt;h2 id="querier-ingesters-shuffle-sharding">Querier→ Ingesters shuffle sharding&lt;/h2>
&lt;p>We’re currently discussing/evaluating different options.&lt;/p>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;p>Cortex must guarantee query correctness; transiently incorrect results may be cached and returned forever. The main problem to solve when introducing ingesters shuffle sharding on the read path is to make sure that a querier fetch data from all ingesters having at least 1 sample for a given tenant.&lt;/p>
&lt;p>The problem to solve is: how can a querier efficiently find which ingesters have data for a given tenant? Each option must consider the changing of the set of ingesters and the changing of each tenant’s subring size.&lt;/p>
&lt;h3 id="proposal-use-only-the-information-contained-in-the-ring">Proposal: use only the information contained in the ring.&lt;/h3>
&lt;p>&lt;em>This section describes an alternative approach. Discussion is still on-going.&lt;/em>&lt;/p>
&lt;p>The idea is for the queries to be able to deduce what ingesters could possibly hold data for a given tenant by just consulting the ring (and the per-tenant sub ring sizes). We posit that this is possible with only a single piece of extra information: a single timestamp per ingester saying when the ingester first joined the ring.&lt;/p>
&lt;h4 id="scenario-ingester-scale-up">Scenario: ingester scale up&lt;/h4>
&lt;p>When a new ingester is added to the ring, there will be a set of user subrings that see a change: an ingester being removed, and a new one being added. We need to guarantee that for some time period (the block flush interval), the ingester removed is also consulted for queries.&lt;/p>
&lt;p>To do this, during the subring selection if we encounters an ingester added within the time period, we will add this to the subring but continue node selection as before - in effect, selecting an extra ingester:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">var&lt;/span> &lt;span style="color:#000;font-weight:bold">(&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">subringSize&lt;/span> &lt;span style="color:#204a87;font-weight:bold">int&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">selectedNodes&lt;/span> &lt;span style="color:#000;font-weight:bold">[]&lt;/span>&lt;span style="color:#000">Node&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">deadline&lt;/span> &lt;span style="color:#000;font-weight:bold">=&lt;/span> &lt;span style="color:#000">time&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Now&lt;/span>&lt;span style="color:#000;font-weight:bold">().&lt;/span>&lt;span style="color:#000">Add&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span>&lt;span style="color:#000">flushWindow&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">for&lt;/span> &lt;span style="color:#204a87">len&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">selectedNodes&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span> &lt;span style="color:#000;font-weight:bold">&amp;lt;&lt;/span> &lt;span style="color:#000">subringSize&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">token&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">:=&lt;/span> &lt;span style="color:#000">random&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Next&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">node&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">:=&lt;/span> &lt;span style="color:#000">getNodeByToken&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">token&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">if&lt;/span> &lt;span style="color:#000">node&lt;/span> &lt;span style="color:#000">in&lt;/span> &lt;span style="color:#000">selectedNodes&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">node&lt;/span> &lt;span style="color:#000;font-weight:bold">=&lt;/span> &lt;span style="color:#000">node&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Next&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">continue&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">if&lt;/span> &lt;span style="color:#000">node&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Added&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">After&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">deadline&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">subringSize&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">++&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">selectedNodes&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Add&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">node&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">node&lt;/span> &lt;span style="color:#000;font-weight:bold">=&lt;/span> &lt;span style="color:#000">node&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Next&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">continue&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">selectedNodes&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Add&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">node&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">break&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="scenario-ingester-scale-down">Scenario: ingester scale down&lt;/h4>
&lt;p>When an ingester is permanently removed from the ring it will flush its data to the object store and the subrings containing the removed ingester will gain a “new” ingester. Queries consult the store and merge the results with those from the ingesters, so no data will be missed.&lt;/p>
&lt;p>Queriers and store-gateways will discover newly flushed blocks on next sync (&lt;code>-blocks-storage.bucket-store.sync-interval&lt;/code>, default 5 minutes).
Multiple ingesters should not be scaled-down within this interval.&lt;/p>
&lt;p>To improve read-performance, queriers and rulers are usually configured with non-zero value of &lt;code>-querier.query-store-after&lt;/code> option.
This option makes queriers and rulers to consult &lt;strong>only&lt;/strong> ingesters when running queries within specified time window (eg. 12h).
During scale-down this needs to be lowered in order to let queriers and rulers use flushed blocks from the storage.&lt;/p>
&lt;h4 id="scenario-increase-size-of-a-tenants-subring">Scenario: increase size of a tenant’s subring&lt;/h4>
&lt;p>Node selection for subrings is stable - increasing the size of a subring is guaranteed to only add new nodes to it (and not remove any nodes). Hence, if a tenant’s subring is increase in size the queriers will notice the config change and start consulting the new ingester.&lt;/p>
&lt;h4 id="scenario-decreasing-size-of-a-tenants-subring">Scenario: decreasing size of a tenant’s subring&lt;/h4>
&lt;p>If a tenant’s subring decreases in size, there is currently no way for the queriers to know how big the ring was previously, and hence they will potentially miss an ingester with data for that tenant.&lt;/p>
&lt;p>This is deemed an infrequent operation that we considered banning, but have a proposal for how we might make it possible:&lt;/p>
&lt;p>The proposal is to have separate read subring and write subring size in the config. The read subring will not be allowed to be smaller than the write subring. When reducing the size of a tenant’s subring, operators must first reduce the write subring, and then two hours later when the blocks have been flushed, the read subring. In the majority of cases the read subring will not need to be specified, as it will default to the write subring size.&lt;/p>
&lt;h3 id="considered-alternative-1-ingesters-expose-list-of-tenants">Considered alternative #1: Ingesters expose list of tenants&lt;/h3>
&lt;p>A possible solution could be keeping in the querier an in-memory data structure to map each ingester to the list of tenants for which it has some data. This data structure would be constructed at querier startup, and then periodically updated, interpolating two information:&lt;/p>
&lt;ol>
&lt;li>The current state of the ring&lt;/li>
&lt;li>The list of tenants directly exposed by each ingester (via a dedicated gRPC call)&lt;/li>
&lt;/ol>
&lt;h4 id="scenario-new-querier-starts-up">Scenario: new querier starts up&lt;/h4>
&lt;p>When a querier starts up and before getting ready:&lt;/p>
&lt;ol>
&lt;li>It scans all ingesters (discovered via the ring) and fetches the list of tenants for which each ingester has some data&lt;/li>
&lt;li>For each found tenant (unique list of tenant IDs across all ingesters responses), the querier looks at the current state of the ring and adds to the map the list of ingesters currently assigned to the tenant shard, even if they don’t hold any data yet (because may start receiving series shortly)&lt;/li>
&lt;/ol>
&lt;p>Then the querier watches the ingester ring and rebuilds the in-memory map whenever the ring topology changes.&lt;/p>
&lt;h4 id="scenario-querier-receives-a-query-for-an-unknown-tenant">Scenario: querier receives a query for an unknown tenant&lt;/h4>
&lt;p>A new tenant starts remote writing to the cluster. The querier doesn’t know it in its in-memory map, so it adds the tenant on the fly to the map just looking at the current state of the ring.&lt;/p>
&lt;h4 id="scenario-ingester-scale-up--down">Scenario: ingester scale up / down&lt;/h4>
&lt;p>When a new ingester is added / removed to / from the ring, the ring topology changes and queriers will update the in-memory map.&lt;/p>
&lt;h4 id="scenario-per-tenant-shard-size-increases">Scenario: per-tenant shard size increases&lt;/h4>
&lt;p>Queriers periodically (every 1m) reload the limits config file. When a tenant shard size change is detected, the querier updates the in-memory map for the affected tenant.&lt;/p>
&lt;p>&lt;strong>Issue:&lt;/strong> some time series data may be missing in queries up to 1m.&lt;/p>
&lt;h4 id="edge-case-queriers-notice-the-ring-topology-change-before-distributors">Edge case: queriers notice the ring topology change before distributors&lt;/h4>
&lt;p>Consider the following scenario:&lt;/p>
&lt;ol>
&lt;li>Tenant A shard is composed by ingesters 1,2,3,4,5,6&lt;/li>
&lt;li>Tenant A is remote writing 1 single series and gets replicated to ingester 1,2,3&lt;/li>
&lt;li>The ring topology changes and tenant A shard is ingesters 1,2,3,7,8,9&lt;/li>
&lt;li>Querier notices the ring topology change and updates the in-memory map. Given tenant A series were only on ingester 1,2,3, the querier maps tenant A to ingester 1,2,3 (because of what received from ingesters via gRPC) and 7,8,9 (because of the current state of the ring)&lt;/li>
&lt;li>Distributor hasn’t updated the ring state yet&lt;/li>
&lt;li>Tenant A remote writes 1 &lt;strong>new&lt;/strong> series, which get replicated to 4,5,6&lt;/li>
&lt;li>Distributor updates the ring state&lt;/li>
&lt;li>&lt;strong>Race condition:&lt;/strong> querier will not know that ingesters 4,5,6 contains tenant A data until the next sync&lt;/li>
&lt;/ol>
&lt;h3 id="considered-alternative-2-streaming-updates-from-ingesters-to-queriers">Considered alternative #2: streaming updates from ingesters to queriers&lt;/h3>
&lt;p>&lt;em>This section describes an alternative approach.&lt;/em>&lt;/p>
&lt;h4 id="current-state">Current state&lt;/h4>
&lt;p>As of today, queriers discover ingesters via the ring:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Ingesters&lt;/strong> register (and update their heartbeat timestamp) to the ring and queriers watch the ring, keeping an in-memory copy of the latest ingesters ring state.&lt;/li>
&lt;li>&lt;strong>Queriers&lt;/strong> use the in-memory ring state to discover all ingesters that should be queried at query time.&lt;/li>
&lt;/ul>
&lt;h4 id="proposal-3">Proposal&lt;/h4>
&lt;p>The proposal is to expose a new gRPC endpoint on ingesters, which allows queriers to receive a stream of real time updates from ingesters about the tenants for which an ingester currently has time series data.&lt;/p>
&lt;p>From the querier side:&lt;/p>
&lt;ul>
&lt;li>At &lt;strong>startup&lt;/strong> the querier discovers all existing ingesters. For each ingester, the querier calls the ingester’s gRPC endpoint WatchTenants() (to be created). As soon as the WatchTenants() rpc is called, the ingester sends the entire set of tenants to the querier and then will send incremental updates (tenant added or removed from ingester) while the WatchTenants() stream connection is alive.&lt;/li>
&lt;li>If the querier &lt;strong>loses the connection&lt;/strong> to an ingester, it will automatically retry (with backoff) while the ingester is within the ring.&lt;/li>
&lt;li>The querier &lt;strong>watches the ring&lt;/strong> to discover added/removed ingesters. When an ingester is added, the querier adds the ingester to the pool of ingesters whose state should be monitored via WatchTenants().&lt;/li>
&lt;li>At &lt;strong>query time,&lt;/strong> the querier looks for all ingesters within the ring. There are two options:
&lt;ol>
&lt;li>The querier knows the state of the ingester: the ingester will be queried only if it contains data for the query’s tenant.&lt;/li>
&lt;li>The querier doesn’t know the state of the ingester (eg. because it was just registered to the ring and WatchTenants() hasn’t succeeded yet): the ingester will be queried anyway (correctness first).&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>The querier will fine tune &lt;a href="https://godoc.org/google.golang.org/grpc/keepalive">gRPC keepalive&lt;/a> settings to ensure a lost connection between the querier and ingester will be early detected and retried.&lt;/li>
&lt;/ul>
&lt;h4 id="trade-offs">Trade-offs&lt;/h4>
&lt;p>Pros:&lt;/p>
&lt;ul>
&lt;li>The querier logic, used to find ingesters for a tenant’s shard, &lt;strong>does not require to watch the overrides&lt;/strong> config file (containing tenant shard size override). Watching the file in the querier is problematic because of introduced delays (ConfigMap update and Cortex file polling) which could lead to distributors apply changes before queriers.&lt;/li>
&lt;li>The querier &lt;strong>never uses the current state of the ring&lt;/strong> as a source of information to detect which ingesters have data for a specific tenant. This information comes directly from the ingesters themselves, which makes the implementation less likely to be subject to race conditions.&lt;/li>
&lt;/ul>
&lt;p>Cons:&lt;/p>
&lt;ul>
&lt;li>Each querier needs to open a gRPC connection to each ingester. Given gRPC supports multiplexing, the underlying TCP connection could be the same connection used to fetch samples from ingesters at query time, basically having 1 single TCP connection between a querier and an ingester.&lt;/li>
&lt;li>The “Edge case: queriers notice the ring topology change before distributors” described in attempt #1 can still happen in case of delays in the propagation of the state update from an ingester to queriers:
&lt;ul>
&lt;li>Short delay: a short delay (few seconds) shouldn’t be a real problem. From the final user perspective, there’s no real difference between this edge case and a delay of few seconds in the ingestion path (eg. Prometheus remote write lagging behind few seconds). In the real case of Prometheus remote writing to Cortex, there’s no easy way to know if the latest samples are missing because has not been remote written yet by Prometheus or any delay in the propagation of this information between ingesters and queriers.&lt;/li>
&lt;li>Long delay: in case of networking issue propagating the state update from an ingester to the querier, the gRPC keepalive will trigger (because of failed ping-pong) and the querier will remove the failing ingesters in-memory data, so the ingester will be always tried by the querier for any query, until the state update will be re-established.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="ruler-sharding">Ruler sharding&lt;/h2>
&lt;h3 id="introduction-1">Introduction&lt;/h3>
&lt;p>The ruler currently supports rule groups sharding across a pool of rulers. When sharding is enabled, rulers form a hash ring and each ruler uses the ring to check if it should evaluate a specific rule group.&lt;/p>
&lt;p>At a polling interval (defaults to 1 minute), the ruler:&lt;/p>
&lt;ul>
&lt;li>List all the bucket objects to find all rule groups (listing is done specifying an empty delimiter so it return objects at any depth)&lt;/li>
&lt;li>For each discovered rule group, the ruler hashes the object key and checks if it belongs to the range of tokens assigned to the ruler itself. If not, the rule group is discarded, otherwise it’s kept for evaluation.&lt;/li>
&lt;/ul>
&lt;h3 id="proposal-4">Proposal&lt;/h3>
&lt;p>We propose to introduce shuffle sharding in the ruler as well, leveraging on the already existing hash ring used by the current sharding implementation.&lt;/p>
&lt;p>The &lt;strong>configuration&lt;/strong> will be extended to allow to configure:&lt;/p>
&lt;ul>
&lt;li>Enable/disable shuffle sharding&lt;/li>
&lt;li>Default shard size&lt;/li>
&lt;li>Per-tenant overrides (reloaded at runtime)&lt;/li>
&lt;/ul>
&lt;p>When shuffle sharding is enabled:&lt;/p>
&lt;ul>
&lt;li>The ruler lists (ListBucketV2) the tenants for which rule groups are stored in the bucket&lt;/li>
&lt;li>The ruler filters out tenants not belonging to its shard&lt;/li>
&lt;li>For each tenant belonging to its shard, the ruler does a ListBucketV2 call with the “&lt;tenant-id>/” prefix and with empty delimiter to find all the rule groups, which are then evaluated in the ruler&lt;/li>
&lt;/ul>
&lt;p>The ruler re-syncs the rule groups from the bucket whenever one of the following conditions happen:&lt;/p>
&lt;ol>
&lt;li>Periodic interval (configurable)&lt;/li>
&lt;li>Ring topology changes&lt;/li>
&lt;li>The configured shard size of a tenant has changed&lt;/li>
&lt;/ol>
&lt;h3 id="other-notes">Other notes&lt;/h3>
&lt;ul>
&lt;li>The “subring” implementation is unoptimized. We will optimize it as part of this work to make sure no performance degradation is introduced when using the subring vs the normal ring.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Support metadata API</title><link>/docs/proposals/support-metadata-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/support-metadata-api/</guid><description>
&lt;ul>
&lt;li>Author: @gotjosh&lt;/li>
&lt;li>Reviewers: @gouthamve, @pracucci&lt;/li>
&lt;li>Date: March 2020&lt;/li>
&lt;li>Status: Accepted&lt;/li>
&lt;/ul>
&lt;h2 id="problem-statement">Problem Statement&lt;/h2>
&lt;p>Prometheus holds metric metadata alongside the contents of a scrape. This metadata (&lt;code>HELP&lt;/code>, &lt;code>TYPE&lt;/code>, &lt;code>UNIT&lt;/code> and &lt;code>METRIC_NAME&lt;/code>) enables &lt;a href="https://github.com/prometheus/prometheus/issues/6395">some Prometheus API&lt;/a> endpoints to output the metadata for integrations (e.g. &lt;a href="https://github.com/grafana/grafana/pull/21124">Grafana&lt;/a>) to consume it.&lt;/p>
&lt;p>At the moment of writing, Cortex does not support the &lt;code>api/v1/metadata&lt;/code> endpoint that Prometheus implements as metadata was never propagated via remote write. Recent &lt;a href="https://github.com/prometheus/prometheus/pull/6815/files">work is done in Prometheus&lt;/a> enables the propagation of metadata.&lt;/p>
&lt;p>With this in place, remote write integrations such as Cortex can now receive this data and implement the API endpoint. This results in Cortex users being able to enjoy a tiny bit more insight on their metrics.&lt;/p>
&lt;h2 id="potential-solutions">Potential Solutions&lt;/h2>
&lt;p>Before we delve into the solutions, let&amp;rsquo;s set a baseline about how the data is received. This applies almost equally for the two.&lt;/p>
&lt;p>Metadata from Prometheus is sent in the same &lt;a href="https://github.com/prometheus/prometheus/blob/master/prompb/remote.proto">&lt;code>WriteRequest&lt;/code> proto message&lt;/a> that the samples use. It is part of a different field (#3 given #2 is already &lt;a href="https://github.com/cortexproject/cortex/blob/master/pkg/ingester/client/cortex.proto#L36">used interally&lt;/a>), the data is a set identified by the metric name - that means it is aggregated across targets, and is sent all at once. Implying, Cortex will receive a single &lt;code>WriteRequest&lt;/code> containing a set of the metadata for that instance at an specified interval.&lt;/p>
&lt;p>. It is also important to note that this current process is an intermediary step. Eventually, metadata in a request will be sent alongside samples and only for those included. The solutions proposed, take this nuance into account to avoid coupling between the current and future state of Prometheus, and hopefully do something now that also works for the future.&lt;/p>
&lt;p>As a reference, these are some key numbers regarding the size (and send timings) of the data at hand from our clusters at Grafana Labs:&lt;/p>
&lt;ul>
&lt;li>On average, metadata (a combination of &lt;code>HELP&lt;/code>, &lt;code>TYPE&lt;/code>, &lt;code>UNIT&lt;/code> and &lt;code>METRIC_NAME&lt;/code>) is ~55 bytes uncompressed.&lt;/li>
&lt;li>at GL, on an instance with about 2.6M active series, we hold ~1241 unique metrics in total.&lt;/li>
&lt;li>with that, we can assume that on a worst-case scenario the metadata set for that instance is ~68 kilobytes uncompressed.&lt;/li>
&lt;li>by default, this data is only propagated once every minute (aligning with the default scrape interval), but this can be adjusted.&lt;/li>
&lt;li>Finally, what this gives us is a baseline worst-case scenario formula for the data to store per tenant: &lt;code>~68KB * Replication Factor * # of Instances&lt;/code>. Keeping in mind that typically, there&amp;rsquo;s a very high overlap of metadata across instances, and we plan to deduplicate in the ingesters.&lt;/li>
&lt;/ul>
&lt;h3 id="write-path">Write Path&lt;/h3>
&lt;ol>
&lt;li>Store the metadata directly from the distributors into a cache (e.g. Memcached)&lt;/li>
&lt;/ol>
&lt;p>Since metadata is received all at once, we could directly store into an external cache using the tenant ID as a key, and still, avoid a read-modify-write. However, a very common use case of Cortex is to have multiple Prometheus sending data for the same tenant ID. This complicates things, as it adds a need to have an intermediary merging phase and thus making a read-modify-write inevitable.&lt;/p>
&lt;ol start="2">
&lt;li>Keep metadata in memory within the ingesters&lt;/li>
&lt;/ol>
&lt;p>Similarly to what we do with sample data, we can keep the metadata in-memory in the ingesters and apply similar semantics. I propose to use the tenant ID as a hash key, distribute it to the ingesters (taking into account the replication factor), using a hash map to keep a set of the metadata across all instances for a single tenant, and implement a configurable time-based purge process to deal with metadata churn. Given, we need to ensure fair-use we also propose implementing limits for both the number of metadata entries we can receive and the size of a single entry.&lt;/p>
&lt;h3 id="read-path">Read Path&lt;/h3>
&lt;p>In my eyes, the read path seems to only have one option. At the moment of writing, Cortex uses a &lt;a href="https://github.com/cortexproject/cortex/blob/master/pkg/querier/dummy.go#L11-L20">&lt;code>DummyTargetRetriever&lt;/code>&lt;/a> as a way to signal that these API endpoints are not implemented. We&amp;rsquo;d need to modify the Prometheus interface to support a &lt;code>Context&lt;/code> and extract the tenant ID from there. Then, use the tenant ID to query the ingesters for the data, deduplicate it and serve it.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>I conclude that solution #2 is ideal for this work on the write path. It allows us to use similar semantics to samples, thus reducing operational complexity, and lays a groundwork for when we start receiving metadata alongside samples.&lt;/p>
&lt;p>There&amp;rsquo;s one last piece to address: Allowing metadata to survive rolling restarts. Option #1 handles this well, given the aim would be to use an external cache such as Memcached. Option #2 lacks this, as it does not include any plans to persist this data. Given Prometheus (by default) sends metadata every minute, and we don&amp;rsquo;t need a high level of consistency. We expect that an eventual consistency of up to 1 minute on the default case is deemed acceptable.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://docs.google.com/document/d/1LoCWPAIIbGSq59NG3ZYyvkeNb8Ymz28PUKbg_yhAzvE/edit#">Prometheus Propagate metadata via Remote Write Design Doc&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prometheus/prometheus/issues/6395">Prometheus Propagate metadata via Remote Write Design Issue&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Time Series Deletion from Blocks Storage</title><link>/docs/proposals/block-storage-time-series-deletion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/block-storage-time-series-deletion/</guid><description>
&lt;ul>
&lt;li>Author: &lt;a href="https://github.com/ilangofman">Ilan Gofman&lt;/a>&lt;/li>
&lt;li>Date: June 2021&lt;/li>
&lt;li>Status: Proposal&lt;/li>
&lt;/ul>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Currently, Cortex only implements a time series deletion API for chunk storage. We present a design for implementing time series deletion with block storage. We would like to have the same API for deleting series as currently implemented in Prometheus and in Cortex with chunk storage.&lt;/p>
&lt;p>This can be very important for users to have as confidential or accidental data might have been incorrectly pushed and needs to be removed. As well as potentially removing high cardinality data that is causing inefficient queries.&lt;/p>
&lt;h2 id="related-works">Related works&lt;/h2>
&lt;p>As previously mentioned, the deletion feature is already implemented with chunk storage. The main functionality is implemented through the purger service. It accepts requests for deletion and processes them. At first, when a deletion request is made, a tombstone is created. This is used to filter out the data for queries. After some time, a deletion plan is executed where the data is permanently removed from chunk storage.&lt;/p>
&lt;p>Can find more info here:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://cortexmetrics.io/docs/guides/deleting-series/">Cortex documentation for chunk store deletion&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/document/d/1PeKwP3aGo3xVrR-2qJdoFdzTJxT8FcAbLm2ew_6UQyQ/edit">Chunk deletion proposal&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="background-on-current-storage">Background on current storage&lt;/h2>
&lt;p>With a block-storage configuration, Cortex stores data that could be potentially deleted by a user in:&lt;/p>
&lt;ul>
&lt;li>Object store (GCS, S3, etc..) for long term storage of blocks&lt;/li>
&lt;li>Ingesters for more recent data that should be eventually transferred to the object store&lt;/li>
&lt;li>Cache
&lt;ul>
&lt;li>Index cache&lt;/li>
&lt;li>Metadata cache&lt;/li>
&lt;li>Chunks cache (stores the potentially to be deleted data)&lt;/li>
&lt;li>Query results cache (stores the potentially to be deleted data)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Compactor during the compaction process&lt;/li>
&lt;li>Store-gateway&lt;/li>
&lt;/ul>
&lt;h2 id="proposal">Proposal&lt;/h2>
&lt;p>The deletion will not happen right away. Initially, the data will be filtered out from queries using tombstones and will be deleted afterward. This will allow the user some time to cancel the delete request.&lt;/p>
&lt;h3 id="api-endpoints">API Endpoints&lt;/h3>
&lt;p>The existing purger service will be used to process the incoming requests for deletion. The API will follow the same structure as the chunk storage endpoints for deletion, which is also based on the Prometheus deletion API.&lt;/p>
&lt;p>This will enable the following endpoints for Cortex when using block storage:&lt;/p>
&lt;p>&lt;code>POST /api/v1/admin/tsdb/delete_series&lt;/code> - Accepts &lt;a href="https://prometheus.io/docs/prometheus/latest/querying/api/#delete-series">Prometheus style delete request&lt;/a> for deleting series.&lt;/p>
&lt;p>Parameters:&lt;/p>
&lt;ul>
&lt;li>&lt;code>start=&amp;lt;rfc3339 | unix_timestamp&amp;gt;&lt;/code>
&lt;ul>
&lt;li>Optional. If not provided, will be set to minimum possible time.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>end=&amp;lt;rfc3339 | unix_timestamp&amp;gt; &lt;/code>
&lt;ul>
&lt;li>Optional. If not provided, will be set to maximum possible time (time when request was made). End time cannot be greater than the current UTC time.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>match[]=&amp;lt;series_selector&amp;gt;&lt;/code>
&lt;ul>
&lt;li>Cannot be empty, must contain at least one label matcher argument.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>POST /api/v1/admin/tsdb/cancel_delete_request&lt;/code> - To cancel a request if it has not been processed yet for permanent deletion. This can only be done before the &lt;code>-purger.delete-request-cancel-period&lt;/code> has passed.
Parameters:&lt;/p>
&lt;ul>
&lt;li>&lt;code>request_id&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;code>GET /api/v1/admin/tsdb/delete_series&lt;/code> - Get all delete requests id’s and their current status.&lt;/p>
&lt;p>Prometheus also implements a &lt;a href="https://prometheus.io/docs/prometheus/latest/querying/api/#clean-tombstones">clean_tombstones&lt;/a> API which is not included in this proposal. The tombstones will be deleted automatically once the permanent deletion has taken place which is described in the section below. By default, this should take approximately 24 hours.&lt;/p>
&lt;h3 id="deletion-lifecycle">Deletion Lifecycle&lt;/h3>
&lt;p>The deletion request lifecycle can follow these 3 states:&lt;/p>
&lt;ol>
&lt;li>Pending - Tombstone file is created. During this state, the queriers will be performing query time filtering. The initial time period configured by &lt;code>-purger.delete-request-cancel-period&lt;/code>, no data will be deleted. Once this period is over, permanent deletion processing will begin and the request is no longer cancellable.&lt;/li>
&lt;li>Processed - All requested data has been deleted. Initially, will still need to do query time filtering while waiting for the bucket index and store-gateway to pick up the new blocks. Once that period has passed, will no longer require any query time filtering.&lt;/li>
&lt;li>Deleted - The deletion request was cancelled. A grace period configured by &lt;code>-purger.delete-request-cancel-period&lt;/code> will allow the user some time to cancel the deletion request if it was made by mistake. The request is no longer cancelable after this period has passed.&lt;/li>
&lt;/ol>
&lt;h3 id="filtering-data-during-queries-while-not-yet-deleted">Filtering data during queries while not yet deleted:&lt;/h3>
&lt;p>Once a deletion request is received, a tombstone entry will be created. The object store such as S3, GCS, Azure storage, can be used to store all the deletion requests. See the section below for more detail on how the tombstones will be stored. Using the tombstones, the querier will be able to filter the to-be-deleted data initially. If a cancel delete request is made, then the tombstone file will be deleted. In addition, the existing cache will be invalidated using cache generation numbers, which are described in the later sections.&lt;/p>
&lt;p>The compactor&amp;rsquo;s &lt;em>BlocksCleaner&lt;/em> service will scan for new tombstone files and will update the bucket-index with the tombstone information regarding the deletion requests. This will enable the querier to periodically check the bucket index if there are any new tombstone files that are required to be used for filtering. One drawback of this approach is the time it could take to start filtering the data. Since the compactor will update the bucket index with the new tombstones every &lt;code>-compactor.cleanup-interval&lt;/code> (default 15 min). Then the cached bucket index is refreshed in the querier every &lt;code>-blocks-storage.bucket-store.sync-interval&lt;/code> (default 15 min). Potentially could take almost 30 min for queriers to start filtering deleted data when using the default values. If the information requested for deletion is confidential/classified, the time delay is something that the user should be aware of, in addition to the time that the data has already been in Cortex.&lt;/p>
&lt;p>An additional thing to consider is that this would mean that the bucket-index would have to be enabled for this API to work. Since the plan is to make to the bucket-index mandatory in the future for block storage, this shouldn&amp;rsquo;t be an issue.&lt;/p>
&lt;p>Similar to the chunk storage deletion implementation, the initial filtering of the deleted data will be done inside the Querier. This will allow filtering the data read from both the store gateway and the ingester. This functionality already exists for the chunk storage implementation. By implementing it in the querier, this would mean that the ruler will be supported too (ruler internally runs the querier).&lt;/p>
&lt;h4 id="storing-tombstones-in-object-store">Storing tombstones in object store&lt;/h4>
&lt;p>The Purger will write the new tombstone entries in a separate folder called &lt;code>tombstones&lt;/code> in the object store (e.g. S3 bucket) in the respective tenant folder. Each tombstone can have a separate JSON file outlining all the necessary information about the deletion request such as the parameters passed in the request, as well as some meta-data such as the creation date of the file. The name of the file can be a hash of the API parameters (start, end, markers). This way if a user calls the API twice by accident with the same parameters, it will only create one tombstone. To keep track of the request state, filename extensions can be used. This will allow the tombstone files to be immutable. The 3 different file extensions will be &lt;code>pending, processed, deleted&lt;/code>. Each time the deletion request moves to a new state, a new file will be added with the same deletion information but a different extension to indicate the new state. The file containing the previous state will be deleted once the new one is created. If a deletion request is cancelled, then a tombstone file with the &lt;code>.deleted&lt;/code> filename extension will be created.&lt;/p>
&lt;p>When it is determined that the request should move to the next state, then it will first write a new file containing the tombstone information to the object store. The information inside the file will be the same except the &lt;code>stateCreationTime&lt;/code>, which is replaced with the current timestamp. The extension of the new file will be different to reflect the new state. If the new file is successfully written, the file with the previous state is deleted. If the write of the new file fails, then the previous file is not going to be deleted. Next time the service runs to check the state of each tombstone, it will retry creating the new file with the updated state. If the write is successful but the deletion of the old file is unsuccessful then there will be 2 tombstone files with the same filename but different extension. When &lt;code>BlocksCleaner&lt;/code> writes the tombstones to the bucket index, the compactor will check for duplicate tombstone files but with different extensions. It will use the tombstone with the most recently updated state and try to delete the file with the older state. There could be a scenario where there are two files with the same request ID but different extensions: {&lt;code>.pending&lt;/code>, &lt;code>.processed&lt;/code>} or {&lt;code>.pending&lt;/code>, &lt;code>.deleted&lt;/code>}. In this case, the &lt;code>.processed&lt;/code> or &lt;code>.deleted &lt;/code> file will be selected as it is always the later state compared to the &lt;code>pending&lt;/code> state.&lt;/p>
&lt;p>The tombstone will be stored in a single JSON file per request and state:&lt;/p>
&lt;ul>
&lt;li>&lt;code>/&amp;lt;tenantId&amp;gt;/tombstones/&amp;lt;request_id&amp;gt;.json.&amp;lt;state&amp;gt;&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>The schema of the JSON file is:&lt;/p>
&lt;pre tabindex="0">&lt;code>{
&amp;#34;requestId&amp;#34;: &amp;lt;string&amp;gt;,
&amp;#34;startTime&amp;#34;: &amp;lt;int&amp;gt;,
&amp;#34;endTime&amp;#34;: &amp;lt;int&amp;gt;,
&amp;#34;requestCreationTime&amp;#34;: &amp;lt;int&amp;gt;,
&amp;#34;stateCreationTime&amp;#34;: &amp;lt;int&amp;gt;,
&amp;#34;matchers&amp;#34;: [
&amp;#34;&amp;lt;string matcher 1&amp;gt;&amp;#34;,
..,
&amp;#34;&amp;lt;string matcher n&amp;gt;&amp;#34;
]
},
&amp;#34;userID&amp;#34;: &amp;lt;string&amp;gt;,
}
&lt;/code>&lt;/pre>&lt;p>Pros:&lt;/p>
&lt;ul>
&lt;li>Allows deletion and un-delete to be done in a single operation.&lt;/li>
&lt;/ul>
&lt;p>Cons:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Negative impact on query performance when there are active tombstones. As in the chunk storage implementation, all the series will have to be compared to the matchers contained in the active tombstone files. The impact on performance should be the same as the deletion would have with chunk storage.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>With the default config, potential 30 minute wait for the data to begin filtering if using the default configuration.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="invalidating-cache">Invalidating cache&lt;/h4>
&lt;p>Using block store, the different caches available are:&lt;/p>
&lt;ul>
&lt;li>Index cache&lt;/li>
&lt;li>Metadata cache&lt;/li>
&lt;li>Chunks cache (stores the potentially to be deleted chunks of data)&lt;/li>
&lt;li>Query results cache (stores the potentially to be deleted data)&lt;/li>
&lt;/ul>
&lt;p>There are two potential caches that could contain deleted data, the chunks cache, and the query results cache. Using the tombstones, the queriers filter out the data received from the ingesters and store-gateway. The cache not being processed through the querier needs to be invalidated to prevent deleted data from coming up in queries.&lt;/p>
&lt;p>Firstly, the query results cache needs to be invalidated for each new delete request or a cancellation of one. This can be accomplished by utilizing cache generation numbers. For each tenant, their cache is prefixed with a cache generation number. When the query front-end discovers a cache generation number that is greater than the previous generation number, then it knows to invalidate the query results cache. However, the cache can only be invalidated once the queriers have loaded the tombstones from the bucket index and have begun filtering the data. Otherwise, to-be deleted data might show up in queries and be cached again. One of the way to guarantee that all the queriers are using the new tombstones is to wait until the bucket index staleness period has passed from the time the tombstones have been written to the bucket index. The staleness period can be configured using the following flag: &lt;code>-blocks-storage.bucket-store.bucket-index.max-stale-period&lt;/code>. We can use the bucket index staleness period as the delay to wait before the cache generation number is increased. A query will fail inside the querier, if the bucket index last update is older the staleness period. Once this period is over, all the queriers should have the updated tombstones and the query results cache can be invalidated. Here is the proposed method for accomplishing this:&lt;/p>
&lt;ul>
&lt;li>The cache generation number will be a timestamp. The default value will be 0.&lt;/li>
&lt;li>The bucket index will store the cache generation number. The query front-end will periodically fetch the bucket index.&lt;/li>
&lt;li>Inside the compactor, the &lt;em>BlocksCleaner&lt;/em> will load the tombstones from object store and update the bucket index accordingly. It will calculate the cache generation number by iterating through all the tombstones and their respective times (next bullet point) and selecting the maximum timestamp that is less than (current time minus &lt;code>-blocks-storage.bucket-store.bucket-index.max-stale-period&lt;/code>). This would mean that if a deletion request is made or cancelled, the compactor will only update the cache generation number once the staleness period is over, ensuring that all queriers have the updated tombstones.&lt;/li>
&lt;li>For requests in a pending or processed state, the &lt;code>requestCreationTime&lt;/code> will be used when comparing the maximum timestamps. If a request is in a deleted state, it will use the &lt;code>stateCreationTime&lt;/code> for comparing the timestamps. This means that the cache gets invalidated only once it has been created or deleted, and the bucket index staleness period has passed. The cache will not be invalidated again when a request advances from pending to processed state.&lt;/li>
&lt;li>The query front-end will fetch the cache generation number from the bucket index. The query front end will compare it to the current cache generation number stored in the front-end. If the cache generation number from the front-end is less than the one from bucket index, then the cache is invalidated.&lt;/li>
&lt;/ul>
&lt;p>In regards to the chunks cache, since it is retrieved from the store gateway and passed to the querier, it will be filtered out like the rest of the time series data in the querier using the tombstones, with the mechanism described in the previous section.&lt;/p>
&lt;h3 id="permanently-deleting-the-data">Permanently deleting the data&lt;/h3>
&lt;p>The proposed approach is to perform the deletions from the compactor. A new background service inside the compactor called &lt;em>DeletedSeriesCleaner&lt;/em> can be created and is responsible for executing the deletion.&lt;/p>
&lt;h4 id="processing">Processing&lt;/h4>
&lt;p>This will happen after a grace period has passed once the API request has been made. By default this should be 24 hours. A background task can be created to process the permanent deletion of time series. This background task can be executed each hour.&lt;/p>
&lt;p>To delete the data from the blocks, the same logic as the &lt;a href="https://thanos.io/tip/components/tools.md/#bucket-rewrite">Bucket Rewrite Tool&lt;/a> from Thanos can be leveraged. This tool does the following: &lt;code>tools bucket rewrite rewrites chosen blocks in the bucket, while deleting or modifying series&lt;/code>. The tool itself is a CLI tool that we won’t be using, but instead we can utilize the logic inside it. For more information about the way this tool runs, please see the code &lt;a href="https://github.com/thanos-io/thanos/blob/d8b21e708bee6d19f46ca32b158b0509ca9b7fed/cmd/thanos/tools_bucket.go#L809">here&lt;/a>.&lt;/p>
&lt;p>The compactor’s &lt;em>DeletedSeriesCleaner&lt;/em> will apply this logic on individual blocks and each time it is run, it creates a new block without the data that matched the deletion request. The original individual blocks containing the data that was requested to be deleted, need to be marked for deletion by the compactor.&lt;/p>
&lt;p>While deleting the data permanently from the block storage, the &lt;code>meta.json&lt;/code> files will be used to keep track of the deletion progress. Inside each &lt;code>meta.json&lt;/code> file, we will add a new field called &lt;code>tombstonesFiltered&lt;/code>. This will store an array of deletion request id&amp;rsquo;s that were used to create this block. Once the rewrite logic is applied to a block, the new block&amp;rsquo;s &lt;code>meta.json&lt;/code> file will append the deletion request id(s) used for the rewrite operation inside this field. This will let the &lt;em>DeletedSeriesCleaner&lt;/em> know that this block has already processed the particular deletions requests listed in this field. Assuming that the deletion requests are quite rare, the size of the meta.json files should remain small.&lt;/p>
&lt;p>The &lt;em>DeletedSeriesCleaner&lt;/em> can iterate through all the blocks that the deletion request could apply to. For each of these blocks, if the deletion request ID isn&amp;rsquo;t inside the meta.json &lt;code>tombstonesFiltered&lt;/code> field, then the compactor can apply the rewrite logic to this block. If there are multiple tombstones that are currently being processing for deletions and apply to a particular block, then the &lt;em>DeletedSeriesCleaner&lt;/em> will process both at the same time to prevent additional blocks from being created. If after iterating through all the blocks, it doesn’t find any such blocks requiring deletion, then the &lt;code>Pending&lt;/code> state is complete and the request progresses to the &lt;code>Processed&lt;/code> state.&lt;/p>
&lt;p>One important thing to note regarding this rewrite tool is that it should not be used at the same time as when another compactor is touching a block. If the tool is run at the same time as compaction on a particular block, it can cause overlap and the data marked for deletion can already be part of the compacted block. To mitigate such issues, these are some of the proposed solutions:&lt;/p>
&lt;p>Option 1: Only apply the deletion once the blocks are in the final state of compaction.&lt;/p>
&lt;p>Pros:&lt;/p>
&lt;ul>
&lt;li>Simpler implementation as everything is contained within the DeletedSeriesCleaner.&lt;/li>
&lt;/ul>
&lt;p>Cons:&lt;/p>
&lt;ul>
&lt;li>Might have to wait for a longer period of time for the compaction to be finished.
&lt;ul>
&lt;li>This would mean the earliest time to be able to run the deletion would be once the last time from the block_ranges in the &lt;a href="https://cortexmetrics.io/docs/blocks-storage/compactor/#compactor-configuration">compactor_config&lt;/a> has passed. By default this value is 24 hours, so only once 24 hours have passed and the new compacted blocks have been created, then the rewrite can be safely run.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Option 2: For blocks that still need to be compacted further after the deletion request cancel period is over, the deletion logic can be applied before the blocks are compacted. This will generate a new block which can then be used instead for compaction with other blocks.&lt;/p>
&lt;p>Pros:&lt;/p>
&lt;ul>
&lt;li>The deletion can be applied earlier than the previous options.
&lt;ul>
&lt;li>Only applies if the deletion request cancel period is less than the last time interval for compaction is.
Cons:&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added coupling between the compaction and the DeletedSeriesCleaner.&lt;/li>
&lt;li>Might block compaction for a short time while doing the deletion.&lt;/li>
&lt;/ul>
&lt;p>Once all the applicable blocks have been rewritten without the deleted data, the deletion request state moves to the &lt;code>Processed&lt;/code> state. Once in this state, the queriers will still have to perform query time filtering using the tombstones until the old blocks that were marked for deletion are no longer queried by the queriers. This will mean that the query time filtering will last for an additional length of &lt;code>-compactor.deletion-delay + -compactor.cleanup-interval + -blocks-storage.bucket-store.sync-interval&lt;/code> in the &lt;code>Processed&lt;/code> state. Once that time period has passed, the queriers should no longer be querying any of the old blocks that were marked for deletion. The tombstone will no longer be used after this.&lt;/p>
&lt;h4 id="cancelled-delete-requests">Cancelled Delete Requests&lt;/h4>
&lt;p>If a request was successfully cancelled, then a tombstone file a &lt;code>.deleted&lt;/code> extension is created. This is done to help ensure that the cache generation number is updated and the query results cache is invalidated. The compactor&amp;rsquo;s blocks cleaner can take care of cleaning up &lt;code>.deleted&lt;/code> tombstones after a period of time of when they are no longer required for cache invalidation. This can be done after 10 times the bucket index max staleness time period has passed. Before removing the file from the object store, the current cache generation number must greater than or equal to when the tombstone was cancelled.&lt;/p>
&lt;h4 id="handling-failedunfinished-delete-jobs">Handling failed/unfinished delete jobs:&lt;/h4>
&lt;p>Deletions will be completed and the tombstones will be deleted only when the DeletedSeriesCleaner iterates over all blocks that match the time interval and confirms that they have been re-written without the deleted data. Otherwise, it will keep iterating over the blocks and process the blocks that haven&amp;rsquo;t been rewritten according to the information in the &lt;code>meta.json&lt;/code> file. In case of any failure that causes the deletion to stop, any unfinished deletions will be resumed once the service is restarted. If the block rewrite was not completed on a particular block, then the original block will not be marked for deletion. The compactor will continue to iterate over the blocks and process the block again.&lt;/p>
&lt;h4 id="tenant-deletion-api">Tenant Deletion API&lt;/h4>
&lt;p>If a request is made to delete a tenant, then all the tombstones will be deleted for that user.&lt;/p>
&lt;h2 id="current-open-questions">Current Open Questions:&lt;/h2>
&lt;ul>
&lt;li>If the start and end time is very far apart, it might result in a lot of the data being re-written. Since we create a new block without the deleted data and mark the old one for deletion, there may be a period of time with lots of extra blocks and space used for large deletion queries.&lt;/li>
&lt;li>There will be a delay between the deletion request and the deleted data being filtered during queries.
&lt;ul>
&lt;li>In Prometheus, there is no delay.&lt;/li>
&lt;li>One way to filter out immediately is to load the tombstones during query time but this will cause a negative performance impact.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Adding limits to the API such as:
&lt;ul>
&lt;li>Max number of deletion requests allowed in the last 24 hours for a given tenent.&lt;/li>
&lt;li>Max number of pending tombstones for a given tenant.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="alternatives-considered">Alternatives Considered&lt;/h2>
&lt;h4 id="adding-a-pre-processing-state">Adding a Pre-processing State&lt;/h4>
&lt;p>The process of permanently deleting the data can be separated into 2 stages, preprocessing and processing.&lt;/p>
&lt;p>Pre-processing will begin after the &lt;code>-purger.delete-request-cancel-period&lt;/code> has passed since the API request has been made. The deletion request will move to a new state called &lt;code>BuildingPlan&lt;/code>. The compactor will outline all the blocks that may contain data to be deleted. For each separate block that the deletion may be applicable to, the compactor will begin the process by adding a series deletion marker inside the series-deletion-marker.json file. The JSON file will contain an array of deletion request id&amp;rsquo;s that need to be applied to the block, which allows the ability to handle the situation when there are multiple tombstones that could be applicable to a particular block. Then during the processing step, instead of checking the meta.json file, we only need to check if a marker file exists with a specific deletion request id. If the marker file exists, then we apply the rewrite logic.&lt;/p>
&lt;h4 id="alternative-permanent-deletion-processing">Alternative Permanent Deletion Processing&lt;/h4>
&lt;p>For processing the actual deletions, an alternative approach is not to wait until the final compaction has been completed and filter out the data during compaction. If the data is marked to be deleted, then don’t include it the new bigger block during compaction. For the remaining blocks where the data wasn’t filtered during compaction, the deletion can be done the same as in the previous section.&lt;/p>
&lt;p>Pros:&lt;/p>
&lt;ul>
&lt;li>The deletion can happen sooner.&lt;/li>
&lt;li>The rewrite tools creates additional blocks. By filtering the metrics during compaction, the intermediary re-written block will be avoided.&lt;/li>
&lt;/ul>
&lt;p>Cons:&lt;/p>
&lt;ul>
&lt;li>A more complicated implementation requiring add more logic to the compactor&lt;/li>
&lt;li>Slower compaction if it needs to filter all the data&lt;/li>
&lt;li>Need to manage which blocks should be deleted with the rewrite vs which blocks already had data filtered during compaction.&lt;/li>
&lt;li>Would need to run the rewrite logic during and outside of compaction because some blocks that might need to be deleted are already in the final compaction state. So that would mean the deletion functionality has to be implemented in multiple places.&lt;/li>
&lt;li>Won’t be leveraging the rewrites tools from Thanos for all the deletion, so potentially more work is duplicated&lt;/li>
&lt;/ul></description></item><item><title>Docs: Timeseries Partitioning in Compactor</title><link>/docs/proposals/timeseries-partitioning-in-compactor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/proposals/timeseries-partitioning-in-compactor/</guid><description>
&lt;ul>
&lt;li>Author: @roystchiang&lt;/li>
&lt;li>Reviewers:&lt;/li>
&lt;li>Date: August 2022&lt;/li>
&lt;li>Status: Proposed&lt;/li>
&lt;/ul>
&lt;h2 id="timeseries-partitioning-in-compactor">Timeseries Partitioning in Compactor&lt;/h2>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>The compactor is a crucial component in Cortex responsible for deduplication of replicated data, and merging blocks across multiple time intervals together. This proposal will not go into great depth with why the compactor is necessary, but aims to focus on how to scale the compactor as a tenant grows within a Cortex cluster.&lt;/p>
&lt;h2 id="problem-and-requirements">Problem and Requirements&lt;/h2>
&lt;p>Cortex introduced horizontally scaling compactor which allows multiple compactors to compact blocks for a single tenant, sharded by time interval. The compactor is capable of compacting multiple smaller blocks into a larger block, to reduce the duplicated information in index. The following is an illustration of how the shuffle sharding compactor works, where each arrow represents a single compaction that can be carried out independently.
&lt;img src="/images/proposals/parallel-compaction-grouping.png" alt="Current Implementation">&lt;/p>
&lt;p>However, if the tenant is sending unique timeseries, the compaction process does not help with reducing the index size. Furthermore, this scaling of parallelism by time interval is not sufficient for a tenant with hundreds of millions of timeseries, as more timeseries means longer compaction time.&lt;/p>
&lt;p>Currently, the compactor is bounded by the 64GB index size, and having a compaction that takes days to complete simply is not sustainable. This time includes the time to download the blocks, merging of the timeseries, writing to disk, and finally uploading to object storage.&lt;/p>
&lt;p>The compactor is able to compact up to 400M timeseries within 12 hours, and will fail with the error of index exceeding 64GB. Depending on the number of labels and the size of labels, one might reach the 64GB limit sooner. We need a solution that is capable of:&lt;/p>
&lt;ul>
&lt;li>handling the 64GB index limit&lt;/li>
&lt;li>reducing the overall compaction time
&lt;ul>
&lt;li>downloading the data in smaller batches&lt;/li>
&lt;li>reducing the time required to compact&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="design">Design&lt;/h2>
&lt;p>A reminder of what a Prometheus TSDB is composed of: an index and chunks. An index is a mapping of timeseries to the chunks, so we can do a direct lookup in the chunks. Each timeseries is effectively a set of labels, mapped to a list of &amp;lt;timestamp, value&amp;gt; pair. This proposal focuses on partitioning of the timeseries.&lt;/p>
&lt;h3 id="partitioning-strategy">Partitioning strategy&lt;/h3>
&lt;p>The compactor will compact a overlapping time-range into multiple sub-blocks, instead of a single block. Cortex can determine which partition a single timeseries should go into by applying a hash to the timeseries label, and taking the modulo of the hash by the number of partition. This guarantees that with same number of partition, the same timeseries will go into the same partition.&lt;/p>
&lt;p>&lt;code>partitionId = Hash(timeseries label) % number of partition&lt;/code>&lt;/p>
&lt;p>The number of partition will be determined automatically, via a configured &lt;code>multiplier&lt;/code>. This &lt;code>multiplier&lt;/code> factor allows us to group just a subset of the blocks together to achieve the same deduplication factor as having all the blocks. Using a &lt;code>multiplier&lt;/code> of 2 as an example, we can do grouping for partition of 2, 4 and 8. We’ll build on the actual number of partition determination in a later section.&lt;/p>
&lt;h3 id="determining-overlapping-blocks">Determining overlapping blocks&lt;/h3>
&lt;p>In order to reduce the amount of time spent downloading blocks, and iterating through the index to filter out unrelated timeseries, we can do smart grouping of the blocks.&lt;/p>
&lt;p>&lt;img src="/images/proposals/timeseries-partitioning-in-compactor-modulo-partition.png" alt="Modulo Partitioning">&lt;/p>
&lt;p>Given that we are always multiplying the number of partition by the &lt;code>multiplier&lt;/code> factor, we can deduce from the modulo which partition could contain overlapping result&lt;/p>
&lt;pre tabindex="0">&lt;code>Given a hash N, if N % 8 == 7, then N % 4 must be 3
Given a hash N, if N % 8 == 3, then N % 4 must be 3
Given a hash N, if N % 8 == 4, then N % 4 must be 0
Given a hash N, if N % 8 == 0, then N % 4 must be 0
&lt;/code>&lt;/pre>&lt;p>Hence it is safe to group blocks with &lt;code>N % 8 == 7&lt;/code> with &lt;code>N % 8 == 3 &lt;/code>together with &lt;code>N % 4 == 3&lt;/code> together, and we are sure that other blocks won’t contain the same timeseries. We also know that if &lt;code>N % 8 == 0&lt;/code>, then we don’t need to download blocks where &lt;code>N % 4 == 1&lt;/code> or &lt;code>N % 4 == 2 &lt;/code>
Given partition count and partition id, we can immediately find out which blocks are required. Using the above modulo example, we get the following partitiong mapping.&lt;/p>
&lt;p>&lt;img src="/images/proposals/timeseries-partitioning-in-compactor-partitions.png" alt="Partition">&lt;/p>
&lt;h3 id="planning-the-compaction">Planning the compaction&lt;/h3>
&lt;p>The shuffle sharding compactor introduced additional logic to group blocks by distinct time intervals. It can also sum up the sizes of all indices to determine how many shards are required in total. Using the above example again, and assuming that each block has an index of 30GB, the sum is 30GB * 14 = 420GB, which needs to be at least 7, since maximum index size is 64GB. Using the &lt;code>multiplier&lt;/code> factor, it will be rounded up to 8.&lt;/p>
&lt;p>Now the planner knows the resulting compaction will have 8 partitions, it can start planning out which groups of blocks can go into a single compaction group. Given that we need 8 partitions in total, the planner will go through the process above to find out what blocks are necessary. Using the above example again, but we have distinct time intervals, T1, T2, and T3. T1 has 2 partitions, T2 has 4 partitions, and T3 has 8 partitions, and we want to produce T1-T3 blocks
&lt;img src="/images/proposals/timeseries-partitioning-in-compactor-grouping.png" alt="Grouping">&lt;/p>
&lt;pre tabindex="0">&lt;code>Compaction Group 1-8
T1 - Partition 1-2
T2 - Partition 1-4
T3 - Partition 1-8
Compaction Group 2-8
T1 - Partition 2-2
T2 - Partition 2-4
T3 - Partition 2-8
Compaction Group 3-8
T1 - Partition 1-2
T2 - Parittion 3-4
T3 - Partition 3-8
Compaction Group 4-8
T1 - Partition 2-2
T2 - Partition 4-4
T3 - Partition 4-8
Compaction Group 5-8
T1 - Partition 1-2
T2 - Partition 1-4
T3 - Partition 5-8
Compaction Group 6-8
T1 - Partition 2-2
T2 - Partition 2-4
T3 - Partition 6-8
Compaction Group 7-8
T1 - Partition 1-2
T2 - Partition 3-4
T3 - Partition 7-8
Compaction Group 8-8
T1 - Partition 2-2
T2 - Partition 4-4
T3 - Partition 8-8
&lt;/code>&lt;/pre>&lt;p>&lt;code>T1 - Partition 1-2&lt;/code> is used in multiple compaction groups, and the following section will describe how the compaction avoids duplicate timeseries in the resulting blocks&lt;/p>
&lt;h3 id="compaction">Compaction&lt;/h3>
&lt;p>Now that the planner has produced a compaction plan for the T1-T3 compaction groups, the compactor can start downloading the necessary blocks. Using compaction group 1-8 from above as example.
&lt;img src="/images/proposals/timeseries-partitioning-in-compactor-compact.png" alt="Grouping">
T1 - Partition 1-2 was created with hash % 2 == 0, and in order to avoid having duplication information in blocks produced by compaction group 3-8, compaction group 5-8, and compaction group 7-8, we need apply the filter the &lt;code>%8 == 0&lt;/code> hash, as that’s the hash of the highest partition count.&lt;/p>
&lt;h3 id="compaction-workflow">Compaction Workflow&lt;/h3>
&lt;ol>
&lt;li>Compactor initializes Grouper and Planner.&lt;/li>
&lt;li>Compactor retrieves block&amp;rsquo;s meta.json and call Grouper to group blocks for compaction.&lt;/li>
&lt;li>Grouper generates partitioned compaction groups:
&lt;ol>
&lt;li>Grouper groups source blocks into unpartitioned groups.&lt;/li>
&lt;li>For each unpartitioned group:
&lt;ol>
&lt;li>Generates partitioned compaction group ID which is hash of min and max time of result block.&lt;/li>
&lt;li>If the ID exists under the tenant directory in block storage, continue on next unpartitioned group.&lt;/li>
&lt;li>Calculates number of partitions. Number of partitions indicates how many partitions one unpartitioned group would be partitioned into based on the total size of indices and number of time series from each source blocks in the unpartitioned group.&lt;/li>
&lt;li>Assign source blocks into each partition with partition ID (value is in range from 0 to number_of_partitions - 1). Note that one source block could be used in multiple partitions (explanation in &lt;a href="#planning-the-compaction">Planning the compaction&lt;/a> and &lt;a href="#compaction">Compaction&lt;/a>). So multiple partition ID could be assigned to same source block. Check more partitioning examples in &lt;a href="#compaction-partitioning-examples">Compaction Partitioning Examples&lt;/a>&lt;/li>
&lt;li>Generates partitioned compaction group that indicates which partition ID each blocks got assigned.&lt;/li>
&lt;li>Partitioned compaction group information would be stored in block storage under the tenant directory it belongs to and the stored file can be picked up by cleaner later. Partitioned compaction group information contains partitioned compaction group ID, number of partitions, list of partitions which has partition ID and list of source blocks.&lt;/li>
&lt;li>Store partitioned compaction group ID in block storage under each blocks&amp;rsquo; directory that are used by the generated partitioned compaction group.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Grouper returns partitioned compaction groups to Compactor. Each returned group would have partition ID, number of partitions, and list of source blocks in memory.&lt;/li>
&lt;li>Compactor iterates over each partitioned compaction group. For each iteration, calls Planner to make sure the group is ready for compaction.&lt;/li>
&lt;li>Planner collects partitioned compaction group which is ready for compaction.
&lt;ol>
&lt;li>For each partitions in the group and for each blocks in the partition:
&lt;ol>
&lt;li>Make sure all source blocks fit within the time range of the group.&lt;/li>
&lt;li>Make sure each source block with assigned partition IDs is currently not used by another ongoing compaction. This could utilize visit marker file that is introduced in #4805 by expanding it for each partition ID of the source block.&lt;/li>
&lt;li>If all blocks in the partition are ready to be compacted,
&lt;ol>
&lt;li>mark status of those blocks with assigned partition ID as &lt;code>pending&lt;/code>.&lt;/li>
&lt;li>The status information of each partition ID would be stored in block storage under the corresponding block directory in order for cleaner to pick it up later.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>If not all blocks in the partition are ready, continue on next partition&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Return all ready partitions to Compactor.&lt;/li>
&lt;li>Compactor starts compacting partitioned blocks. Once compaction completed, Compactor would mark status of all blocks along with assigned partition ID in the group as &lt;code>completed&lt;/code>. Compactor should use partitioned compaction group ID to retrieve partitioned compaction group information from block storage to get all partition IDs assigned to each block. Then, retrieve status information of each partition ID this assigned to block under current block directory in block storage. If all assigned partition ID of the block have status set to &lt;code>completed&lt;/code>, upload deletion marker for this block. Otherwise, no deletion marker would be uploaded.&lt;/li>
&lt;/ol>
&lt;h3 id="clean-up-workflow">Clean up Workflow&lt;/h3>
&lt;p>Cleaner would periodically check any tenants having deletion marker. If there is a deletion marker for the tenant, Cleaner should remove all blocks and then clean up other files including partitioned group information files after tenant clean up delay. If there is no deletion marker for tenant, Clean should scan any source blocks having a deletion marker. If there is a deletion marker for the block, Cleaner should delete it.&lt;/p>
&lt;h2 id="performance">Performance&lt;/h2>
&lt;p>Currently a 400M timeseries takes 12 hours to compact, without taking block download into consideration. If we have a partition count of 2, we can reduce this down to 6 hours, and a partition count of 10 is 3 hours. The scaling is not linear, and I’m still attempting to find out why. The initial result is promising enough to continue though.&lt;/p>
&lt;h2 id="alternatives-considered">Alternatives Considered&lt;/h2>
&lt;h3 id="dynamic-number-of-partition">Dynamic Number of Partition&lt;/h3>
&lt;p>We can also increase/decrease the number of partition without needing the &lt;code>multiplier&lt;/code> factor. However, if a tenant is sending highly varying number of timeseries or label size, the index size can be very different, resulting in highly dynamic number of partitions. To perform deduplication, we’ll end up having to download all the sub-blocks, and it can be inefficient as less parallelization can be done, and we will spend more time downloading all the unnecessary blocks.&lt;/p>
&lt;h3 id="consistent-hashing">Consistent Hashing&lt;/h3>
&lt;p>Jump consistent hash, rendezvous hashing, and other consistent hashing are great algorithms to avoid
reshuffling of data when introducing/removing partitions on the fly. However, it does not bring much of a benefit when determining which partition contains the same timeseries, which we need to deduplication of index.&lt;/p>
&lt;h3 id="partition-by-metric-name">Partition by metric name&lt;/h3>
&lt;p>It is likely that when a query comes, a tenant is interested in an aggregation of a single metric, across all label names. The compactor can partition by metric name, so that all timeseries with the same name will go into the same block. However, this can result in very uneven partitioning.&lt;/p>
&lt;h2 id="architecture">Architecture&lt;/h2>
&lt;h3 id="planning">Planning&lt;/h3>
&lt;p>The shuffle partitioning compactor introduced a planner logic, which we can extend on. This planner is responsible for grouping the blocks together by time interval, in order to compact blocks in parallel. The grouper can also determine the number of partition by looking at the sum of index file sizes. In addition, it can also do the grouping of the sub-blocks together, so we can achieve even higher parallelism.&lt;/p>
&lt;h3 id="clean-up">Clean up&lt;/h3>
&lt;p>Cortex compactor cleans up obsolete source blocks by looking at a deletion marker. The current architecture does not have the problem of having a single source block involved in multiple compaction. However, this proposal is able to achieve higher parallelism than before, hence it is possible that a source block is involved multiple times. Changes needs to be made on the compactor regarding how many plans a particular blocked is involved in, and determining when a block is safe to be deleted.&lt;/p>
&lt;h2 id="changes-required-in-dependencies">Changes Required in Dependencies&lt;/h2>
&lt;h3 id="partitioning-during-compaction-time">Partitioning during compaction time&lt;/h3>
&lt;p>Prometheus exposes the possibility to pass in a custom &lt;a href="https://github.com/prometheus/prometheus/blob/a1fcfe62dbe82c6292214f50ee91337566b0d61b/tsdb/compact.go#L148">mergeFunc&lt;/a>. This allows us to plug in the custom partitioning strategy. However, the mergeFunc is only called when the timeseries is successfully replicated to at least 3 replicas, meaning that we’ll produce duplicate timeseries across blocks if the data is only replicated once. To work around the issue, we can propose Prometheus to allow the configuration of the &lt;a href="https://github.com/prometheus/prometheus/blob/a1fcfe62dbe82c6292214f50ee91337566b0d61b/tsdb/compact.go#L757">MergeChunkSeriesSet&lt;/a>.&lt;/p>
&lt;h3 id="source-block-checking">Source block checking&lt;/h3>
&lt;p>Cortex uses Thanos’s compactor logic, and it has a check to make sure the source blocks of the input blocks do not overlap. Meaning that if BlockA is produced from BlockY, and BlockB is also produced from BlockY, it will halt. This is not desirable for us, since partitioning by timeseries means the same source blocks will produce multiple blocks. Reason for having this check in Thanos is supposed to prevent having duplicate chunks, but the change was introduced without knowing whether it will actually help. We’ll need to introduce a change in Thanos to disable this check, or start using Thanos compactor as a library instead of a closed box.&lt;/p>
&lt;h2 id="work-plan">Work Plan&lt;/h2>
&lt;ul>
&lt;li>Performance test the impact on query of having multiple blocks&lt;/li>
&lt;li>Get real data on the efficiency of modulo operator for partitioning&lt;/li>
&lt;li>Get the necessary changes in Prometheus approved and merged&lt;/li>
&lt;li>Get the necessary changes in Thanos approved and merged&lt;/li>
&lt;li>Implement the number of partition determination in group&lt;/li>
&lt;li>Implement the grouper logic in Cortex&lt;/li>
&lt;li>Implement the clean up logic in Cortex&lt;/li>
&lt;li>Implement the partitioning strategy in Cortex, passed to Prometheus&lt;/li>
&lt;li>Produce the partitioned blocks&lt;/li>
&lt;/ul>
&lt;h2 id="appendix">Appendix&lt;/h2>
&lt;h3 id="risks">Risks&lt;/h3>
&lt;ul>
&lt;li>Is taking the modulo of the hash sufficient to produce a good distribution of partitions?&lt;/li>
&lt;li>What’s the effect of having too many blocks for the same time range?&lt;/li>
&lt;/ul>
&lt;h3 id="frequently-asked-questions">Frequently Asked Questions&lt;/h3>
&lt;ul>
&lt;li>Are we able to decrease the number of partition?
&lt;ul>
&lt;li>Using partitions of 2, and 4, and 8 as example&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>T1 partition 1 - Hash(timeseries label) % 2 == 0
T1 partition 2 - Hash(timeseries label) % 2 == 1
T2 partition 1 - Hash(timeseries label) % 4 == 0
T2 partition 2 - Hash(timeseries label) % 4 == 1
T2 partition 3 - Hash(timeseries label) % 4 == 2
T2 partition 4 - Hash(timeseries label) % 4 == 3
T3 partition 1 - Hash(timeseries label) % 8 == 0
T3 partition 2 - Hash(timeseries label) % 8 == 1
T3 partition 3 - Hash(timeseries label) % 8 == 2
T3 partition 4 - Hash(timeseries label) % 8 == 3
T3 partition 5 - Hash(timeseries label) % 8 == 4
T3 partition 6 - Hash(timeseries label) % 8 == 5
T3 partition 7 - Hash(timeseries label) % 8 == 6
T3 partition 8 - Hash(timeseries label) % 8 == 7
We are free to produce a resulting timerange T1-T3, without
having to download all 14 blocks in a single compactor
If T1-T3 can fit inside 4 partitions, we can do the following grouping
T1 partition 1 - Hash(timeseries label) % 2 == 0 &amp;amp;&amp;amp; % 4 == 0
T2 partition 1 - Hash(timeseries label) % 4 == 0 &amp;amp;&amp;amp;
T3 partition 1 - Hash(timeseries label) % 8 == 0
T3 partition 5 - Hash(timeseries label) % 8 == 4
T1 partition 2 - Hash(timeseries label) % 2 == 1 &amp;amp;&amp;amp; % 4 == 01
T2 partition 2 - Hash(timeseries label) % 4 == 1
T3 partition 2 - Hash(timeseries label) % 8 == 1
T3 partition 7 - Hash(timeseries label) % 8 == 5
T1 partition 1 - Hash(timeseries label) % 2 == 0 &amp;amp;&amp;amp; % 4 == 2
T2 partition 3 - Hash(timeseries label) % 4 == 2
T3 partition 3 - Hash(timeseries label) % 8 == 2
T3 partition 7 - Hash(timeseries label) % 8 == 6
T1 partition 2 - Hash(timeseries label) % 2 == 1 &amp;amp;&amp;amp; % 4 == 3
T2 partition 4 - Hash(timeseries label) % 4 == 3
T3 partition 4 - Hash(timeseries label) % 8 == 3
T3 partition 8 - Hash(timeseries label) % 8 == 7
If T1-T3 can fit inside 16 partitions, we can do the same grouping, and hash on top
T1 partition 1 - Hash(timeseries label) % 2 == 0 &amp;amp;&amp;amp; % 8 == 0
T2 partition 1 - Hash(timeseries label) % 4 == 0 &amp;amp;&amp;amp; % 8 == 0
T3 partition 1 - Hash(timeseries label) % 8 == 0
T1 partition 2 - Hash(timeseries label) % 2 == 1 &amp;amp;&amp;amp; % 8 == 1
T2 partition 2 - Hash(timeseries label) % 4 == 1 &amp;amp;&amp;amp; % 8 == 1
T3 partition 2 - Hash(timeseries label) % 8 == 1
T1 partition 1 - Hash(timeseries label) % 2 == 0 &amp;amp;&amp;amp; % 8 == 2
T2 partition 3 - Hash(timeseries label) % 4 == 2 &amp;amp;&amp;amp; % 8 == 2
T3 partition 3 - Hash(timeseries label) % 8 == 2
T1 partition 2 - Hash(timeseries label) % 2 == 1 &amp;amp;&amp;amp; % 8 == 3
T2 partition 4 - Hash(timeseries label) % 4 == 3 &amp;amp;&amp;amp; % 8 == 3
T3 partition 4 - Hash(timeseries label) % 8 == 3
T1 partition 1 - Hash(timeseries label) % 2 == 0 &amp;amp;&amp;amp; % 8 == 4
T2 partition 1 - Hash(timeseries label) % 4 == 0 &amp;amp;&amp;amp; % 8 == 4
T3 partition 5 - Hash(timeseries label) % 8 == 4
T1 partition 2 - Hash(timeseries label) % 2 == 1 &amp;amp;&amp;amp; % 8 == 5
T2 partition 2 - Hash(timeseries label) % 4 == 1 &amp;amp;&amp;amp; % 8 == 5
T3 partition 6 - Hash(timeseries label) % 8 == 5
T1 partition 1 - Hash(timeseries label) % 2 == 0 &amp;amp;&amp;amp; % 8 == 6
T2 partition 3 - Hash(timeseries label) % 4 == 2 &amp;amp;&amp;amp; % 8 == 6
T3 partition 7 - Hash(timeseries label) % 8 == 6
T1 partition 2 - Hash(timeseries label) % 2 == 1 &amp;amp;&amp;amp; % 8 == 7
T2 partition 4 - Hash(timeseries label) % 4 == 3 &amp;amp;&amp;amp; % 8 == 7
T3 partition 8 - Hash(timeseries label) % 8 == 7
&lt;/code>&lt;/pre>&lt;h3 id="compaction-partitioning-examples">Compaction Partitioning Examples&lt;/h3>
&lt;h4 id="scenario-all-source-blocks-were-compacted-by-partitioning-compaction-idea-case">Scenario: All source blocks were compacted by partitioning compaction (Idea case)&lt;/h4>
&lt;p>All source blocks were previously compacted through partitioning compaction. In this case for each time range, the number of blocks belong to same time range would be 2^x if multiplier is set to 2.&lt;/p>
&lt;pre tabindex="0">&lt;code>Time ranges:
T1, T2, T3
Source blocks:
T1: B1, B2
T2: B3, B4, B5, B6
T3: B7, B8, B9, B10, B11, B12, B13, B14
Total indices size of all source blocks:
200G
&lt;/code>&lt;/pre>&lt;p>Number of Partitions = (200G / 64G = 3.125) =&amp;gt; round up to next 2^x = 4&lt;/p>
&lt;p>Partitioning:&lt;/p>
&lt;ul>
&lt;li>For T1, there are only 2 blocks which is &amp;lt; 4. So
&lt;ul>
&lt;li>B1 (index 0 in the time range) can be grouped with other blocks having N % 4 == 0 or 2. Because 0 % 2 == 0.&lt;/li>
&lt;li>B2 (index 1 in the time range) can be grouped with other blocks having N % 4 == 1 or 3. Because 1 % 2 == 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>For T2,
&lt;ul>
&lt;li>B3 (index 0 in the time range) can be grouped with other blocks having N % 4 == 0.&lt;/li>
&lt;li>B4 (index 1 in the time range) can be grouped with other blocks having N % 4 == 1.&lt;/li>
&lt;li>B5 (index 2 in the time range) can be grouped with other blocks having N % 4 == 2.&lt;/li>
&lt;li>B6 (index 3 in the time range) can be grouped with other blocks having N % 4 == 3.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>For T3,
&lt;ul>
&lt;li>B7 (index 0 in the time range) can be grouped with other blocks having N % 4 == 0.&lt;/li>
&lt;li>B8 (index 1 in the time range) can be grouped with other blocks having N % 4 == 1.&lt;/li>
&lt;li>B9 (index 2 in the time range) can be grouped with other blocks having N % 4 == 2.&lt;/li>
&lt;li>B10 (index 3 in the time range) can be grouped with other blocks having N % 4 == 3.&lt;/li>
&lt;li>B11 (index 4 in the time range) can be grouped with other blocks having N % 4 == 0.&lt;/li>
&lt;li>B12 (index 5 in the time range) can be grouped with other blocks having N % 4 == 1.&lt;/li>
&lt;li>B13 (index 6 in the time range) can be grouped with other blocks having N % 4 == 2.&lt;/li>
&lt;li>B14 (index 7 in the time range) can be grouped with other blocks having N % 4 == 3.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Partitions in Partitioned Compaction Group:&lt;/p>
&lt;ul>
&lt;li>Partition ID: 0 &lt;br>
Number of Partitions: 4 &lt;br>
Blocks: B1, B3, B7, B11&lt;/li>
&lt;li>Partition ID: 1 &lt;br>
Number of Partitions: 4 &lt;br>
Blocks: B2, B4, B8, B12&lt;/li>
&lt;li>Partition ID: 2 &lt;br>
Number of Partitions: 4 &lt;br>
Blocks: B1, B5, B9, B13&lt;/li>
&lt;li>Partition ID: 3 &lt;br>
Number of Partitions: 4 &lt;br>
Blocks: B2, B6, B10, B14&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h4 id="scenario-all-source-blocks-are-level-1-blocks">Scenario: All source blocks are level 1 blocks&lt;/h4>
&lt;p>All source blocks are level 1 blocks. Since number of level 1 blocks in one time range is not guaranteed to be 2^x, all blocks need to be included in each partition.&lt;/p>
&lt;pre tabindex="0">&lt;code>Time ranges:
T1
Source blocks:
T1: B1, B2, B3
Total indices size of all source blocks:
100G
&lt;/code>&lt;/pre>&lt;p>Number of Partitions = (100G / 64G = 1.5625) =&amp;gt; round up to next 2^x = 2&lt;/p>
&lt;p>Partitioning: There is only one time range from all source blocks which means it is compacting level 1 blocks. Partitioning needs to include all source blocks in each partition.&lt;/p>
&lt;p>Partitions in Partitioned Compaction Group:&lt;/p>
&lt;ul>
&lt;li>Partition ID: 0 &lt;br>
Number of Partitions: 2 &lt;br>
Blocks: B1, B2, B3&lt;/li>
&lt;li>Partition ID: 1 &lt;br>
Number of Partitions: 2 &lt;br>
Blocks: B1, B2, B3&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h4 id="scenario-all-source-blocks-are-with-compaction-level--1-and-were-generated-by-compactor-without-partitioning-compaction">Scenario: All source blocks are with compaction level &amp;gt; 1 and were generated by compactor without partitioning compaction&lt;/h4>
&lt;p>If source block was generated by compactor without partitioning compaction, there should be only one block per time range. Since there is only one block in one time range, that one block would be included in all partitions.&lt;/p>
&lt;pre tabindex="0">&lt;code>Time ranges:
T1, T2, T3
Source blocks:
T1: B1
T2: B2
T3: B3
Total indices size of all source blocks:
100G
&lt;/code>&lt;/pre>&lt;p>Number of Partitions = (100G / 64G = 1.5625) =&amp;gt; round up to next 2^x = 2&lt;/p>
&lt;p>Partitioning:&lt;/p>
&lt;ul>
&lt;li>For T1, there is only one source block. Include B1 in all partitions.&lt;/li>
&lt;li>For T2, there is only one source block. Include B2 in all partitions.&lt;/li>
&lt;li>For T3, there is only one source block. Include B3 in all partitions.&lt;/li>
&lt;/ul>
&lt;p>Partitions in Partitioned Compaction Group:&lt;/p>
&lt;ul>
&lt;li>Partition ID: 0 &lt;br>
Number of Partitions: 2 &lt;br>
Blocks: B1, B2, B3&lt;/li>
&lt;li>Partition ID: 1 &lt;br>
Number of Partitions: 2 &lt;br>
Blocks: B1, B2, B3&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h4 id="scenario-all-source-blocks-are-with-compaction-level--1-and-some-of-them-were-generated-by-compactor-with-partitioning-compaction">Scenario: All source blocks are with compaction level &amp;gt; 1 and some of them were generated by compactor with partitioning compaction&lt;/h4>
&lt;p>Blocks generated by compactor without partitioning compaction would be included in all partitions. Blocks generated with partitioning compaction would be partitioned based on multiplier.&lt;/p>
&lt;pre tabindex="0">&lt;code>Time ranges:
T1, T2, T3
Source blocks:
T1: B1 (unpartitioned)
T2: B2, B3
T3: B4, B5, B6, B7
Total indices size of all source blocks:
100G
&lt;/code>&lt;/pre>&lt;p>Number of Partitions = (100G / 64G = 1.5625) =&amp;gt; round up to next 2^x = 2&lt;/p>
&lt;p>Partitioning:&lt;/p>
&lt;ul>
&lt;li>For T1, there is only one source block. Include B1 in all partitions.&lt;/li>
&lt;li>For T2,
&lt;ul>
&lt;li>B2 (index 0 in the time range) can be grouped with other blocks having N % 2 == 0.&lt;/li>
&lt;li>B3 (index 1 in the time range) can be grouped with other blocks having N % 2 == 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>For T3,
&lt;ul>
&lt;li>B4 (index 0 in the time range) can be grouped with other blocks having N % 2 == 0.&lt;/li>
&lt;li>B5 (index 1 in the time range) can be grouped with other blocks having N % 2 == 1.&lt;/li>
&lt;li>B6 (index 2 in the time range) can be grouped with other blocks having N % 2 == 0.&lt;/li>
&lt;li>B7 (index 3 in the time range) can be grouped with other blocks having N % 2 == 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Partitions in Partitioned Compaction Group:&lt;/p>
&lt;ul>
&lt;li>Partition ID: 0 &lt;br>
Number of Partitions: 2 &lt;br>
Blocks: B1, B2, B4, B6&lt;/li>
&lt;li>Partition ID: 1 &lt;br>
Number of Partitions: 2 &lt;br>
Blocks: B1, B3, B5, B7&lt;/li>
&lt;/ul></description></item></channel></rss>